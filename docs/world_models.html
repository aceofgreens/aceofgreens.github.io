<!DOCTYPE html>
<html lang="en">

<head>
    <title>World Models | The Critical Section</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Pelican" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <link rel="stylesheet" href="/assets/main.css" />
    <link rel="icon" href="/assets/favicon.png" />
    <link href="https://fonts.googleapis.com/css?family=Roboto|Oswald|Open+Sans" rel="stylesheet">


    <meta name="description" content="World models are still a central topic in RL, even after 30 years since their inception. But they're also growing in popularity, even outside of RL. Nowadays interest is fueled by video models like OpenAI's Sora, Luma's DreamMachine, Google's Veo, or Runway's Gen-3-Alpha. The idea here is that to be able to generate accurate physics, occlusion, deformations, or interactions, you need to have proper knowledge of how the real world works. This post will trace the development of world models from the last ten or so years up to today, focusing on key ideas and developments." />

    <meta name="tags" content="rl" />

</head>

<body onload="welcomeFunction()" >

  <header class="site-header" role="banner">
    <div class="wrapper">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> <!--An icon library for the button icon-->

      <a class="site-title" rel="author" href="/">The Critical Section</a>

      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
        </label>

        <div class="trigger">
            <a class="page-link" href="/about.html">About</a>
            <a class="page-link" href="/posts.html">Posts</a>
            <a class="page-link" href="/tags.html">Tags</a>
        </div>

      </nav>

    </div>
  </header>


  <main class="page-content" aria-label="Content">
    <div class="wrapper">

  <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <header class="post-header">
      <h1 class="post-title p-name" itemprop="name headline">World Models</h1>
      <p class="post-meta">
        <time class="dt-published" datetime="2024-08-23T07:00:00+02:00" itemprop="datePublished">
          23 Aug 2024
        </time>
      </p>

    </header>

    <div class="post-content">
      <p>World models are still a central topic in RL, even after 30 years since their inception. But they're also growing in popularity, even outside of RL. Nowadays interest is fueled by video models like OpenAI's <a href="https://openai.com/index/sora/">Sora</a>, Luma's <a href="https://lumalabs.ai/dream-machine">DreamMachine</a>, Google's <a href="https://deepmind.google/technologies/veo/">Veo</a>, or Runway's <a href="https://runwayml.com/research/introducing-gen-3-alpha">Gen-3-Alpha</a>. The idea here is that to be able to generate accurate physics, occlusion, deformations, or interactions, you need to have proper knowledge of how the real world works. This post will trace the development of world models from the last ten or so years up to today, focusing on key ideas and developments.</p>
<p>A world model is a very specific thing. It's a mapping <span class="math">\((s_t, a_t) \mapsto s_{t+1}\)</span> from state-action pairs to subsequent states and represents causal knowledge of the world, in the sense that taking an action <span class="math">\(a_t\)</span> might lead to <span class="math">\(s_{t+1}\)</span> but taking a different action <span class="math">\(a_t'\)</span> might lead to a different <span class="math">\(s_{t+1}'\)</span>. Video generation does not necessarily indicate world knowledge because the space of possible realistic videos corresponding to the prompt is very large and a video generation model needs to successfully sample only one of these videos.</p>
<p>Often a world model also requires learning the reward dynamics <span class="math">\((s_t, a_t) \mapsto r_{t}\)</span> and even the observations <span class="math">\((s_t) \mapsto o_{t}\)</span> in the case of partial observability. A dominant approach back in the day was to have a RNN processing sequences of observations and then an autoencoder that takes in the hidden state and produces a kind of latent <span class="math">\(z_t\)</span> which represents the state.</p>
<p>The benefits of having a world model are that:</p>
<ol>
<li>It enables planning, which is guaranteed to minimize some cost function at test time</li>
<li>It is more sample-efficient compared to model-free methods</li>
<li>It forms abstract representations, as typically the dynamics predictor takes in latent representations, not raw observations.</li>
<li>It allows for transfer and generalization across tasks and domains.</li>
</ol>
<p>When it comes to planning, there are different ways to do it. For discrete action spaces <a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">MCTS</a> is common (but we'll discuss it in another post). For continuous ones, <a href="https://en.wikipedia.org/wiki/Model_predictive_control">MPC</a> and Dyna are common:</p>
<ul>
<li><em>Model predictive control</em> (MPC) essentially plans a trajectory, executes a single step from it, and then replans again. It is sample-efficient and well-understood. It's also adaptive (because of the closed loop planning) but costly to execute because you replan at every step. It is also myopic since you do not commit to executing the full planned trajectory but only its first step. The actual planning of the trajectory typically involves the <a href="https://en.wikipedia.org/wiki/Cross-entropy_method">cross-entropy method</a> (CEM).</li>
<li><em>Dyna</em> is similar but allows for deep exploration where the agent commits to executing its learned policy. Essentially, you fully-train a policy within your world model, which acts as a simulator, and then you deploy that policy in the real environment, letting it collect new data. This data is added to the replay buffer from which you will retrain a policy and repeat the process as necessary.</li>
</ul>
<p>The idea of learning from imagined experiences has been around at least since the '90s [1, 2] but getting these things to work on bigger, more realistic problems has been slow. In 2015 it was shown that action-conditioned frame generation can be learned in the context of Atari games [3]. The architecture used a CNN feature extractor, a LSTM module whose output hidden state gets concatenated with the encoded actions, and a decoder CNN to produce images. In 2018, a major paper came out - Schmidhuber's "World models" [4]. In it, a VAE extracts a low-dimensional representation of the visual observation, <span class="math">\(z_t\)</span>, which is then passed to a RNN that predicts the future <span class="math">\(z_{t+1}\)</span>, conditional on past actions. The hidden state along with <span class="math">\(z_t\)</span> is passed to a small controller which selects actions. The decoder part of the VAE reconstructs the imagined frames. Then, training in the world model gives you pretty good performance in the actual environment.</p>
<p>In 2019 Hafner introduced the recurrent state-space model [5] (RSSM), where the transitions are decomposed into both deterministic and stochastic components. It used MPC for planning. This work was then adapted into Dreamer [6] which uses Dyna-style planning and is truly a big achievement in world model research. Dreamer learns the dynamics from sequences of executed roll-outs and the policy from interactions in the imagined world dynamics. The gradients with respect to the policy weights actually go through <em>all</em> the imagined trajectories. Subsequently, DreamerV2 [7] was introduced to plan with discrete world models, and then DreamerV3 [4], which could mine diamonds in Minecraft without human data or curricula. In "Daydreamer" [9] they applied Dreamer on a real world quadruped robot, which learned how to stand up and walk straight just within an hour.</p>
<figure>
    <img class='big_img' src="/images/dreamer.png" alt="DreamerV1" width="1200">
    <figcaption>Figure 1: DreamerV1. (a) Observation $o_t$ is encoded into latent $z_t$, which is evolved using an RNN according to the action $a_t$. A reward predictor for $r_t$ is learned. A decoder reconstructs the observation $o_t$ from $z_t$. (b) A policy is learned on entirely simulated roll-outs. (c) The policy is evaluated in the real environment. Image taken from [6]. </figcaption>
</figure>

<p>All of these methods learn a world model that is specific to the current environment and then act (optimally) in that same environment. A recent trend has been to focus instead on generalization and transfer. That is, can the world model capture patterns which are common across multiple environments thereby helping the agent to transfer policies to newer settings?</p>
<p>In that setting "Visual foresight" [10] was one of the first methods. At train time it collects experience in an autonomous unsupervised manner and trains a video prediction model. At test time novel manipulation tasks, based on novel reward functions, are solved using MPC. A similar method is "Plan2Explore" [11]. Here we first train a wold model without having any task-specific rewards, which is called the <em>exploration</em> phase and then finally enable the task rewards, at which point the agent very quickly learns to solve the task, due to the world model being already trained. For the exploration phase, a policy that maximizes next state novelty is trained within simulated trajectories. The authors measure epistemic uncertainty as the information gain of the next frame hidden values.</p>
<p>LEXA [12] is an interesting model where we use the world model to train two agents - one <em>explorer</em> which is trained on imagined rollouts and maximizes some exploration objective, and one <em>achiever</em> which is trained to learn policies that achieve, or reach a given target state. As you can see the idea is relatively similar to the papers before. And one can imagine that adding language, as a universal interface for describing tasks and goals, can allow for the zero-shot solving of novel tasks from text at test time. This is what was done recently [13]. Researchers took DreamerV3 and generalized it to take in per-frame text tokens and to be able to decode the latent state into images <em>and</em> text.</p>
<p>Since 2021 a new trend has started to pick up stream, that of foundation models. Here the idea is that a world model will be trained on massive internet-scale offline datasets, without interacting with any environments. Then, the world model is used to plan action sequences that can easily generalize to novel environments and tasks.</p>
<p>In general, to learn a good world model from offline data one needs to balance the potential gain in performance by escaping the behavioural distribution and finding a better policy, and the risk of overfitting to the errors of the dynamics at regions far away from the behavioural distribution. In MOPO [14] they address this by subtracting some quantity proportional to the model uncertainty from each reward, thereby obtaining a new, more conservative MDP, on which to train the policy. In [15] they show that adding noise to the dynamics of the world model helps generalization, allowing better performance with new dynamics at test time.</p>
<p>And finally, the latest development in world models is to utilize generative models. For example, diffusion models can serve as game engines. You play DOOM and the world model generates the next game frame, in real time, based on past and current frames and actions [17]. So you're in effect playing DOOM within your neural network. Another similar idea is digital twins. Say you train a generative world model on top of all the sensors in a large-scale industrial factory. And then you add a language interface for good measure. Then you'll be able to ask things like <em>"Tell me, factory, were there any unusual events last night?</em>".</p>
<p>The first big generative world model was Wayve's GAIA-1 [18], for driving. It has about 7B parameters. It works by taking in multiple modalities, like image sequences, IMU values, and text explanations, tokenizes them using separate encoders and feeds all resulting tokens to a LLM-like <em>autoregressive</em> predictor. The predicted tokens are decoded back to video using <em>diffusion</em>. The end result is a model that can generate future imagined photorealistic image sequences, all conditional on actions, text, and past images.</p>
<p>Another example here is UniSim [19] which tries to learn a universal simulator of real-world interaction through generative modeling. It is a video diffusion model (applies diffusion over pixels) that predicts the next observation frames from noisy previous frames and actions. And actions can be supplemented with language so that the model can also plan using language.</p>
<figure>
    <img class='big_img' src="/images/unisim.png" alt="UniSim" width="1200">
    <figcaption>Figure 2: UniSim, a world model for realistic human and robotic manipulation and navigation tasks. The model predicts future frames $o_1$ from noisy past frames, proprioceptive actions, linguistic commands, or camera controls. Image taken from [19].</figcaption>
</figure>

<p>The latest big advancement is Genie [20]. This is a generator for interactive environments. The user can requests a "futuristic, sci-fi, pixelated, platformer" and Genie will create a fully-playable environment styled according to that request. The model is trained from video-only data and has the following components:</p>
<ul>
<li>A video tokenizer, based on a VQ-VAE [21], takes in video frames <span class="math">\(\mathbf{x}_{1:T}\)</span> of shape <span class="math">\((T, H, W, C)\)</span> and outputs tokens <span class="math">\(\mathbf{z}_{1:T}\)</span> of shape <span class="math">\((T, D)\)</span>. </li>
<li>A latent action model (LAM) infers <em>latent</em> actions from frame sequences in an unsupervised manner. It works as follows. First, from a frame sequence <span class="math">\(\mathbf{x}_{1:t+1}\)</span> we estimate latent actions <span class="math">\(\tilde{\mathbf{a}}_{1:t}\)</span>. Then, from <span class="math">\(\mathbf{x}_{1:t}\)</span> and <span class="math">\(\tilde{\mathbf{a}}_{1:t}\)</span> we predict <span class="math">\(\hat{\mathbf{x}}_{t+1}\)</span>. Using another VQ-VAE-style loss, we can force each of the <span class="math">\(\tilde{\mathbf{a}}_{1:t}\)</span> to be one of a small number of action tokens, e.g. <span class="math">\(8\)</span>. These end up perfectly corresponding to standard platformer actions (left, right, up, down, etc.).</li>
<li>A dynamics model which takes in past tokens <span class="math">\(\mathbf{z}_{1:t-1}\)</span> and detached latent actions <span class="math">\(\text{sg}[\tilde{\mathbf{a}}_{1:t-1}]\)</span> and outputs the next tokens <span class="math">\(\hat{z}_t\)</span>. A special MaskGIT transformer [22] is used, one that separates attention over time and space into separate layers in each block.</li>
</ul>
<p>Thus, the current trend of scaling world models to billions of parameters and to internet-size training data from all kinds of scenarios, seems to be very promising. Naturally, there are still a lot of challenges and the community has not converged on what are the best practices for this approach. But it is believed that there's a lot of room for improvement. Scaling laws apply, and most of these models are still not practically usable to yield real value. The most exciting time will be tomorrow. The second most exciting time is today.</p>
<h3>References</h3>
<p>[1] Schmidhuber, Jürgen. <a href="https://people.idsia.ch/~juergen/FKI-126-90ocr.pdf">Making the world differentiable: on using self supervised fully recurrent neural networks for dynamic reinforcement learning and planning in non-stationary environments.</a> Vol. 126. Inst. für Informatik, 1990. <br>
[2] Sutton, Richard S. <a href="https://dl.acm.org/doi/abs/10.1145/122344.122377">Dyna, an integrated architecture for learning, planning, and reacting.</a> ACM Sigart Bulletin 2.4 (1991): 160-163. <br>
[3] Oh, Junhyuk, et al. <a href="https://proceedings.neurips.cc/paper/2015/hash/6ba3af5d7b2790e73f0de32e5c8c1798-Abstract.html">Action-conditional video prediction using deep networks in atari games.</a> Advances in neural information processing systems 28 (2015). <br>
[4] Ha, David, and Jürgen Schmidhuber. <a href="https://arxiv.org/abs/1803.10122">World models.</a> arXiv preprint arXiv:1803.10122 (2018). <br>
[5] Hafner, Danijar, et al. <a href="https://proceedings.mlr.press/v97/hafner19a.html">Learning latent dynamics for planning from pixels.</a> International conference on machine learning. PMLR, 2019. <br>
[6] Hafner, Danijar, et al. <a href="https://arxiv.org/abs/1912.01603">Dream to control: Learning behaviors by latent imagination.</a> arXiv preprint arXiv:1912.01603 (2019). <br>
[7] Hafner, Danijar, et al. <a href="https://arxiv.org/abs/2010.02193">Mastering atari with discrete world models.</a> arXiv preprint arXiv:2010.02193 (2020). <br>
[8] Hafner, Danijar, et al. <a href="https://arxiv.org/abs/2301.04104">Mastering diverse domains through world models.</a> arXiv preprint arXiv:2301.04104 (2023). <br>
[9] Wu, Philipp, et al. <a href="https://proceedings.mlr.press/v205/wu23c.html">Daydreamer: World models for physical robot learning.</a> Conference on robot learning. PMLR, 2023. <br>
[10] Ebert, Frederik, et al. <a href="https://arxiv.org/abs/1812.00568">Visual foresight: Model-based deep reinforcement learning for vision-based robotic control.</a> arXiv preprint arXiv:1812.00568 (2018). <br>
[11] Sekar, Ramanan, et al. <a href="https://proceedings.mlr.press/v119/sekar20a.html">Planning to explore via self-supervised world models.</a> International conference on machine learning. PMLR, 2020. <br>
[12] Mendonca, Russell, et al. <a href="https://proceedings.neurips.cc/paper/2021/hash/cc4af25fa9d2d5c953496579b75f6f6c-Abstract.html">Discovering and achieving goals via world models.</a> Advances in Neural Information Processing Systems 34 (2021): 24379-24391. <br>
[13] Lin, Jessy, et al. <a href="https://arxiv.org/abs/2308.01399">Learning to model the world with language.</a> arXiv preprint arXiv:2308.01399 (2023). <br>
[14] Yu, Tianhe, et al. <a href="https://proceedings.neurips.cc/paper_files/paper/2020/hash/a322852ce0df73e204b7e67cbbef0d0a-Abstract.html">Mopo: Model-based offline policy optimization.</a> Advances in Neural Information Processing Systems 33 (2020): 14129-14142. <br>
[15] Ball, Philip J., et al. <a href="https://proceedings.mlr.press/v139/ball21a.html">Augmented world models facilitate zero-shot dynamics generalization from a single offline environment.</a> International Conference on Machine Learning. PMLR, 2021. <br>
[16] Lu, Cong, et al. <a href="https://arxiv.org/abs/2206.04779">Challenges and opportunities in offline reinforcement learning from visual observations.</a> arXiv preprint arXiv:2206.04779 (2022). <br>
[17] Valevski, Dani, et al. <a href="https://arxiv.org/abs/2408.14837">Diffusion Models Are Real-Time Game Engines.</a> arXiv preprint arXiv:2408.14837 (2024). <br>
[18] Hu, Anthony, et al. <a href="https://arxiv.org/abs/2309.17080">Gaia-1: A generative world model for autonomous driving.</a> arXiv preprint arXiv:2309.17080 (2023). <br>
[19] Yang, Mengjiao, et al. <a href="https://ai-data-base.com/wp-content/uploads/2023/10/2310.06114_compressed.pdf">Learning interactive real-world simulators.</a> arXiv preprint arXiv:2310.06114 (2023). <br>
[20] Bruce, Jake, et al. <a href="https://openreview.net/forum?id=bJbSbJskOS">Genie: Generative interactive environments.</a> Forty-first International Conference on Machine Learning. 2024. <br>
[21] Van Den Oord, Aaron, and Oriol Vinyals. <a href="https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html">Neural discrete representation learning.</a> Advances in neural information processing systems 30 (2017). <br>
[22] Chang, Huiwen, et al. <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chang_MaskGIT_Masked_Generative_Image_Transformer_CVPR_2022_paper.html">Maskgit: Masked generative image transformer.</a> Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = "https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML";

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'], ['$', '$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div>
  
  <p class="post-meta">
    Tag: rl
  </p>

  </article>


    </div>
  </main>


<footer class="site-footer h-card">
  <div class="wrapper">
  
  <p></p>
  

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">


    <div class="footer-col-wrapper">

      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">The Critical Section</li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          <li><a href="https://github.com/aceofgreens"><i class="fa fa-github"></i> GitHub</a></li>


        </ul>
      </div>


      <div class="footer-col footer-col-3">
        <p>A personal blog for artificial intelligence and similar topics.</p>
      </div>
    </div>

  </div>
</footer>

<script type="text/javascript">
function welcomeFunction() {
  var items = document.getElementsByTagName("code");
    for (var i = items.length; i--;) {
      items[i].setAttribute("class", "highlight");
  }
}
</script>

</body>
</html>