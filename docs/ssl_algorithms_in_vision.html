<!DOCTYPE html>
<html lang="en">

<head>
    <title>SSL Algorithms in Vision | The Critical Section</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Pelican" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <link rel="stylesheet" href="/assets/main.css" />
    <link rel="icon" href="/assets/favicon.png" />
    <link href="https://fonts.googleapis.com/css?family=Roboto|Oswald|Open+Sans" rel="stylesheet">


    <meta name="description" content="Self-supervised learning (SSL) has emerged as the cornerstone behind the surge of large-scale models permeating the mainstream. In this post, we'll delve into the main families of SSL algorithms, primarily with a focus on those in the realm of computer vision." />

    <meta name="tags" content="ai" />

</head>

<body onload="welcomeFunction()" >

  <header class="site-header" role="banner">
    <div class="wrapper">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> <!--An icon library for the button icon-->

      <a class="site-title" rel="author" href="/">The Critical Section</a>

      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
        </label>

        <div class="trigger">
            <a class="page-link" href="/about.html">About</a>
            <a class="page-link" href="/posts.html">Posts</a>
            <a class="page-link" href="/tags.html">Tags</a>
        </div>

      </nav>

    </div>
  </header>


  <main class="page-content" aria-label="Content">
    <div class="wrapper">

  <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <header class="post-header">
      <h1 class="post-title p-name" itemprop="name headline">SSL Algorithms in Vision</h1>
      <p class="post-meta">
        <time class="dt-published" datetime="2023-08-22T07:00:00+02:00" itemprop="datePublished">
          22 Aug 2023
        </time>
      </p>

    </header>

    <div class="post-content">
      <p>Self-supervised learning (SSL) has emerged as the cornerstone behind the surge of large-scale models permeating the mainstream. In this post, we'll delve into the main families of SSL algorithms, primarily with a focus on those in the realm of computer vision.</p>
<p>SSL is one of the most important developments in machine learning. It's the idea that by modeling the structure of the data, even without the use of any target labels, one can still learn incredibly useful features for downstream applications. This has been verified experimentally to be true for many different tasks and datasets. In fact, the concept of <em>linear probing</em>, where one uses a single linear output layer on top of the SSL features, shows that the model trained with SSL learns sophisticated representations in a space where the targets are linearly separable. Why this happens is, as of now, unknown.</p>
<p>SSL is not a novel topic. There are many methods prior to 2020, when modern SSL took off, that go in the direction of learning structural information from the data itself. The most popular examples are:</p>
<ol>
<li><strong>Information restoration</strong> - this includes colorization from grayscale or methods like masked inpainting, which later evolved into the extremely popular and successful masked autoencoders approach.</li>
<li>Using <strong>temporal relationships in video</strong> - here one can do a lot of things - remove audio and predict it from the video, use triplet losses to make the representations of similar objects from two different frames be similar, predict the next frame from the past and so on.</li>
<li>Learning <strong>spatial context</strong>. This boils down to learning spatial relations between objects. One can apply random rotations to the objects and have the model predict the rotations, or simply predict the relative location of two patches from the image. The similar, but harder task of creating a "jigsaw" puzzle by permuting patches of the image and then trying to put them back into place is also very popular.</li>
<li><strong>Grouping similar images together</strong> - here the idea is to learn good features by grouping similar images together. One successful method clusters images using K-means and forces the neural image embeddings to be similar if the underlying images belong to the same cluster, and different otherwise. </li>
<li><strong>Generative models</strong> - GANs have been found to be useful for transfer learning and there have been successful attempts to borrow ideas from their adversarial nature into more complicated systems.</li>
<li><strong>Multi-view invariance</strong>. Here the idea is to be invariant with respect to different random augmentations of the image. One of the first successful approaches maximized the mutual information between different representations. Other methods have tried to use weakly-trained predictors to obtain pseudolabels, which are then used to force similarity between the different augmentations.</li>
</ol>
<h3>The Deep Metric Learning Family</h3>
<p>This is the first family of modern top-performing SSL methods. The idea here is to learn similar representations for similar images according to some criteria like labels, distance between labels, or fixed transformations. Positive samples are those whose representations should be similar, while negative ones are opposite. Since some samples are so different that their representations are trivially very different, it's important to find negative samples which are sufficiently similar and challenge the model. This aspect is called <em>hard negative mining</em>. One way to insure that enough hard negatives are observed is to have very large batch sizes when training.</p>
<p>The most basic objective is given by the margin loss </p>
<div class="math">$$
\mathcal{L}(x_1, x_2, y) = \max(0, -y {\lVert x_1 - x_2 \rVert}_p + m).
$$</div>
<p>Here, <span class="math">\(x_1\)</span> and <span class="math">\(x_2\)</span> are the two representations, typically projected to a smaller dimension, <span class="math">\({\lVert \cdot \rVert}_p\)</span> is the <span class="math">\(p\)</span>-norm, <span class="math">\(y \in \{ -1, 1 \}\)</span> shows whether they should be the same or not, and <span class="math">\(m\)</span> is a tolerable margin between the distance. If <span class="math">\(y = 1\)</span>, then the loss is <span class="math">\(0\)</span> if the difference between the two representations is smaller than <span class="math">\(m\)</span>. If <span class="math">\(y=-1\)</span>, the loss is <span class="math">\(0\)</span> if the difference is larger than <span class="math">\(m\)</span>.</p>
<p>The similar triplet loss compares an anchor point <span class="math">\(x_0\)</span> with a positive point <span class="math">\(x_p\)</span> of the same class, and a negative point <span class="math">\(x_-\)</span> of a different class and enforces the distances to be sufficiently different:</p>
<div class="math">$$ \mathcal{L}(x_0, x_+, x_-) = \max ({\lVert x_a - x_+ \rVert}_p - {\lVert x_0 - x_- \rVert}_p + m, 0).$$</div>
<p>Ideally, a model trained with the triplet loss is able to minimize the distance between the anchor and the positive while maximizing the distance between the anchor and the negative. The model FaceNet uses a triplet loss to learn feature spaces where the distance between features directly corresponds to how similar any two human faces are.</p>
<p>With time, deep metric learning has gradually evolved into <em>contrastive predictive coding</em>, also called contrastive SSL. This change has led to the usage of bigger datasets, deeper networks, and the use of <em>projectors</em> which map the representation into a lower-dimensional space where the loss is calculated, and are otherwise discarded at test time. The hard negative sampling has given way to random sampling of augmentations where the positives are those views which come from the same sample, and the nagatives are all other samples.</p>
<p>The most popular loss of this kind is the infoNCE loss [1]:</p>
<div class="math">$$
\mathcal{L} = - \sum_{(i, j) \in \mathbb{P}} \log \Big( \frac{e^{\text{CoSim}(z_i, z_j)/\tau}}{\sum_{k = 1}^N e^{\text{CoSim}(z_i, z_k)/\tau}}  \Big),
$$</div>
<p>where we try to identify a single positive sample from the set of <span class="math">\(N-1\)</span> other noise samples. It's a form of cross-entropy. In general, entropy gives you the amount of information contained in a probabilistic event. Consequently, a perfect encoding of the underlying probability distribution will require exactly that many bits of information to encode that event. Cross-entropy instead gives us the amount of information needed to encode (or identify) that event if our coding is inefficient. It is lower-bounded by the entropy. Hence, if the underlying random variable is degenerate and is deterministic (e.g. when classifying cats and dogs, where the labels are <span class="math">\(0\)</span> and <span class="math">\(1\)</span>) the lower bound of the cross-entropy is <span class="math">\(0\)</span>.</p>
<p><strong>SimCLR</strong> [2] is a very successful SSL model that uses the infoNCE loss. It works by sampling a batch of <span class="math">\(N\)</span> images, <span class="math">\(\{x_k \}_{k = 1}^N\)</span>, applying two sets of random augmentations like cropping, scaling, colour jittering to each image, pushing each view through the encoder, and then computing the infoNCE loss on the resulting representations. In particular, of the <span class="math">\(2N\)</span> total image views, we can calculate the loss on a single positive pair, defined as that tuple of views which originate from the same sample, as </p>
<div class="math">$$ \ell(i, j) = - \log \Big ( \frac{\exp(s_{i, j} /\tau  )} {\sum_{k = 1}^{2N} \mathbb{I}_{[k \ne i]} \exp (s_{i, k} \tau)} \Big ). $$</div>
<p>Here <span class="math">\(s_{i, j}\)</span> is some similarity metric like a cosine similarity or a simple dot product and <span class="math">\(\tau\)</span> is a temperature parameter which increases or decreases the sharpness of events in predictions. Note that in the denominator the sum runs across all other <span class="math">\(2N - 1\)</span> views different from <span class="math">\(i\)</span>. Therefore, if we order them such that those views from the same sample have subsequent indices, the model learns that views <span class="math">\(2i - 1\)</span> and <span class="math">\(2i\)</span> are positive pairs, but <span class="math">\(2i - 1\)</span> and any other view from <span class="math">\(\{1, ..., 2i-2, 2i + 1, ... 2N\}\)</span> are negative. The end result is the ability to recognize when two augmentations come from the same sample (a form of augmentation invariance) which turns out to produce very useful features for downstream tasks.</p>
<p>The total loss is given by <span class="math">\(\mathcal{L} = \frac{1}{2N} \sum_{k=1}^N (\ell(2k - 1, 2k) + \ell(2k, 2k-1))\)</span> which simply aggregates across all positive samples and handles the asymmetric nature of the infoNCE loss.</p>
<p>The <strong>NNCLR</strong> model [3] is similar to SimCLR except that it learns similarity not within a single instance, but across different instances in the dataset. This yields more diverse semantic variations than only using pre-defined transformations. Essentially, given a minibatch of images, NNCLR starts by applying two sets of random transformations to each image like in SimCLR, obtaining two sets of views. Now for each view in the first set, it finds the nearest neighbor latent embedding from a support set of <em>other instances</em> from the dataset. This support set is implemented as a simple FIFO queue which stores running latent representations across instances. Then the infoNCE is applied on the set of nearest neighbors and the set of random augmentations.</p>
<p><strong>MeanShift</strong> [4] is another popular SSL method that works in a similar way. From a single image, two random views are extracted. One is fed to a online encoder updated with gradient descent, the other is fed to a target encoder updated using slow moving average on the weights of the online one. The latent from the target encoder goes into a memory bank, from which a set of nearest neighbors (more than one) are retrieved. The latent from the online decoder is pushed towards the mean of the nearest neighbors.</p>
<h3>The Self-Distillation Family</h3>
<p><em>Model distillation</em> is a technique where knowledge from a large, complex model (often referred to as the "teacher" model) is transferred to a smaller, simpler model (known as the "student" model). The goal is to get the student model to perform as closely as possible to the teacher model, but with the benefits of reduced size, faster inference times, and possibly fewer computational resources for deployment. <em>Self-distillation</em> instead refers to a process where a trained model (the teacher) is used to generate pseudo-labels or soft targets for the training data, and then the same model (the student) is trained on these pseudo-labels or soft targets to improve its performance.</p>
<p><strong>BYOL</strong> (bootstrap your own latent) [5] is the canonical model here. It samples two augmentations of the image and feeds them to two encoders - one is the student, <span class="math">\(f_{\theta_s}\)</span>, the other is the teacher, <span class="math">\(f_{\theta_t}\)</span>. Then a predictor network <span class="math">\(p_\gamma\)</span> maps the latent from student into a prediction which gets compared with the latent from the teacher. The student is updated using gradient descent. The teacher is updated using an exponential moving average parameterized by <span class="math">\(\xi\)</span> on the weights of the student, which prevents collapse.</p>
<div class="math">$$
\begin{align}
\mathcal{L} &amp;= \mathbb{E}_{x, t_1, t_2 \sim (X, T_1, T_2)}  \Big[ { \big\lVert h\big(p_\gamma(f_{\theta_s}(t_1(x))) \big) - h \big(f_{\theta_t} (t_2(x)) \big) \big\rVert}_2^2 \Big] \\
h(v) &amp;= \frac{v}{\max( {\lVert v \rVert}_2 + \epsilon)} \\
\theta_t &amp;= \xi \theta_t + (1 - \xi)\theta_s 
\end{align}
$$</div>
<p>Here, <span class="math">\(h\)</span> is a <span class="math">\(\ell_2\)</span> normalization on the latents and <span class="math">\(t_1\)</span>, <span class="math">\(t_2\)</span> are the random augmentations. Notice that the MeanShift method is exactly equal to BYOL when the number of neighbors is one.</p>
<p><strong>SimSiam</strong> [6] simplifies the objective by not using EMA updates on the target network, but simply detaching the tensors at the right place in the computational graph. <strong>DINO</strong> [7] centers the outputs of the encoder using a running mean across the batch, applies a temperature-conditioned softmax, in effect discretizing the representations, normalizes, and finally minimizes the cross entropy between the student output and the detached teacher output.</p>
<p>Compared to BYOL which contrasts between two augmentations of the same image but processed through slightly different networks, <strong>MoCo</strong> relies on contrastive learning with the infoNCE loss [8]. It introduces a momentum encoder, a moving average of the main encoder, which further stores representations from previous batches into a queue. When training, we sample two sets of augmentations, called the <em>queries</em> and the <em>keys</em>, from the current batch. Now their pairs are considered positive samples, while the pairs between the queries and the latents in the queue are considered negatives. The infoNCE loss is applied on all positive and negative pairs.</p>
<h3>The Canonical Correlation Analysis Family</h3>
<p><a href="https://en.wikipedia.org/wiki/Canonical_correlation">Canonical correlation analysis</a> (CCA) is a very general way to learn the relationships between two random vectors <span class="math">\(X \in \mathbb{R}^n\)</span> and <span class="math">\(Y \in \mathbb{R}^m\)</span> from their <a href="https://en.wikipedia.org/wiki/Cross-covariance_matrix">cross-covariance matrix</a>. CCA seeks two vectors <span class="math">\(a\)</span> and <span class="math">\(b\)</span> such that the correlation <span class="math">\(\text{corr}(a^TX, b^TY)\)</span> is maximal. In that case <span class="math">\(a\)</span> and <span class="math">\(b\)</span> are called the first pair of canonical variables. One can find the second pair by finding a similar pair of vectors which maximize that correlation but are themselves uncorrelated to the previous pairs of canonical variables.</p>
<p>In the context of deep learning we usually seek nonlinear transformations. If they are <span class="math">\(f_x\)</span> and <span class="math">\(f_y\)</span>, then their outputs are <span class="math">\(U = f_x(X)\)</span> and <span class="math">\(V = f_y(Y)\)</span>. The CCA framework optimizes for <span class="math">\(f_x\)</span> and <span class="math">\(f_y\)</span> such that <span class="math">\(U\)</span> and <span class="math">\(V\)</span> have zero-mean, identity covariance, and maximal agreement across the whole dataset:</p>
<div class="math">$$
\mathcal{L} = - \sum_{n = 1}^N \langle U_n, V_n \rangle \\
\text{s.t. } \frac{1}{N}\sum_{n = 1}^N U_n = \frac{1}{N}\sum_{n = 1}^N V_n = \mathbf{0}, \frac{1}{N} \textbf{U}^T\textbf{U} = \frac{1}{N} \textbf{V}^T \textbf{V} = \textbf{I}
$$</div>
<p><strong>VICReg</strong> (variance, invariance, covariance regularization) [9] samples two sets of augmented views <span class="math">\(X\)</span> and <span class="math">\(X'\)</span> from the current image batch, and encodes them into representations <span class="math">\(Z\)</span> and <span class="math">\(Z'\)</span>, after which it uses a loss with three regularization terms. The first encourages the variance of the variables within <span class="math">\(Z\)</span> and <span class="math">\(Z'\)</span> to be above a certain threshold, thus encouraging diversity across the samples in the batch. The second is a covariance term which acts on the individual variables in <span class="math">\(Z\)</span> and separately in <span class="math">\(Z'\)</span> by forcing them to have zero covariance. The intuition is that the individual elements of <span class="math">\(Z\)</span> should capture as much information as possible by being decorrelated. Lastly, an invariance term acts to minimize the mean squared distance between the representations in <span class="math">\(Z\)</span> and those in <span class="math">\(Z'\)</span>. This acts across the two sets of encodings. Obviously, such a term is needed because <span class="math">\(X\)</span> and <span class="math">\(X'\)</span> come from the same set of underlying images.</p>
<p>Another similar method is <strong>BarlowTwins</strong> [10], where from the representations <span class="math">\(Z\)</span> and <span class="math">\(Z'\)</span> we directly compute a cross-correlation matrix and push it towards the identity matrix. This itself has two effects: it forces the individual elements of the representation to have unit variance, and also forces the elements to be as diverse as possible (to have zero covariances).  <strong>SwAV</strong> (swapped assignments between views) [11] is a particularly interesting method that does not compare image features directly but rather uses a "swapped" prediction task. First, apart from training the encoder, this method also learns a number of "prototype" vectors, or "codes". They are learned jointly with the other parameters using gradient descent. Then each representation from the two sets of the augmented views is assigned to one of these code vectors. Supose <span class="math">\(z\)</span> and <span class="math">\(z'\)</span> are the representations and <span class="math">\(q\)</span> and <span class="math">\(q'\)</span> are the correspondingly assigned code vectors. The loss, at a high level, has the following form <span class="math">\(\mathcal{L} = \ell(z, q') + \ell(z', q)\)</span>, i.e. it mixes them up. The code vectors can be <em>roughly</em> thought of as the canonical variables from CCA.</p>
<h3>The Masked Image Modeling Family</h3>
<p>The idea of image inpainting, where a large part of the image is greyed-out and then predicted from the remaining data, has been shown to yield strong results and to produce good features. An important model is <strong>BEiT</strong> (bidirectional encoder representation from image transformers) [12] which introduced the idea of masked image modeling (MIM), similar to how BERT is trained.</p>
<p>BEiT keeps two representations for the image - <em>image patches</em> and <em>visual tokens</em>. An image patch is simply a small square patch of pixels, flattened and linearly projected. They preserve the raw pixel values. Visual tokens are the equivalent of word tokens from NLP - a fixed number (8192) of discrete visual tokens are learned using a discrete variational autoencoder. The discrete nature makes the model non-differentiable so tricks like the Gumbel-softmax relaxation are used. This is the <em>pre</em>-pre-training part. The end result is a <em>tokenizer</em> which can take an image and produce a grid of <span class="math">\(14 \times 14\)</span> visual tokens.</p>
<p>Then, in the actual pre-training we have a transformer encoder which takes in image patches, of which some are masked. The transformer outputs a hidden representation for these masked patches and a softmax is applied. This corresponds to the probability that the visual token for a given masked patch corresponds to any of the predefined and learned visual tokens. In essence, the model maximizes the log-likelihood of the correct visual tokens. After the pre-training the model can be fine-tuned on downstream tasks.</p>
<p>BEiT is a complex model. <strong>MAE</strong> (masked autoencoders) [13] simplify it by directly reconstructing image patches instead of discrete tokens. MAE randomly selects visible image patches, passes them to an encoder, adds mask tokens and uses a lightweight decoder to reconstruct the whole image. <strong>SimMIM</strong> [14] also goes in the direction of simplifying the pipeline by having a single encoder and a single-layer prediction head.</p>
<figure>
    <img class='extra_big_img' src="/images/ssl_families.svg" alt="SSL Families" width="1200">
    <figcaption>Figure 1: Simplified schematics of three popular SSL algorithm families. The contrastive learning one (left) learns features invariant to semantically-preserving transformations. The self-distillation family (middle) learns features by distilling information across lagged versions of the same network. The CCA approach (right) learns features by regularizing the learned representations. Green boxes represent data. Yellow boxes are learnable components. Red boxes are operations. The three concrete models depicted are SimCLR, BYOL, and VICReg.</figcaption>
</figure>

<h3>References</h3>
<p>[1] Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. <a href="https://arxiv.org/abs/1807.03748">Representation learning with contrastive predictive coding.</a> <em>arXiv preprint arXiv:1807.03748</em> (2018).<br>
[2] Chen, Ting, et al. <a href="https://proceedings.mlr.press/v119/chen20j.html">A simple framework for contrastive learning of visual representations.</a> <em>International conference on machine learning</em>. PMLR, 2020.<br>
[3] Dwibedi, Debidatta, et al. <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Dwibedi_With_a_Little_Help_From_My_Friends_Nearest-Neighbor_Contrastive_Learning_ICCV_2021_paper.html">With a little help from my friends: Nearest-neighbor contrastive learning of visual representations.</a> <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 2021.<br>
[4] Koohpayegani, Soroush Abbasi, Ajinkya Tejankar, and Hamed Pirsiavash. <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Koohpayegani_Mean_Shift_for_Self-Supervised_Learning_ICCV_2021_paper.html">Mean shift for self-supervised learning.</a> <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 2021.<br>
[5] Grill, Jean-Bastien, et al. <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf">Bootstrap your own latent-a new approach to self-supervised learning.</a> <em>Advances in neural information processing systems 33</em> (2020): 21271-21284.<br>
[6] Chen, Xinlei, and Kaiming He. <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Exploring_Simple_Siamese_Representation_Learning_CVPR_2021_paper.html">Exploring simple siamese representation learning.</a> <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 2021. <br>
[7] Caron, Mathilde, et al. <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html">Emerging properties in self-supervised vision transformers.</a> <em>Proceedings of the IEEE/CVF international conference on computer vision</em>. 2021.<br>
[8] He, Kaiming, et al. <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html">Momentum contrast for unsupervised visual representation learning.</a> <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 2020.<br>
[9] Bardes, Adrien, Jean Ponce, and Yann LeCun. <a href="https://arxiv.org/abs/2105.04906">Vicreg: Variance-invariance-covariance regularization for self-supervised learning.</a> <em>arXiv preprint arXiv:2105.04906</em> (2021).<br>
[10] Zbontar, Jure, et al. <a href="https://proceedings.mlr.press/v139/zbontar21a.html">Barlow twins: Self-supervised learning via redundancy reduction.</a> <em>International Conference on Machine Learning</em>. PMLR, 2021.<br>
[11] Caron, Mathilde, et al. <a href="https://proceedings.neurips.cc/paper/2020/hash/70feb62b69f16e0238f741fab228fec2-Abstract.html">Unsupervised learning of visual features by contrasting cluster assignments.</a> <em>Advances in neural information processing systems 33</em> (2020): 9912-9924.
[12] Bao, Hangbo, et al. <a href="https://arxiv.org/abs/2106.08254">Beit: Bert pre-training of image transformers.</a> <em>arXiv preprint arXiv:2106.08254</em> (2021).<br>
[13] He, Kaiming, et al. <a href="https://openaccess.thecvf.com/content/CVPR2022/html/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper">Masked autoencoders are scalable vision learners.</a> <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 2022.<br>
[14] Xie, Zhenda, et al. <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xie_SimMIM_A_Simple_Framework_for_Masked_Image_Modeling_CVPR_2022_paper.html">Simmim: A simple framework for masked image modeling.</a> <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2022.  </p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = "https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML";

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'], ['$', '$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div>
  
  <p class="post-meta">
    Tag: ai
  </p>

  </article>


    </div>
  </main>


<footer class="site-footer h-card">
  <div class="wrapper">
  
  <p></p>
  

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">


    <div class="footer-col-wrapper">

      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">The Critical Section</li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          <li><a href="https://github.com/aceofgreens"><i class="fa fa-github"></i> GitHub</a></li>


        </ul>
      </div>


      <div class="footer-col footer-col-3">
        <p>A personal blog for artificial intelligence and similar topics.</p>
      </div>
    </div>

  </div>
</footer>

<script type="text/javascript">
function welcomeFunction() {
  var items = document.getElementsByTagName("code");
    for (var i = items.length; i--;) {
      items[i].setAttribute("class", "highlight");
  }
}
</script>

</body>
</html>