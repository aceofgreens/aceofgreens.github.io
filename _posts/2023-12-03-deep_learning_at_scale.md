---
layout: post
title: Deep Learning at Scale
date: 2023-12-03 07:00:00 +0200
tags: [ai]
# excerpt_separator: <!--more-->
---

As the scale of my ML projects started to increase, I have found myself in need of understanding more and more the actual engineering aspects around the models. I am perfectly fine with deep learning being an engineering-first discipline, rather than a theory-first one. Yet there are considerably more high-quality sources about the algorithms than about the implementation tricks needed. This post attempts to summarize important knowledge useful in understanding how to scale the modern deep learning approach to larger datasets, more nodes, and across different accelerators. Most likely, this is the first of multiple posts on this matter.

To start, I think it pays to understand what happens when a forward pass is executed. And for that, let's explore a bit how PyTorch works under the hood. Ultimately, calculating the outputs of a neural network natively in Python will be too slow, due to the interpreter wasting too much time decoding every single line. Thus, it is natural that at some lower level the critical computations be executed in a faster language. Consider the forward pass of the `nn.Conv2d` module from PyTorch. Here's what happens:

1. The source code of `nn.Conv2d` is actually a thin wrapper storing the weights, bias, and any other parameters. For the actual forward pass, `F.conv2d` is called.
2. Now my Python debugger starts skipping this call when I try to step into it, indicating that `F.conv2d` is a binary file. Nonetheless, we can find the source code, currently at `pytorch/aten/src/ATen/native/Convolution.cpp`. Looking at the commit tagged `v2.0.0`, the function `conv2d` takes in various parameters like input, weight, bias, stride, padding, and dilation, performs some basic checks, and calls `at::convolution`.
3. In turn, `at::convolution` reads a global context, specifically whether the underlying CuDNN module should be in deterministic mode. It then calls `at::_convolution`.
4. This function extracts all the convolution arguments into a small container and checks whether all weights and biases are consistent with the input size. Next, it selects a backend - one of the libraries optimized for the specific hardware that is available at execution time - e.g. Nvidia GPUs offer a specific library, CuDNN, for highly optimized neural net calculations. Alternative backends available are CPU (including whether to use various advanced features like [AVX](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions)), [CUDA](https://en.wikipedia.org/wiki/CUDA), [OpenMP](https://en.wikipedia.org/wiki/OpenMP), [MKL](https://en.wikipedia.org/wiki/Math_Kernel_Library). It is a great design choice to allow higher-level functions be agnostic to the underlying low-level calculations.
5. Let's suppose that CuDNN has been selected as a backend. Then the subsequent function is `at::cudnn_convolution` which itself calls `at::cudnn_convolution_forward`. The latter checks whether all inputs and weights are on the same GPU and calls `raw_cudnn_convolution_forward_out`. This is where it gets particularly interesting. Here, depending on the precision we may call `raw_cudnn_convolution_forward_out_32bit` which itself, finally, calls the `cudnnConvolutionForward` library function. Ultimately, CuDNN is a closed-source library and we can't inspect any deeper than this. But we can actually choose which convolution algorithm to use from either an implicit general matrix-matrix algorithm (GEMM) or a transform-based algorithm such as FFT or Winograd. If the proper flag is enabled, the library can run different algorithms to benchmark their performance and choose the best one for future use on similarly sized inputs. Alternatively, an algorithm can be chosen based on heuristics. After that, we get the convolution output.

Hence, one can see that executing even a simple module requires traversing a deep call stack, the necessity of which comes from the requirement to support multiple backends, each optimized for a different hardware configuration. Moreover, some of the fastest algorithms are non-deterministic and may produce slightly different results compared to the deterministic variants. Libraries like CuDNN offer the possibilities to use only deterministic algorithms with/or without benchmarking for selecting the best one. I find it fascinating how much [analysis](https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html) can go in trying to squeeze out as much performance as possible from these low-level components.


