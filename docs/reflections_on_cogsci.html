<!DOCTYPE html>
<html lang="en">

<head>
    <title>Reflections on CogSci | The Critical Section</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Pelican" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <link rel="stylesheet" href="/assets/main.css" />
    <link rel="icon" href="/assets/favicon.png" />
    <link href="https://fonts.googleapis.com/css?family=Roboto|Oswald|Open+Sans" rel="stylesheet">


    <meta name="description" content="CogSci2024 was a blast. Lots of interesting talks and posters. It was particularly nice to see how the cogsci community approaches certain questions differently from the ML people and yes, it's quite different, even the methodology seems to be different. From what I saw cogsci people tend to engage more in the classic scientific method - hypothesis creation, implementation, and experiments, conlusions. With ML, deep learning specifically, it's more of an engineering approach, where the emphasis is on practical results and proper benchmarking. This is what we'll discuss here. All in all, the conference was great. I presented some ideas, made new friends, learned some things, disagreed on others, explored Rotterdam (the host city) and left excited and with new ideas." />

    <meta name="tags" content="rl" />

</head>

<body onload="welcomeFunction()" >

  <header class="site-header" role="banner">
    <div class="wrapper">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> <!--An icon library for the button icon-->

      <a class="site-title" rel="author" href="/">The Critical Section</a>

      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
        </label>

        <div class="trigger">
            <a class="page-link" href="/about.html">About</a>
            <a class="page-link" href="/posts.html">Posts</a>
            <a class="page-link" href="/tags.html">Tags</a>
        </div>

      </nav>

    </div>
  </header>


  <main class="page-content" aria-label="Content">
    <div class="wrapper">

  <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <header class="post-header">
      <h1 class="post-title p-name" itemprop="name headline">Reflections on CogSci</h1>
      <p class="post-meta">
        <time class="dt-published" datetime="2024-07-30T07:00:00+02:00" itemprop="datePublished">
          30 Jul 2024
        </time>
      </p>

    </header>

    <div class="post-content">
      <p><a href="https://cognitivesciencesociety.org/cogsci-2024/">CogSci2024</a> was a blast. Lots of interesting talks and posters. It was particularly nice to see how the cogsci community approaches certain questions differently from the ML people and yes, it's <em>quite</em> different, even the methodology seems to be different. From what I saw cogsci people tend to engage more in the classic scientific method - hypothesis creation, implementation, and experiments, conlusions. With ML, deep learning specifically, it's more of an engineering approach, where the emphasis is on practical results and proper benchmarking. This is what we'll discuss here. All in all, the conference was great. I presented some ideas, made new friends, learned some things, disagreed on others, explored Rotterdam (the host city) and left excited and with new ideas.</p>
<p>My strategy here is to briefly cover multiple topics at a relatively superficial level. This still has the benefit that one learns what are the main questions and approaches, and can therefore look up the details later on. </p>
<h3>Workshops</h3>
<p>First, an interesting workshop was <a href="https://coggraph.github.io/">CogGraph</a> - a mixture of cognitive science and computer graphics. There are many graphics programs which are meant to be "consumed" by people, and in that case what is rendered has to go through our perception, which has peculiar biases and heuristics. Therefore, a question arises: how do we adapt the graphics pipelines or the visual parts in an application so as to maximize some effect on the user? For example, it matters how you present visual results, as some graphs have naturally salient visual features which are quickly picked up by us. And in those cases it matters whether the salient features (trendlines, clusters, spatial distributions) in your presentation are aligned to highlight accurately the underlying data or not. This can be used to produce intuitive memorable plots or to intentionally confuse others.</p>
<p>Another example is with graphics simulations. Modern renderers are becoming quite sophisticated - they are <a href="https://github.com/NVlabs/nvdiffrast">modular</a>, efficient, and often <a href="https://github.com/mitsuba-renderer/mitsuba3">differentiable</a>, so you can take derivatives of the entire simulation with respect to camera poses, geometry, <a href="https://en.wikipedia.org/wiki/Bidirectional_scattering_distribution_function">BSDFs</a>, and volumes. And the question of interest is basically how do humans reconstruct the world around us, that is, how do we estimate positional relationships, material properties, physically-realistic object interactions? This is sometimes called "inverse graphics" - given an image, how do we estimate the scene object properties that upon rendering give rise to that image?</p>
<p>Note that modern computer graphics have more or less achieved their longstanding dream of complete photorealism. In some tasks they are becoming superhuman, as we can simulate things which humans cannot even imagine (in sufficient details) - objects breaking into many pieces, particles interacting, knots twisting, nonlinear elastic deformations [1], objects warping [2]. But humans have other abilities which are still missing from software approaches, e.g. the ability to infer object stiffness from shape and motion cues [3], or the ability to "see" liquids from static snapshots [4]. A useful approach to bridging the gap between photorealistic graphics and intuitive humanlike inverse graphics has been to use 3D physics engines where inference of object properties is achieved using MCMC sampling [5]. All in all, this type of vision mixed with cognitive science has <em>many</em> interesting questions and is a joy to explore.</p>
<p>There was also a nice <a href="https://jacquespesnot.github.io/2024_CogSci_Workshop/">workshop on in-context learning</a>. This refers to learning from multiple input-output examples at test time. Instruction-tuned LLMs have shown the incredible value of such a capability. Note that when the prompt already contains a few examples for the task at hand the important information mixing happens at the level of individual tokens, not at the level of the weights. In various personal experiments I've found improved performance because of this. Another thing I realized is that with in-context learning, the fact that we have a few ready examples allows us to learn a small dynamics model and a critic over them. Once we have the dynamics and a critic we can do planning. This raises the question of whether the critic or the responder can be extracted in an implicit form, or if we can build actual planning on top of the network.</p>
<p>Humans have an extraordinary ability for in-context learning. Consider a sequence problem where we are given relations and letters.</p>
<div class="math">$$
L \rightarrow B \downarrow M \rightarrow M \downarrow T \leftarrow C \leftarrow E \uparrow F \uparrow \ ?
$$</div>
<p>It's difficult to say what symbol stands at the location of the <span class="math">\(?\)</span>. Yet, if we picture a two-dimensional grid where we start at <span class="math">\((0, 0)\)</span> and move according to the arrows, encountering the letters as we go, it becomes clear that the answer is <span class="math">\(L\)</span>, as we have returned to the starting point.</p>
<p>The hippocampus and the prefrontal cortex are engaged with such spatial reasoning. Moreover, spatial is meant in an abstract sense, as we can compare different things semantically, which also involves an abstract "spatial" comparison. The neurons in the hippocampus represent your current location, as evidenced by the fact that they light up when encountering particular locations. This results in well-defined "grid", "place", or "border" cells. The prefrontal cortex also encodes positional information but differently, displaying joint past, present, and future sequence features, to be explained shortly. </p>
<p>To solve a task like the one above, one needs to have an abstract relational schema, which represents the structure of the problem, and memory, which stores sensory observations (the letters) associated with the spatial locations. To predict the next sensory observation, one has to do <em>path integration</em> to trace the observed sequence onto the structural graph. Now, an RNN with external memory, some (key, value) store implemented as a Hopfield network, can solve this task. In its hidden state it learns to represent positional features in a manner that generalizes across tasks. The association between current position and sensory observation is stored in the (key, value) episodic memory. Based on this there is an argument that the hippocampus works like a RNN with external memory.</p>
<p>The case with the prefrontal cortex (PFC) is trickier. It is generally thought that unlike episodic memories in the hippocampus, the PFC stores sequence memories in the dynamics of neural activity (the exact patterns in neuronal firings). Can we model this setup using a RNN without external memory? Yes. First, we can think of total neural activity as a bunch of <em>slots</em>. And to predict the next observation we have to read-out from one of these slots, one which is fixed. Therefore, to predict a different observation we need to swap the content from a different slot into this fixed readout slot. The RNN thus learns to dynamically copy and shifts slot contents across different slots. Intuitively, the hidden state and the transition matrix are bigger because we are now tracking the relative position <em>to</em> each observation, rather than abstract task position. This is a theory of how PFC spatial reasoning works [6]. There are ways to compare RNN and human PFC activations.</p>
<p>There was also a cool workshop on compositionality in brains and machines. Compositionality refers to the ability to hierarchically compose concepts, obtaining newer concepts with altered meaning - obviously a crucial aspect of intelligence. In LLMs for example compositionality is achieved by mixing token features using self-attention. This kind of mixing can happen either step by step at generation time, or with many tokens in parallel, such as when processing the prompt. In humans I'd say it's still unclear. It's common to study how brain activity changes when we process a sentence slowly word by word vs when we process it in parallel, such as when a short sentence is shown only for a split second before it disappears. But not only word meanings compose, visual patterns and action sequences also compose.</p>
<h3>Talks</h3>
<p>There were multiple interesting talks, but my favourite was probably Alison Gopnik's talk. According to her, there are three different cognitive capacities that trade-off against each other - exploration, exploitation, and care. Exploitation is dominant during adulthood, when we strive to achieve goals and maximize utilities. Exploration is dominant in our infancy and child years. Here our intelligence is directed towards trying and figuring out things. Common strategies are trial-and-error, and learning by doing. This is the area of intrinsic motivation, open-ended learning, even developmental robotics. But we're not really interested in learning <em>everything</em> about the world - just that which can be affected by our own actions.</p>
<p>This idea is quite profound. Our early-years curiosity is guided toward learning causal models of the world. To learn such models we need <em>interventions</em> (the <code>do</code> operator) and that's precisely why infants are hardcoded to act almost randomly, moving all the time, trying out stuff, in other words physically acting. All these interventions allow us to learn how the world changes following our actions and is precisely the idea of <em>empowerment</em> in RL - the behavioural bias which leads us to select actions likely to have larger rather than smaller impact on the environment.</p>
<p>However, exploration is costly. By definition it means not optimizing and not providing. You need somebody to take care of you while you explore. That's why there's the third type of cognitive drive - care, predominantly in the elderly. Evolution has developed this drive for obvious reasons. And while the talk was cheerful and engaging, my thoughts on this subject are much more depressing. The fact is that as we age, we become less curious, less creative, less open-minded. Our brains become less malleable. Our world models become more crystalized and less fluid. We stop recognizing new patterns. Learning stops and inference settles in. That's why even though the life expectancy increases, our <em>effective</em> lifespan is <em>much much</em> shorter.</p>
<p>There were other interesting talks on various topics like deep learning as a basis of cognition, lots of stuff on LLMs, predictive coding, autotelic goal-generating agents, etc. I realized that humans are not <em>fully</em>-autotelic. We can choose short- and long-term goals but we can't really choose what gives us biochemical rewards (almost impossible to rewire your dopaminergic circuits). Similarly, the brain, I think, does not do anything more than pattern matching. In that sense I think I am a "pattern purist". Sometimes I hear claims about the brain doing "combinatorial" reasoning, but what does that even look like? I am skeptical because I can't picture how anything other than pattern matching can be implemented in the substrate of our neurons.</p>
<p>In general, there were a lot of talks that place LLMs in a cognitive context and try to compare with humans. The general approach is: you run some carefully-designed cognitive experiment on human subjects and analyse their results, then you run it on a LLM. If its results have similar patterns to those of the human subjects, we can conclude that the LLM has similar biases, judgements, risk profiles, and whatnot. Naturally, this treats the LLM as a black box, only looking at its inputs and outputs. It does not utilize the fact that we can actually build these complicated systems to test our hypotheses. After all, AI is the only playground that allows us to build models of human cognition and test them out. Thus, I don't find this approach particularly interesting because it has the same empirical methodology as social science studies, except that the problem setting is not social, but rather has at least something to do with human reasoning.</p>
<p>ML and cogsci seem to have different methodologies. Cogsci is based on the scientific method - design experiments, collect results, fit a theory around them. Such theories offer mostly theoretical explanations about human behaviour and reasoning. ML, at least fields like NLP or vision, has more of an engineering approach - invent new algorithms, build models, benchmark. It emphasizes not understanding per se, but practical results. And it's easier to extract value from practical results as opposed to theoretical ones. This is where the free market comes in - it takes the research output and adapts it for value extraction by productizing it.</p>
<h3>My Project</h3>
<p>I presented a poster on how to build RL agents that support a subjective virtual narrative about themselves. This is what we refer to as consciousness in humans. It's obviously a very reductionist approach. Even though I've had very good discussions with people who are non-reductionists as time passes I'm becoming more and more reductionist. After all, this <em>is</em> the way to understand things - you break them apart into primitives and learn how they compose. If something meaningful is missing from the model, you add it, and you still have <em>a model</em>. There is no reason to believe that the concept in question is anything other than the explanation offered by your model. If you claim so without scientific evidence, I can disprove it without scientific envidence. No one I've spoken with has been able to refute this logic so far.</p>
<p>I had very nice discussion at the poster session. People were genuinely curious and interested. They found it novel and I was happy explaining it. One person said <em>"But it's still missing phenomenological experiences"</em> and I was like <em>"What does that mean? You know that's not a healthy scientific term, right?"</em>. I also found out who my meta-reviewer was. In general, I wish some of the legendary scientists who were there (like Josh T.) saw my poster but, alas, they were busy.</p>
<p>In terms of how my interests evolve, I feel that learning about neuroscience and cognitive science, while definitely interesting, has only increased my appreciation towards computer science and engineering as a whole. AI is what I want to do right now and I'd be more than happy to devote a large part of my life to it. You see, cogsci has a fundametal limit, that of the capability of the human brain, whereas AI is limitless. You can go beyond what's biologically possible... to incredible results.</p>
<h3>The Trip</h3>
<p>Finally, some observations on Rotterdam. It's a great city, very modern, with a beautiful skyline and a strong maritime feel. The arrival was tough - two flights and a train from Schiphol to Rotterdam. But the remaining free time that day was worth it. The maritime museum was pretty good. It had one section decorated like an offshore drilling rig. You put on a safety vest and climb up some stairs, reaching a closed room with guardrails and huge video walls playing animations of a thunderous grey sky and a stormy sea. There were lots of miniature models of specialized drilling and support ships and it was great to learn above how wind turbines are installed, how oil is extracted, or how a <a href="https://en.wikipedia.org/wiki/Remotely_operated_underwater_vehicle">remotely operated underwater vehicle</a> (ROUV) is handled.</p>
<p>Outside there's a great harbour and you can climb onto some of the ships. Further down there's a place with artificial waves in one of the canals where people can surf. I saw a goose casually walking on the street there and crossing like a human. In general the city is quite nice, there are lots of canals with clear green water and kelp sticking out of the seafloor. The architecture is modern because the city was rebuilt after all the WW2 bombing. The <a href="https://en.wikipedia.org/wiki/Erasmusbrug">Erasmusbrug</a> and the <a href="https://en.wikipedia.org/wiki/Depot_Museum_Boijmans_Van_Beuningen">Depot Boijmans Van Beuningen</a> were great to explore. But the absolute highlight was the <a href="https://en.wikipedia.org/wiki/Euromast">Euromast</a> tower with its gorgeous view and nice club beats. From the top one can see both the downtown and the docklands. Foodwise, one cannot complain. There are Dutch restaurants, English pubs, Asian places. We ate a 6-plate meal in a chic Asian-fusion restaurant and paid only 45 euro ppax. In the closing day of the conference there was some strange summer carnival and the streets were busy. But my plan was to go to Amsterdam.</p>
<p>Amsterdam is a crazy city. Very beautiful but also crushingly crowded with people of all backgrounds and nationalities. I only spent around 24 hours there but I got a good sense of the ambience. It has stunning picturesque canals and great museums (e.g. Van Gogh, <a href="https://en.wikipedia.org/wiki/Rijksmuseum">Rijksmuseum</a>). My hotel was far enough from the city center to allow for a good amount of walking, while still being close enough to see attractions like the <a href="https://en.wikipedia.org/wiki/Royal_Palace_of_Amsterdam">Royal Palace</a> along the way. Some of the other attractions are not for blogging<span class="math">\(^*\)</span>. All in all, the place definitely has a very unique feel to it. I like it. And it was a worthy conclusion to the trip.</p>
<figure>
    <img class='extra_big_img' src="/images/euromast.jfif" alt="View from Euromast" width="1200">
    <figcaption>Figure 1: The view from Euromast. </figcaption>
</figure>

<h3>References</h3>
<p>[1] Li, Minchen, et al. <a href="https://par.nsf.gov/servlets/purl/10170679">Incremental potential contact: intersection-and inversion-free, large-deformation dynamics.</a> ACM Trans. Graph. 39.4 (2020): 49. <br>
[2] Mehta, Ishit, Manmohan Chandraker, and Ravi Ramamoorthi. <a href="https://link.springer.com/chapter/10.1007/978-3-031-20086-1_41">A level set theory for neural implicit evolution under explicit flows.</a> European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.<br>
[3] Paulun, Vivian C., et al. <a href="https://pdfs.semanticscholar.org/e89f/e963e13269274c17dbe1c5d263d9fff254de.pdf">Shape, motion, and optical cues to stiffness of elastic objects.</a> Journal of vision 17.1 (2017): 20-20. <br>
[4] Paulun, Vivian C., et al. <a href="https://www.sciencedirect.com/science/article/pii/S0042698915000401">Seeing liquids from static snapshots.</a> Vision research 115 (2015): 163-174.  <br>
[5] Wu, Jiajun, et al. <a href="https://proceedings.neurips.cc/paper/2015/hash/d09bf41544a3365a46c9077ebb5e35c3-Abstract.html">Galileo: Perceiving physical object properties by integrating a physics engine with deep learning.</a> Advances in neural information processing systems 28 (2015). <br>
[6] Whittington, James CR, et al. <a href="https://www.biorxiv.org/content/biorxiv/early/2024/03/04/2023.11.05.565662.full.pdf">On prefrontal working memory and hippocampal episodic memory: Unifying memories stored in weights and activity slots.</a> bioRxiv (2023): 2023-11.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = "https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML";

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'], ['$', '$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div>
  
  <p class="post-meta">
    Tag: rl
  </p>

  </article>


    </div>
  </main>


<footer class="site-footer h-card">
  <div class="wrapper">
  
  <p></p>
  

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">


    <div class="footer-col-wrapper">

      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">The Critical Section</li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          <li><a href="https://github.com/aceofgreens"><i class="fa fa-github"></i> GitHub</a></li>


        </ul>
      </div>


      <div class="footer-col footer-col-3">
        <p>A personal blog for artificial intelligence and similar topics.</p>
      </div>
    </div>

  </div>
</footer>

<script type="text/javascript">
function welcomeFunction() {
  var items = document.getElementsByTagName("code");
    for (var i = items.length; i--;) {
      items[i].setAttribute("class", "highlight");
  }
}
</script>

</body>
</html>