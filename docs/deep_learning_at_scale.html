<!DOCTYPE html>
<html lang="en">

<head>
    <title>Deep Learning at Scale | The Critical Section</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Pelican" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <link rel="stylesheet" href="/assets/main.css" />
    <link rel="icon" href="/assets/favicon.png" />
    <link href="https://fonts.googleapis.com/css?family=Roboto|Oswald|Open+Sans" rel="stylesheet">


    <meta name="description" content="As the scale of my ML projects started to increase, I have found myself in need of understanding more and more the actual engineering aspects around the models. I am perfectly fine with deep learning being an engineering-first discipline, rather than a theory-first one. Yet there are considerably more high-quality sources about the algorithms than about the implementation tricks needed. This post attempts to summarize important knowledge useful in understanding how to scale the modern deep learning approach to larger datasets, more nodes, and across different accelerators. Most likely, this is the first of multiple such posts." />

    <meta name="tags" content="ai" />
    <meta name="tags" content="cs" />

</head>

<body onload="welcomeFunction()" >

  <header class="site-header" role="banner">
    <div class="wrapper">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> <!--An icon library for the button icon-->

      <a class="site-title" rel="author" href="/">The Critical Section</a>

      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
            <a class="page-link" href="/about.html">About</a>
            <a class="page-link" href="/posts.html">Posts</a>
            <a class="page-link" href="/tags.html">Tags</a>
        </div>

      </nav>

    </div>
  </header>


  <main class="page-content" aria-label="Content">
    <div class="wrapper">

  <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <header class="post-header">
      <h1 class="post-title p-name" itemprop="name headline">Deep Learning at Scale</h1>
      <p class="post-meta">
        <time class="dt-published" datetime="2023-12-03T07:00:00+02:00" itemprop="datePublished">
          3 Dec 2023
        </time>
      </p>

    </header>

    <div class="post-content">
      <p>As the scale of my ML projects started to increase, I have found myself in need of understanding more and more the actual engineering aspects around the models. I am perfectly fine with deep learning being an engineering-first discipline, rather than a theory-first one. Yet there are considerably more high-quality sources about the algorithms than about the implementation tricks needed. This post attempts to summarize important knowledge useful in understanding how to scale the modern deep learning approach to larger datasets, more nodes, and across different accelerators. Most likely, this is the first of multiple such posts.</p>
<h3>Implementation</h3>
<p>To start, I think it pays to understand what happens when a forward pass is executed. And for that, let's explore a bit how PyTorch works under the hood. Ultimately, calculating the outputs of a neural network natively in Python will be too slow, due to the interpreter wasting too much time decoding every single line. Thus, it is natural that at some lower level the critical computations be executed in a faster language. Consider the forward pass of the <code>nn.Conv2d</code> module from PyTorch. Here's what happens:</p>
<ol>
<li>The source code of <code>nn.Conv2d</code> is actually a thin wrapper storing the weights, bias, and any other parameters. For the actual forward pass, <code>F.conv2d</code> is called.</li>
<li>Now my Python debugger starts skipping this call when I try to step into it, indicating that <code>F.conv2d</code> is a binary file. Nonetheless, we can find the source code, currently at <code>pytorch/aten/src/ATen/native/Convolution.cpp</code>. Looking at the commit tagged <code>v2.0.0</code>, the function <code>conv2d</code> takes in various parameters like input, weight, bias, stride, padding, and dilation, performs some basic checks, and calls <code>at::convolution</code>.</li>
<li>In turn, <code>at::convolution</code> reads a global context, specifically whether the underlying CuDNN module should be in deterministic mode. It then calls <code>at::_convolution</code>.</li>
<li>This function extracts all the convolution arguments into a small container and checks whether all weights and biases are consistent with the input size. Next, it selects a backend - one of the libraries optimized for the specific hardware that is available at execution time - e.g. Nvidia GPUs offer a specific library, CuDNN, for highly optimized neural net calculations. Alternative backends available are CPU (including whether to use various advanced features like <a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions">AVX</a>), <a href="https://en.wikipedia.org/wiki/CUDA">CUDA</a>, <a href="https://en.wikipedia.org/wiki/OpenMP">OpenMP</a>, <a href="https://en.wikipedia.org/wiki/Math_Kernel_Library">MKL</a>. It is a great design choice to allow higher-level functions be agnostic to the underlying low-level calculations.</li>
<li>Let's suppose that CuDNN has been selected as a backend. Then the subsequent function is <code>at::cudnn_convolution</code> which itself calls <code>at::cudnn_convolution_forward</code>. The latter checks whether all inputs and weights are on the same GPU and calls <code>raw_cudnn_convolution_forward_out</code>. This is where it gets particularly interesting. Here, depending on the precision we may call <code>raw_cudnn_convolution_forward_out_32bit</code> which itself, finally, calls the <code>cudnnConvolutionForward</code> library function. Ultimately, CuDNN is a closed-source library and we can't inspect any deeper than this. But we can actually choose which convolution algorithm to use from either an implicit general matrix-matrix algorithm (GEMM) or a transform-based algorithm such as FFT or Winograd. If the proper flag is enabled, the library can run different algorithms to benchmark their performance and choose the best one for future use on similarly sized inputs. Alternatively, an algorithm can be chosen based on heuristics. After that, we get the convolution output.</li>
</ol>
<p>A similar design is used with all other modules, for both forward and backward calls. For example, the considerably simpler <code>F.grid_sample</code> calls <code>at::grid_sampler</code>, which calls <code>cudnn_grid_sampler</code>, which calls the CuDNN function <code>cudnnSpatialTfSamplerForward</code>. Likewise, for the backward there is <code>cudnnSpatialTfSamplerBackward</code>.</p>
<p>Hence, one can see that executing even a simple module requires traversing a deep call stack, the necessity of which comes from the requirement to support multiple backends, each optimized for a different hardware configuration. Moreover, some of the fastest algorithms are non-deterministic and may produce slightly different results compared to the deterministic variants. Libraries like CuDNN offer the possibilities to use only deterministic algorithms with/or without benchmarking for selecting the best one. I find it fascinating how much <a href="https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html">analysis</a> can go in trying to squeeze out as much performance as possible from these low-level components.</p>
<h3>Distributed Training</h3>
<p>Larger datasets may require more than one GPU in order to train the model in a reasonable amount of time. The most common approach is by parallelizing the dataset samples across the GPUs and it falls in the SPMD category - <em>single program multiple data</em>. PyTorch offers a few different ways to achieve this.</p>
<p>The first one is <code>torch.nn.DataParallel</code> (DP). When initialized, this module wraps the model, retrieves the indices of all available GPUs, whose number let's say is <span class="math">\(G\)</span>, sets the output device and performs various checks. When the forward is called the input tensor is chunked along the batch dimension into <span class="math">\(G\)</span> parts of approximately the same size. The <span class="math">\(i\)</span>-th chunk is sent to the <span class="math">\(i\)</span>-th GPU in a process called <em>scattering</em>. Subsequently, the model is also replicated to those GPUs. This requires, in simple terms, the copying of the module's children (submodules), parameters and buffers (non-trainable parameters like the running mean and standard deviation for BatchNorms). The process is called <em>broadcasting</em> because a single object, the module, is sent to all participating devices. Subsequently, the model is called on all of them. This happens by literally creating a bunch of <code>threading.Thread</code>s in the context of which each model replica is ran. Finally, the predictions are <em>gathered</em> from all devices and concatenated.</p>
<p>In the backward pass, gradients flow back through the individual devices, after which they are gathered on the primary device and then summed/averaged. Overall, in this approach there is one process in which we replicate the model to all GPUs in every single forward pass. This does not take much time, since the GPUs belong to the same host. For the parallel forward pass, in principle Pythonâ€™s GIL can be a limiting factor for CPU-bound parallel tasks because it allows only one thread to execute Python bytecode at a time. However, for I/O-bound tasks or tasks that primarily involve executing compiled code (like PyTorch operations on GPUs), this is less of an issue.</p>
<p>PyTorch offers also <code>torch.nn.parallel.DistributedDataPrallel</code> (DDP) which is a significant generalization. Here instead of a single process, we will have one process per GPU, all the processes will form a process group, and they will synchronize the gradients via actual inter-process communication (IPC). This also allows training on multiple nodes and avoids any GIL contention.</p>
<p>The overall idea is that multiple processes run on multiple GPUs separately. Each process computes the forward pass only on a portion of the whole dataset. After the gradients are calculated, they are broadcasted to all other processes where they are averaged in a process called <code>AllReduce</code>, and applied to each model replica. This happens at every training iteration and prevents any divergence in the model parameters across the devices.</p>
<p>Broadcasting the local gradients to the other devices in a naive way is very slow, because this would require calling <code>AllReduce</code> on every single parameter tensor, many of which are of small size. It is much more efficient and fast to batch multiple tensors into larger <em>buckets</em> and <code>AllReduce</code> the buckets directly. Thus, when initializing the DDP wrapper, the module is replicated to all devices and subsequently each parameter is assigned to one bucket. The buckets are ordered and the parameters closer to the last layer are put in the first few buckets, while those from the first layer are put in the last few buckets. It is important that bucket <span class="math">\(i\)</span> on device <span class="math">\(p\)</span> contains exactly the same parameters as bucket <span class="math">\(i\)</span> on device <span class="math">\(q\)</span>.</p>
<p>The number of buckets can have a noticeable effect on performance. In principle, using as many buckets as parameter tensors defeats the purpose of using buckets in the first place. On the other hand, using a single bucket requires waiting for the backward pass to completely finish before launching the <code>AllReduce</code>. With any number of buckets in between, one can <strong>overlap the gradient computations with the asynchronous communications</strong>. </p>
<figure>
    <img class='img' src="/images/ddp.png" alt="DDP schematic" width="1200">
    <figcaption>Figure 1: A schematic showing the gradient synchronization by DDP. Image taken from <a href="https://arxiv.org/pdf/2006.15704.pdf">here</a>.</figcaption>
</figure>

<p>Gradients are populated from the last layer backwards. Hence, we can launch the <code>AllReduce</code> for one bucket as soon as all the gradients for those tensors which belong to it are computed, saving a lot of time in the process. In practice, DDP registers an autograd hook for each parameter tensor which is activated after the gradients for that tensor are computed. Within the hook the appropriate offset in the bucket is found and the gradients are copied in their location. Then, if all gradients in the bucket have been computed, the bucket is marked ready. Then, <code>AllReduce</code> is launched on all ready buckets in order. Finally, the reduction is <code>wait</code>-ed and the averaged gradients are written over the current gradients computed over the local replica's data batch.</p>
<p>There are some complications however. Consider that Torch supports dynamic computation graphs and suppose a model uses an <code>if</code> branch to select one of two possible layers to use for the forward pass. Naturally, only gradients for one of the layers will be computed. As a result, the buckets to which belong the parameters of the layer that was not used will never be marked ready... and training will hang. For that reason DDP offers the argument <code>find_unused_parameters</code> which will scan the parameters, find those which are not used and mark them as ready, so they don't hinder the communication.</p>
<p>It is also interesting how the <code>AllReduce</code> computation works. Suppose we have <span class="math">\(N\)</span> processes, each on a separate GPU device. Each device <span class="math">\(i\)</span> also holds <span class="math">\(M\)</span> gradients <span class="math">\( g_1^i, ..., g_M^i\)</span>. Assuming the reduction operation is addition, the goal is to compute the reduced gradients $ \sum_{i=1}^N g_1^i, ..., \sum_{i=1}^N g_M^i $ and to distribute them to all nodes.</p>
<p>In reality, the topology of how the nodes are connected informs the algorithm to use. Or better said, each possible algorithm relies on an assumption of how the devices are <em>logically</em> connected. In a star topology all gradients may be sent to one particular node which then reduces them and distributes the result back. This is simple and fast, but it does not scale well due to communication bottlenecks in the main device. A tree topology offers better scalability but may have uneven bandwidth. Instead, the ring has become a good choice, offering increased bandwidth efficiency at the cost of slightly higher latency.</p>
<p>In a ring-based all-reduce the devices form a ring, each one communicating only with its neighbors on the left and right. We proceed as follows. Device <span class="math">\(i\)</span> cuts its gradients into <span class="math">\(N\)</span> parts, which we call <span class="math">\(\mathbf{b}_1^i, ..., \mathbf{b}_N^i\)</span>. It sends <span class="math">\(\mathbf{b}_i^i\)</span> to its neighbor on the right and similarly receives <span class="math">\(\mathbf{b}\_{i-1}^{i-1}\)</span> from its left neighbor. At the next step, device <span class="math">\(i\)</span> reduces the received gradients with its own corresponding chunk <span class="math">\(\mathbf{b}\_{i-1}^i\)</span> and sends the result <span class="math">\(\mathbf{b}\_{i-1}^{i-1} + \mathbf{b}\_{i-1}^i\)</span> to the right neighbor, while receiving a reduced chunk from its left neighbor. This process continues for <span class="math">\(N-1\)</span> steps until each device has one chunk which is fully reduced. Then, all that remains is for each device to distribute that chunk to the others. This takes <span class="math">\(N-1\)</span> additional steps after which the AllReduce has been completed.</p>
<figure>
    <img class='big_img' src="/images/ring_based_all_reduce.svg" alt="Ring-based AllReduce" width="1200">
    <figcaption>Figure 2: A <i>rough</i> outline of the ring-based AllReduce algorithm. There are three devices, shown as dashed boxes, whose gradients are colored red, green, and blue. Their local gradients are broken down into three chunks, shown as boxes. Their colors represent the state of the system at the current iteration. The arrows indicate which chunk will be sent at this iteration. After two steps, each device has one fully-reduced chunk. The last two steps simply broadcast the reduced tensors. Design inspired from <a url="https://images.nvidia.com/events/sc15/pdfs/NCCL-Woolley.pdf">here</a>.  </figcaption>
</figure>

<p>The ring-based approach is usually hidden from the end-user. One typically uses a library that already implements and optimizes all the collective communications between the devices. For example, NVidia offers <a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/overview.html">NCCL</a> which provides a convenient function <code>ncclAllReduce</code> that dynamically chooses which algorithm to use depending on the available hardware. Upon initialization, NCCL gathers information about the GPUs - how they are connected (e.g. PCIe or NVLink) and what are their bandwidth characteristics. Based on this, it determines which algorithm to select for the AllReduce. Moreover, the individual GPU-to-GPU data transfers are very quick, since they can avoid copying the data into main memory.</p>
<p>While NCCL may be the most efficient and fast library for NVidia GPUs, it is certainly not the most popular. The <a href="https://en.wikipedia.org/wiki/Message_Passing_Interface">Message Passing Interface</a> (MPI) has been the de-facto standard for parallel computing for decades. Most other major libraries are built on its concepts of <em>size</em>, <em>rank</em>, <em>local rank</em>, and the collective communication functions like AllReduce, AllToAll, and AllGather.</p>
<p>The idea is quite simple. Suppose we run a training job on 6 nodes, each with 8 GPUs. Then the total <em>world size</em> is 48, and the <em>local size</em> within each node is 8. Each GPU will be identified by its local rank from 0 to 7 within the node, and at the same time with its global rank from 0 to 47. All the common functions for point-to-point communication like <code>send</code>, <code>recv</code>, <code>isend</code>, <code>irecv</code> identify the devices by their ranks. </p>
<p>The <code>torch.distributed</code> package manages quite well to abstract away all the complexity related to the communication libraries. Like before, it refers to them as different backends which the user selects and sets the environment for. However, to coordinate the overall distributed training process, a higher-level communication package called <code>c10d</code> is used. It takes care of the overall training process.</p>
<p>In general, the script is started on all devices either manually by spawning all the processes or automatically by using a launcher like <code>torchrun</code>. After the processes (workers) are initialized, they need to find each other. Thus, <code>c10d</code> requires that the user set a master IP address and port (belonging to one of the nodes) where all the workers will rendezvous. Once they connect to the master address, they can identify each other, establish a process group, synchronize initially, and then begin training.</p>
<p>Since each worker will be training on a subset of the dataset, one needs to use a <code>DistributedSampler</code> in the dataloader. With <span class="math">\(N\)</span> GPUs, and a batch size of <span class="math">\(B\)</span>, the effective batch size per update will be <span class="math">\(NB\)</span>. Hence, a total of <span class="math">\(N\)</span> times fewer updates will be performed. It is also common to increase the learning rate <span class="math">\(N\)</span> times to keep the total distance travelled in weight space roughly the same.</p>
<p>Overall, this covers the basic elements of distributed training. There are many other interesting details I didn't cover, for example how to optimize the data loading process or how to debug and profile these systems. Additionally, the distributed training discussed here is entirely synchronous. Alternative approaches based on asynchronous parameter servers such as those in federated learning or in some reinforcement learning agents are also a big topic. In those settings there are other interesting problems such as gradients from one worker becoming stale before they are applied on the main server. But I'll leave these for a future post.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = "https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML";

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'], ['$', '$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div>
  
  <p class="post-meta">
    Tags: ai, cs
  </p>

  </article>


    </div>
  </main>


<footer class="site-footer h-card">
  <div class="wrapper">
  
  <p></p>
  

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">


    <div class="footer-col-wrapper">

      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">The Critical Section</li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          <li><a href="https://github.com/aceofgreens"><i class="fa fa-github"></i> GitHub</a></li>


        </ul>
      </div>


      <div class="footer-col footer-col-3">
        <p>A personal blog for artificial intelligence and similar topics.</p>
      </div>
    </div>

  </div>
</footer>

<script type="text/javascript">
function welcomeFunction() {
  var items = document.getElementsByTagName("code");
    for (var i = items.length; i--;) {
      items[i].setAttribute("class", "highlight");
  }
}
</script>

</body>
</html>