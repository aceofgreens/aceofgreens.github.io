<!DOCTYPE html>
<html lang="en">

<head>
    <title>Some Deep Learning Optimizers | The Critical Section</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Pelican" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <link rel="stylesheet" href="/assets/main.css" />
    <link rel="icon" href="/assets/favicon.png" />
    <link href="https://fonts.googleapis.com/css?family=Roboto|Oswald|Open+Sans" rel="stylesheet">


    <meta name="description" content="Gradient descent is by far the most dominant optimization algorithm in AI. It has proven itself capable of training even the biggest and baddest models out there - multi-billion parameter LLMs, small networks with complicated forward passes, and anything in between. It's the best optimization algorithm we have right now. It's not very biologically realistic, but it compensates by being able to train those models which can simulate biological realism. And the idea of simply following the steepest direction is as beautiful as it gets. Here we'll explore some of the most widely used deep learning optimizers." />

    <meta name="tags" content="cs" />
    <meta name="tags" content="ai" />

</head>

<body onload="welcomeFunction()" >

  <header class="site-header" role="banner">
    <div class="wrapper">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> <!--An icon library for the button icon-->

      <a class="site-title" rel="author" href="/">The Critical Section</a>

      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
            <a class="page-link" href="/about.html">About</a>
            <a class="page-link" href="/posts.html">Posts</a>
            <a class="page-link" href="/tags.html">Tags</a>
        </div>

      </nav>

    </div>
  </header>


  <main class="page-content" aria-label="Content">
    <div class="wrapper">

  <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <header class="post-header">
      <h1 class="post-title p-name" itemprop="name headline">Some Deep Learning Optimizers</h1>
      <p class="post-meta">
        <time class="dt-published" datetime="2023-04-22T07:00:00+02:00" itemprop="datePublished">
          22 Apr 2023
        </time>
      </p>

    </header>

    <div class="post-content">
      <p>Gradient descent is by far the most dominant optimization algorithm in AI. It has proven itself capable of training even the biggest and baddest models out there - multi-billion parameter LLMs, small networks with complicated forward passes, and anything in between. It's the best optimization algorithm we have right now. It's not very biologically realistic, but it compensates by being able to train those models which can simulate biological realism. And the idea of simply following the steepest direction is as beautiful as it gets. Here we'll explore some of the most widely used deep learning optimizers.</p>
<p>Gradient descent works by calculating the gradient (partial derivatives) of the loss function with respect to each parameter. The gradient gives the direction of the steepest increase in the function, and the algorithm updates the parameters by taking steps in the opposite direction of the gradient, which represents the steepest decrease in the function. This process is repeated iteratively until convergence is reached, such as when the absolute difference of the parameters from two sequential updates falls below some small value, or a stopping criterion is met, typically a maximum number of iterations.</p>
<p>Consider the following two-dimensional <a href="https://en.wikipedia.org/wiki/Rosenbrock_function">Rosenbrock function</a> which can be considered as a testing example:</p>
<div class="math">$$
F(x, y) = (a - x)^2 + b(y - x^2)^2. \\ 
$$</div>
<p>It is common to set <span class="math">\(a=1\)</span> and <span class="math">\(b=100\)</span>, after which the single global minimum is found at <span class="math">\((1, 1)\)</span>. The contour plot is shown in Figure 1. This function is useful for testing optimization algorithms because the global minimum is found in a very flat and long valley. Reaching this valley, but moving along it is difficult because the gradient is very weak there.</p>
<figure>
    <img class='img' src="/images/rosenbrock.png" alt="Rosenbrock" width="1200">
    <figcaption>Figure 1: The Rosenbrock function for testing optimization algorithms. The left plot shows the contour of the function for various levels. The orange star shows the minimum of the function. The right plot shows the function's corresponding 3D surface plot.</figcaption>
</figure>

<p><strong>Gradient descent.</strong> Let's rename <span class="math">\((x, y)\)</span> with <span class="math">\(\theta = (\theta_1, \theta_2)\)</span> to better indicate that <span class="math">\((x, y)\)</span> are the parameters <span class="math">\(\theta\)</span> that we are optimizing over. We write the gradient evaluated at <span class="math">\(\theta_t\)</span> as <span class="math">\(\nabla_\theta F(\theta_t)\)</span>. Then, standard gradient descent updates <span class="math">\(\theta\)</span> by taking a small step in the direction of the negative gradient:</p>
<div class="math">$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta F(\theta_t)
$$</div>
<p>Here <span class="math">\(\theta_t\)</span> are the parameters at iteration <span class="math">\(t\)</span>, which for the Rosenbrock function includes both <span class="math">\(x\)</span> and <span class="math">\(y\)</span>. We can refer to them individually using the notation <span class="math">\(\theta_{t, i}\)</span> where <span class="math">\(i \in \{ 1, 2 \}\)</span>. <span class="math">\(\eta\)</span> is the <em>learning rate</em>, also called the <em>step size</em>, and contributes to how big the change in the parameters is. The other thing which determines the change is the gradient <span class="math">\(\nabla_\theta F(\theta_t)\)</span> which causes large updates in regions where the loss landscape is steep and small updates otherwise. If the learning rate is sufficiently small, following the negative gradient will eventually converge to a local minimum. If the learning rate is large, the parameters may diverge to infinity or may start oscillating in a limit cycle.</p>
<p>In standard gradient descent the learning rate is constant both across iterations <span class="math">\(t\)</span> and across individual parameters <span class="math">\(i\)</span>. This setup is very simple but unfortunately has significant drawbacks. For functions which are steep along one direction, but flat along another, gradient descent will typically lead to very slow movement along the flat direction. This is the case with the Rosenbrock function. Our estimates reach the valley within just a few steps but after that the updates become tiny. Mathematically, the Hessian, the matrix of second derivatives, which shows the curvature in the neighborhood of the current point, has very different eigenvalues. One of them is very small, showing that the curvature of the loss landscape changes very slowly along the first eigenvector, in the direction of the minimum. And the other eigenvalue is very large (even hundreds of times larger), showing that the curvature changes rapidly in the transverse direction, along its corresponding eigenvector.</p>
<p><strong>Adagrad.</strong> From this it seems that we need a method which can adapt the individual learning rate depending on the parameter. <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad">Adagrad</a> was the first to propose such a modification [1].</p>
<div class="math">$$
\begin{cases}
\begin{align}
\theta_{t+1} &amp; = \theta_{t} - \frac{\eta}{\sigma_{t}} g_t \\
\sigma_t &amp;= \sqrt{\sum_{\tau = 0}^{t} g_t^2} \\
g_t &amp;= \nabla_{\theta} F(\theta_t)
\end{align}
\end{cases}
$$</div>
<p>Here the idea is to divide the learning rate <span class="math">\(\eta\)</span> by a parameter - a time-dependent vector <span class="math">\(\sigma_{t}\)</span> which scales the updates applied to each individual parameter. Note that <span class="math">\(\sigma_t\)</span> and <span class="math">\(g_t\)</span> are vectors of size equal to the number of parameters. All operations are applied element-wise. We keep the cumulative sum of the squared derivative of each parameter as the number of iterations grows. Why? Because dividing by the magnitude of the gradient gives us a large learning rate in flat regions and a small learning rate in steep ones. The accumulation of the squared gradients smoothes out this effect. Note that this accumulation never decreases. As a result, we get an automatic learning rate scheduler that decreases the updates in a monotone way. This can be addressed by changing it to the average, in which case the learning rate is reduced only when the current gradient is larger than the average past gradient.</p>
<p>Empirically, it has been noted that Adagrad works particularly well for problems where there are sparse gradients. For parameters whose gradient is large or dense (frequently non-zero) the sum will increase fast and hence the learning rate for that parameter, after dividing by the square root of the cumulative sum, will be decreased. On the other hand, for parameters whose gradients are sparse (contain mostly zeros), the cumulative sum will increase slower, and hence the learning rate will be larger. Sparse gradients typically occur when the inputs are sparse, when we are using sparsity-inducing <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">activations</a> or <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)">losses</a>, or when the outputs are sparse. </p>
<p><strong>RMSProp.</strong> That being said, one problem is that in a non-convex function the learning rate may decay too rapidly due to the accumulation of the gradients. If we are in one basin of attraction in the loss landscape, it is reasonable to want to forget the gradients collected from different regions. One improvement proposed in 2012, is Hinton's <em>Root Mean Squared Propogation</em> (<a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp">RMSProp</a>) optimizer:</p>
<div class="math">$$
\begin{cases}
\begin{align}
\theta_{t+1} &amp; = \theta_{t} - \frac{\eta}{\sigma_{t}} g_{t} \\
\sigma_{t} &amp;= \sqrt{ \alpha \sigma_{t-1}^2 + (1 - \alpha) g_{t}^2} \\
g_{t} &amp;= \nabla_{\theta} F(\theta_t)  \\
\sigma_{0} &amp;= | g_{0} |
\end{align}
\end{cases}
$$</div>
<p>In this variation the vector <span class="math">\(\sigma_{t}\)</span> is updated using a weighted average of the past learning rate modifiers <span class="math">\(\sigma_{t-1}\)</span> and the current gradient <span class="math">\(g_{t}\)</span>, i.e. <span class="math">\(\sigma_{t} = \sqrt{\alpha \sigma_{t-1}^2 + (1 - \alpha) g_t^2}\)</span>. Since <span class="math">\(\alpha \in (0,1)\)</span>, the weighted average ensures that some part of the previous gradients is forgotten, which also prevents the fast learning rate decay problem that Adagrad has.</p>
<p>Consider what happens with RMSProp when we enter the flat valley in our test function. At some point, when the previous gradients outside of the valley are forgotten, the current gradients will be <span class="math">\(g_{t} = (a, b)\)</span> and the previous sigma will be <span class="math">\(\sigma_{t-1} = (| a |, |b |)\)</span>. Then, as a result, the current sigma will be <span class="math">\(\sigma_{t} = (|a|, |b|)\)</span> and the update will be <span class="math">\(-\eta (a, b)/(|a|, |b|) = -\eta (\text{sgn}(a), \text{sgn}(b))\)</span>. If the valley is symmetric in the direction perpendicular to the gradient, it may happen that at the next iteration the update is the same, except the signs will be reversed. This causes oscillation in the parameter trajectory. The parameters will still move in the right direction, but they will oscillate from side to side, reducing the rate of convergence.</p>
<p><strong>Stochastic Gradient Descent.</strong> Virtually all problems that are encountered in deep learning have a loss function that is a sum or mean across many samples. This is completely different from our Rosenbrock test function where there is no data <span class="math">\(x_i\)</span>, only parameters to estimate. In deep learning the simplest loss function is the squared loss between the ground truth label for sample <span class="math">\(i\)</span>, <span class="math">\(y_i\)</span>, and the prediction <span class="math">\(\hat{y}(x_i, \theta)\)</span> that depends on the parameters <span class="math">\(\theta\)</span>:</p>
<div class="math">$$
\mathcal{L}(\theta) = \mathbb{E}_{x, y \sim \mathcal{D}} \big[(x - y)^2\big] \approx \frac{1}{n} \sum_{i = 1}^n (\hat{y}(\theta, x_i) - y_i)^2.
$$</div>
<p>We're trying to find the parameters <span class="math">\(\theta\)</span> such that the average squared error between our predictions <span class="math">\(\hat{y}(\theta, x_i)\)</span> and the labels <span class="math">\(y_i\)</span> is minimal. Gradient descent, obviously, requires us to compute the gradient of the loss function in order to do a single step. But here the gradient is given by </p>
<div class="math">$$\frac{2}{n} \sum_{i = 1}^n
(\hat{y}_i - y_i)  \nabla_{\theta} \hat{y}_i( \theta )$$</div>
<p>which contains a sum over all the samples in the dataset. Clearly, if the dataset is large, this sum will be too slow and expensive to compute. Instead what we can do is compute the gradient over a much smaller, random subset, or <em>minibatch</em> of the data samples. Since the samples are random, the gradient will be random as well. For that reason, gradient descent on small random subsets of the data is called stochastic gradient descent.</p>
<p>This is one of the hidden wonders of deep learning. The stochasticity actually helps with generalization, as the random movements introduced by it help to jump out of the basins of attraction of bad local minima. It's very hard to quantify this effect, but it's very easy to see it in practice. The batch size, or technically the size of the minibatch, is the parameter controlling how much stochasticity is introduced in the training and subsequently how likely it is that we jump around the true gradient, instead of following it. All else being equal, a larger batch size leads to less noise in the gradients, less generalization capability, but faster training. Finally, when increasing the batch size by a factor of <span class="math">\(k\)</span>, it is advisable to increase the learning rate by a factor of <span class="math">\(\sqrt{k}\)</span>, so as to keep the variance of the step size the same.</p>
<p><strong>Momentum.</strong> A separate improvement which is somewhat orthogonal to the discussion of the adaptive learning rate to different parameters is that of momentum, also known as the heavy ball method, by analogy with a ball which accelerates when it rolls downhill. The idea is that the parameters will now have a kind of velocity <span class="math">\(\Delta \theta_t = \theta_t - \theta_{t-1}\)</span>, dampened by a parameter <span class="math">\(0 &lt; \alpha &lt; 1\)</span>, which also acts as an exponential decay when it comes to forgetting previous velocities. The update is given by:</p>
<div class="math">$$
\begin{cases}
\begin{align}
\theta_{t+1} &amp;= \theta_t + v_t \\
v_t &amp;= \alpha v_{t-1} - \eta \nabla_\theta F(\theta_t) \\
v_0 &amp;= 0. 
\end{align}
\end{cases}
$$</div>
<p>In practice, momentum can speed up convergence significantly. In cases where the gradient is first steep and then flattens out, like in the Rosenbrock function, the parameters will continue to update by a lot after entering the flat region, and thus will cover a lot of distance in just a few steps. Of course, if the loss landscape is highly non-convex or jagged having too much momentum can cause the parameters to overshoot, similar to having a learning rate which is too large.</p>
<p>A further improvement is what's called <em>Nesterov momentum</em>. Here the entire update consists of two parts: first we make a big jump using the momentum <span class="math">\(\alpha v_t\)</span> and then perform a correction in the direction of the gradient <span class="math">\(-\eta \nabla_\theta F(\theta_t + \alpha v_t)\)</span> evaluated at the new location. Summing these two terms yields the full update:</p>
<div class="math">$$
\begin{cases}
\begin{align}
\theta_{t+1} &amp;= \theta_t + v_t \\
v_t &amp;= \alpha v_{t-1} - \eta \nabla_\theta F(\theta_t + \alpha v_{t-1}) \\
v_0 &amp;= 0.
\end{align}
\end{cases}
$$</div>
<p>Applying the gradient step after the momentum step acts as a kind of lookahead and works very well in practice. However, computing the gradient in a shifted position is a bit inconvenient. We can reparametrize in the following way. Let <span class="math">\(u_t = \theta_t + \alpha v_t\)</span>. Then <span class="math">\(u_t - \alpha v_t = \theta_t\)</span> and <span class="math">\(u_t - \alpha v_t + v_{t+1} = \theta_t + v_{t+1} = \theta_t + \alpha v_t - \eta \nabla_\theta F(u_t) = \theta_{t+1} = u_{t+1} - \alpha v_{t+1}\)</span>. After some simplification we get</p>
<div class="math">$$
\begin{cases}
\begin{align}
u_{t+1} &amp;= u_t - \alpha v_t + (1 + \alpha) v_{t+1} \\
v_{t+1} &amp;= \alpha v_t - \eta \nabla_\theta F(u_t) \\
v_0 &amp;= 0.
\end{align}
\end{cases}
$$</div>
<p>At this point <span class="math">\(u_t\)</span> can be simply renamed to <span class="math">\(\theta_t\)</span> again and we now have a version of the Nesterov momentum that does not require computing a shifted gradient. Compared to standard momentum, the Nesterov variant is can alleviate the overshooting that would result from setting the momentum parameter or the learning rate too large. By using information about the "future" gradient it can deal with oscillations somewhat better, converging faster to a minimum.</p>
<p><strong>Adam.</strong> Combining adaptive gradients with momentum yields the most popular and widely used optimizer in AI right now - adaptive moment estimation, or Adam [2]. Here, we'll keep a running estimate for the first and the second moment for the derivative of every single parameter. The running estimates are updated using an exponential decay, similar to RMSProp - <span class="math">\(m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t\)</span> for the first moment and <span class="math">\(v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\)</span> for the second one.</p>
<p>The first thing to do is to unroll the estimates for <span class="math">\(m_t\)</span> and <span class="math">\(g_t\)</span> and obtain a closed form for them:</p>
<div class="math">$$
\begin{align}
v_t &amp;= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\ 
&amp;= \beta_2^2 v_{t-2} + \beta_2(1 - \beta_2) g_{t-1}^2 + (1 - \beta_2) g_t^2 \\
&amp;= \beta_2^3 v_{t-3} + \beta_2^2(1-\beta_2) g_{t-2}^2 + \beta_2(1 - \beta_2) g_{t-1}^2 + (1 - \beta_2)g_t^2 \\
&amp;= (1 - \beta_2) \sum_{i = 0}^{t-1} \beta_2^i g_{t - i}^2.
\end{align}
$$</div>
<p>A similar expression holds also for <span class="math">\(v_t\)</span>. Now, it can happen, and most often does, that the gradients <span class="math">\(g_t\)</span> are random. This occurs if one is using stochastic gradient descent or if the task itself is stochastic - for example optimizing the policy parameters of an agent operating in a stochastic environment. In that case we are interested in the expectation of our moments. If we assume that the gradients have a stationary distribution so that <span class="math">\(\mathbb{E}[g_{t-1}^2] = \mathbb{E}[g_{t-2}^2] = ... \mathbb{E}[g_0^2]\)</span>, then for the expectation of the exponential moving average we get:</p>
<div class="math">$$
\mathbb{E}[v_t] = (1 - \beta_2^t) \mathbb{E}[g_t^2].
$$</div>
<p>It's clear that by dividing <span class="math">\(\hat{v}_t = v_t/(1 - \beta_2^t)\)</span> is an unbiased estimator for <span class="math">\(g_t^2\)</span>. The analogously bias-corrected <span class="math">\(\hat{m_t} = m_t/(1 - \beta_1^t)\)</span> acts as the momentum term which will add modify the behaviour of the parameters. Dividing the parameters by <span class="math">\(\sqrt{\hat{v_t}}\)</span> ensures that in steep regions of the loss landscape the step size will be decreased. Adam is then given by:</p>
<div class="math">$$
\begin{cases}
\begin{align}
g_t &amp;= \nabla_\theta F(\theta_t) \\
m_t &amp;= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
v_t &amp;= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\
\hat{m}_t &amp;= m_t / (1 - \beta_1^t) \\
\hat{v}_t &amp;= v_t / (1 - \beta_2^t) \\
\theta_{t + 1} &amp;= \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t}}.
\end{align}
\end{cases}
$$</div>
<p>In practice the default hyperparameter values are <span class="math">\(\beta_1 = 0.9\)</span> and <span class="math">\(\beta_2 = 0.999\)</span>. <span class="math">\(\beta_1\)</span> increases the effect that the acceleration from an initial steep gradient will have on the learning and it's a very useful parameter to tweak for loss landscapes which have wide flat regions. <span class="math">\(\beta_2\)</span> <em>roughly</em> controls how fast the step sizes of individual parameters are scaled with respect to changes in the curvature of the loss landscape.</p>
<p>It is common to use regularization alongside the original objective function - for example weight decay <span class="math">\(\lambda {\lVert \theta_t \rVert}^2_2\)</span>. If we add the regularization term naively in the total loss function, then the regularization will enter the <span class="math">\(m_t\)</span> and <span class="math">\(v_t\)</span> terms and they will start interacting in strange ways. Instead, the gradient of the regularization term should be added separately as another additive component to the updates of <span class="math">\(\theta_t\)</span> directly. This prompts the AdamW (adaptive moment estimation with weight decay) algorithm which is quite popular as well.</p>
<p><strong>Radam.</strong> Let's do some quick statistical analysis on the properties of Adam. In what follows it's useful to know that if <span class="math">\(X \sim N(0, 1)\)</span>, then <span class="math">\(X^2\)</span> has a Chi-squared distribution with <span class="math">\(1\)</span> degree of freedom, <span class="math">\(X^2 \sim \chi^2(1)\)</span>. Similarly, <span class="math">\(1/X \sim \text{Inv}\)</span>-<span class="math">\(\chi^2(1)\)</span> - the inverse chi squared distribution.</p>
<p>Let's assume that the gradients have a normal distribution, i.e. <span class="math">\(g_1, g_2, ..., g_t \sim N(0, \sigma^2)\)</span>. Then the inverse square of each gradient comes from the <a href="https://en.wikipedia.org/wiki/Scaled_inverse_chi-squared_distribution">scaled inverse chi squared distribution</a>, which is quite similar to the standard inverse chi squared distribution, <span class="math">\(1/g_i^2 \sim \text{Scale-Inv}\)</span>-<span class="math">\(\chi^2(1, 1/\sigma^2)\)</span>. And here is the problem, depending on some parameters, this distribution may have infinitely large variance. As a result, averaging the gradients may not really converge to the true gradients of the whole dataset and the steps we take initially may be in the wrong direction. We need to find a way to reduce the variance in the early stages of the training. This is the problem that the Radam optimizer tackles [3].</p>
<p>The adaptive learning rate in Adam comes from the <span class="math">\(\frac{1}{\sqrt{\hat{v_t}}}\)</span> term. Let's denote the square root of its inverse with <span class="math">\(\psi(g_1, ..., g_t)\)</span>:</p>
<div class="math">$$
\psi(g_1, ..., g_t) =  \sqrt{\frac{1 - \beta_2^t}{(1 - \beta_2) \sum_{i=0}^{t-1}\beta_2^i g_{t-i}^2}}.
$$</div>
<p>We can assume that this exponential moving average (EMA) is well approximated, at least early on, by a simple average (SMA). As a result, the distribution of the EMA will be approximately that of the SMA:</p>
<div class="math">$$
p \Bigg (\sqrt{\frac{1 - \beta_2^t}{(1 - \beta_2) \sum_{i=0}^{t-1}\beta_2^i g_{t-i}^2}} \Bigg) = p \Bigg(\sqrt{\frac{t}{\sum_{i=1}^t g_i^2}} \Bigg).
$$</div>
<p>Since  <span class="math">\(\frac{t}{\sum_{i=1}^t g_i^2} \sim \text{Scale-Inv}\)</span>-<span class="math">\(\chi^2(t, 1/\sigma^2)\)</span>, we can also claim that <span class="math">\(\psi^2 \sim \text{Scale-Inv}\)</span>-<span class="math">\(\chi^2(\rho, 1/\sigma^2)\)</span>, for some parameter <span class="math">\(\rho\)</span>. Now, the EMA is a good approximation of the SMA for some particular length <span class="math">\(f(t, \beta_2)\)</span> that makes the SMA have the same "center of mass" as the EMA. The degrees of freedom <span class="math">\(\rho\)</span> can be taken to be an estimate for the length <span class="math">\(f(t, \beta_2)\)</span>. We can find <span class="math">\(\rho\)</span> as follows:</p>
<div class="math">$$
\frac{(1 - \beta_2) \sum_{i=0}^{t-1}\beta_2^i g_{t-i}^2}{1 - \beta_2^t} = \frac{\sum_{i=1}^{f(t, \beta_2)} g_{t+1-i}^2}{f(t, \beta_2)} \Rightarrow f(t, \beta_2) = \rho_t = \frac{2}{1 - \beta_2} - 1 - \frac{2t\beta_2^2}{1 - \beta_2^t}.
$$</div>
<p>From now on, let's set <span class="math">\(r_\infty = \frac{2}{1 - \beta_2} - 1\)</span> as it's the highest value that <span class="math">\(\rho_t\)</span> can take. Now, it can be proved (here we take it for granted) that the variance of <span class="math">\(\psi\)</span> is minimal when <span class="math">\(\rho_t = \rho_\infty\)</span>.</p>
<p>Since <span class="math">\(\psi^2\)</span> is distributed as a scaled inverse chi-square, we can actually compute the variance of <span class="math">\(\psi\)</span>. To enforce consistent variance of <span class="math">\(\psi\)</span> throughout the training, we can multiply <span class="math">\(\psi\)</span> by some rectification term <span class="math">\(r_t\)</span> such that <span class="math">\(\text{Var}(r_t \psi)\)</span> is minimal - which occurs when <span class="math">\(\rho_t = \rho_\infty\)</span>. Skipping the derivation, we have</p>
<div class="math">$$
r_t = \sqrt{\frac{(\rho_t - 4)(\rho_t - 2) \rho_\infty}{(\rho_\infty - 4)(\rho_\infty - 2)\rho_t}}.
$$</div>
<p>So here's how Radam works. We approximate the EMA for the second moment of the gradients with a sufficiently long SMA that has a familiar distribution. At the beginning, <span class="math">\(r_\infty\)</span> is computed. Then, in step <span class="math">\(t\)</span> of the optimization process, Radam computes <span class="math">\(\rho_t\)</span> from which the variance of <span class="math">\(\psi\)</span> can be determined. If it's tractable, we multiply the learning rate by <span class="math">\(r_t\)</span> to obtain a small and consistent variance over the course of the training. If it's intractable, which happens when <span class="math">\(\beta_2 &lt; 0.6\)</span>, tough luck.</p>
<p>It's worth mentioning that this kind of enhanced optimizers is not the only way to deal with large variance early on. One can also start updating the parameters after a number of steps, so that <span class="math">\(\psi\)</span> has been computed from a sufficiently large number of samples. Additionally, one could adopt a <em>warmup</em> schedule on the learning rate, starting from a very small value and gradually increasing it to the desired level.</p>
<p><strong>Lookahead</strong>. There is another line of work apart from adaptive learning rates and momentum which has become quite important recently. The idea is that one keeps two sets of weights - <span class="math">\(\theta_t\)</span>, called the "fast weights", which are updated frequently, and <span class="math">\(\phi_t\)</span>, called "slow weights", which are updated only periodically. The motivation here is that while using most SGD variants works well in practice, these methods may be quite sensitive to hyperparameters, especially on some datasets. By using two sets of weights, where the fast weights are used only as a form of lookahead, we essentially collect precious information about the loss landscape and then make a more informed decision by updating the slow weights [4].</p>
<p>Suppose the fast weights <span class="math">\(\theta_t\)</span> are updated by any optimizer <span class="math">\(A\)</span>. Then at timestep <span class="math">\(t\)</span>, we first synchronize the fast weights with the slow weights from the previous iteration, then perform <span class="math">\(k\)</span> steps with the inner optimizer -  <span class="math">\(\theta_{t,i} = \theta_{t, i-1} + A(\mathcal{L}, \theta_{t, i-1}, d)\)</span>. Each of these updates is computed on a random minibatch of data <span class="math">\(d \sim \mathcal{D}\)</span>. After the <span class="math">\(k\)</span> steps, we update the slow weights by linearly interpolating in the direction of <span class="math">\(\theta_{t,k} - \phi_{t-1}\)</span> using a step size for the slow weights <span class="math">\(\alpha\)</span>:</p>
<div class="math">$$
\begin{cases}
\begin{align}
\phi_{t+1} &amp;= \phi_t + \alpha(\theta_{t+1, k} - \phi_t) \\
\theta_{t+1, k} &amp;= \theta_{t+1, k-1} + A(\mathcal{L}, \theta_{t+1, k-1}, d) \\
\theta_{t+1, 0} &amp;= \phi_t \\
\end{align}
\end{cases}
$$</div>
<p>By rolling out the recursive updates, we can see that the slow weights are updated in an exponentially moving average manner.</p>
<div class="math">$$
\phi_{t+1} = (1 - \alpha)^t \phi_0 + \alpha \sum_{i=0}^t (1 - \alpha)^i\theta_{t-i, k}
$$</div>
<p>When analyzing the convergence rate of optimizers, one can assume the noisy quadratic model as a loss function - essentially a quadratic function <span class="math">\((x-c)^T A (x-c)\)</span> where <span class="math">\(c\)</span> is a random vector. Suppose in that context we compare Lookahead with SGD as the inner optimizer and plain SGD. The iterates of both algorithms would reach fixed points with expectation <span class="math">\(0\)</span> but with different variances. It can be proved that the fixed point of the Lookahead method has strictly smaller variance than that of the fixed point of the SGD inner-loop optimizer for the same learning rate.</p>
<p><strong>Ranger</strong>. We are slowly reaching state-of-the-art. FastAI's Ranger was for quite some time the fastest optimizer out there. And the idea behind it is more than straightforward - it is simply the combination of RadamW with Lookahead. RadamW itself is a variant Radam, where alongsite the rectification term, we have properly added weight decay regularization, similar to AdamW. By using RadamW as the inner-loop optimizer in Lookahead, the resulting optimization is made even more stable and robust to hyperparameter changes.</p>
<p><strong>Layer-wise adaptive learning</strong>.
Consider what happens if we use <span class="math">\(N\)</span> GPUs instead of only <span class="math">\(1\)</span> to train a network. What is typically done is to split the samples in the minibatch evenly across all computational units. Each unit computes the loss and the gradients over <span class="math">\(B/N\)</span> samples, where <span class="math">\(B\)</span> is the batch size and <span class="math">\(N\)</span> is the number of GPUs. The gradients are then averaged across all GPUs and each copy of the model on every GPU is updated with the synchronized weights. Using many computational units allows us to process more samples in the same iteration, which is quite like increasing the batch size. But if the batch size increases by <span class="math">\(k\)</span> times, then we'll perform <span class="math">\(k\)</span> times fewer updates. So to keeps things similar, we should increase the learning rate by <span class="math">\(k\)</span> times. Do you see where this leads? Increasing the number of computational units requires us to increase the learning rate, but increasing that too much can become quite unstable for training.</p>
<p>To get a sense of how stable an update is we can measure the ratio between the norm of the layer weights and the norm of the gradients, <span class="math">\(\lVert \theta^l\rVert / \lVert \nabla_{\theta^l} \mathcal{L}(\theta^l)\rVert\)</span>. If this ratio is too high, the training may become unstable. If it's too low, then the weights aren't updated fast enough. Moreover, this ratio typically varies wildly across the different layers in the network, which in turn motivates the need for individual per-layer learning rates. LARS was the first approach to do that [5]. </p>
<p>In LARS (layer-wise adaptive rate scaling) we have a global learning rate <span class="math">\(\eta_t\)</span> which depends only on the iteration <span class="math">\(t\)</span>. We also have per-layer learning rates <span class="math">\(\lambda_t^l\)</span> which control the learning rates of each layer (not each individual weight):</p>
<div class="math">$$
\begin{cases}
\begin{align}
\theta_{t+1}^l &amp;= \theta_{t}^l - \eta_t \lambda_t^l m_t^l \\
\lambda^l_t &amp; \propto \frac{\lVert \theta^l_t \rVert}{\lVert g_t^l \rVert + \delta \lVert \theta^l_t \rVert} \\
m_t^l &amp;= \beta_1 m_{t-1} + (1 - \beta_1)( g_t^l + \delta \theta_t^l) \\
g_t^l &amp;= \nabla_{\theta} F(\theta_t^l) \\
m_0^l &amp;= 0, \ \forall l. \\
\end{align}
\end{cases}
$$</div>
<p>Here <span class="math">\(\theta_t^l\)</span> are the weights of layer <span class="math">\(l\)</span> at time <span class="math">\(t\)</span>, <span class="math">\(\beta_1\)</span> is a momentum term, <span class="math">\(\delta\)</span> is a weight decay term, and <span class="math">\(g_t^l\)</span> is, for lack of a better notation, the gradient at time <span class="math">\(t\)</span> for layer <span class="math">\(l\)</span>. So LARS is essentially SGD with momentum, weight decay, and per-layer adaptive learning rates, given by <span class="math">\(\lambda^l_t\)</span>. This modification allows us to train large convolutional networks on many GPUs while still being able to afford a large global learning rate.</p>
<p>Nonetheless, it has been observed that LARS doesn't perform well on specific attention-based models. A straightforward improvement is LAMB [6] - the equivalent of LARS but with ADAM as the main optimizer. The idea is to compute the first and second moment estimates <span class="math">\(m_t\)</span> and <span class="math">\(v_t\)</span>, bias-correct them, and calculate their ratio <span class="math">\(r_t = \hat{m}_t / \sqrt{\hat{v}_t}\)</span>, similar to Adam. Now the only change in <span class="math">\(\lambda_t^l\)</span> becomes <span class="math">\(\lambda_t^l = \lVert \theta_t^l \rVert / (\lVert r_t^l \rVert + \lVert \delta \theta_t^l \rVert)\)</span>. </p>
<p><strong>RangerLARS.</strong>
The final optimizer we'll consider here is RangerLARS - the combination of Ranger and LARS. It's also called RangerLamb, or even Over9000. It has been benchmarked extensively on many different datasets and tasks and is probably the fastest optimizer that we have right now. The inner-loop optimizer consists of Radam and LARS/LAMB and works as follows. The Radam rectification <span class="math">\(r_t\)</span> is computed and based on it we get the Radam step size <span class="math">\(- \eta_t r_t \hat{m}_t l_t\)</span>. Here <span class="math">\(\eta_t\)</span> is the learning rate, <span class="math">\(\hat{m}_t\)</span> is the bias-corrected first moment, and <span class="math">\(l_t = \sqrt{(1 - \beta_2^t)/v_t}\)</span> is the adaptive modifier. Then comes the LARS part - we calculate the ratio of the norms <span class="math">\( \lambda_t = \lVert \theta_t \rVert / \lVert  \eta_t r_t \hat{m}_t l_t \rVert\)</span>, multiply the radam step size with it, and update... and all this happens independently for each layer... And periodically update the slow weights using Lookahead.</p>
<p>In practice, there are a lot of tricky corner cases that need to be handled in the implementation. First the Radam step may not be applicable, in which case it has to be substituted with a simpler form that does not use variance rectification. Second, the gradient of the weight decay has to be accounted for. Third, what happens if <span class="math">\(\lVert w_t \rVert\)</span> is very big? In practice, it's common to clamp it to <span class="math">\([0, 10]\)</span>. But even when RangerLARS does in 25 epochs what Ranger does in 120, there are very small rooms for improvement. Some people have noticed that the weights become rather noisy as the training continues. To mitigate this, the clamping process, given by the formula <span class="math">\(\text{max}(0, \text{min}(r_t, 10))\)</span> can be changed to <span class="math">\(\text{max}(1, \text{min}((r_t b_t)^{-1}, r_t))\)</span>, where <span class="math">\(b_t = \sqrt{1 - \beta_2^t}/(1 - \beta_1^t)\)</span> is the Adam bias correction term. This modification is called <em>Paratrooper</em> and forces RangerLARS to become more and more like Ranger as time goes on, reducing the noise in the weights. The convergence to the optimum is not slowed down, since RangerLARS dominates initially.</p>
<p><strong>Loss landscapes</strong>. Deep learning optimizers have come a long way - they are becoming more complicated and more specialized. Like with most other things, complexity arises out of the desire to outperform current methods and designs. And rightfully so, because the optimization problems in deep learning are hard. Let's end by quickly exploring how to get a sense of what we're up against.</p>
<p>Obviously, the loss landscape is very high-dimensional and there's no way to visualize it in a 2D or 3D plot. However, we can get a very approximate, and maybe inaccurate plot using the following procedure [7]:</p>
<ol>
<li>Train a network to obtain a minimum <span class="math">\(\theta^*\)</span></li>
<li>Sample two new random sets of weights <span class="math">\(\theta'\)</span> and <span class="math">\(\theta''\)</span></li>
<li>Plot the 3D surface of the points <span class="math">\((\alpha, \beta, \mathcal{L}(\theta^* + \alpha \theta' + \beta \theta''))\)</span>.</li>
</ol>
<p>Figure 2 shows one loss landscape found using this method. The task is a simple noise-free regression of a sine. We train a small 3 layer net with ReLUs to obtain the optimal weights <span class="math">\(\theta^*\)</span>. Then we sample two random sets of weights <span class="math">\(\theta'\)</span> and <span class="math">\(\theta''\)</span>, and linearly interpolate the optimal weights with the random ones based on a two dimensional grid of coefficients <span class="math">\(alpha\)</span> and <span class="math">\(beta\)</span>. Then, we evaluate all of the resulting networks on the loss and plot that.</p>
<figure>
    <img class='small_img' src="/images/nn_loss_landscape.png" alt="Loss landscape" width="1200">
    <figcaption>Figure 2: A rough visualization of the loss landscape for a simple regression task. Using two random sets of weights and one optimal set, we form linear combinations based on coefficients $\alpha$ and $\beta$, evaluate the corresponding neural network and plot the resulting surface.</figcaption>
</figure>

<p>Implementation-wise, I think this use-case is where JAX shines. When linearly interpolating the weights based on the coefficients we can <code>vmap</code> the interpolation function for a single <span class="math">\((\alpha, \beta)\)</span> combination to get wildly improved performance. Similarly, for the evaluation, the functional style of JAX allows us to <code>vmap</code> the function call itself across the parameters. This basically allows us to evaluate <em>many</em> different networks in an efficient manner as if they were simply elements in a vector.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">import</span> <span class="nn">jax.tree_util</span>

<span class="c1"># The neural network model</span>
<span class="k">def</span> <span class="nf">neural_network</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">outputs</span>

<span class="k">def</span> <span class="nf">create_weights</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">ref_point</span><span class="p">,</span> <span class="n">pert1</span><span class="p">,</span> <span class="n">pert2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    alpha: float</span>
<span class="sd">    beta: float</span>
<span class="sd">    ref_point: the optimal weights</span>
<span class="sd">    pert1: perturbed set of weights</span>
<span class="sd">    pert2: perturbed set of weights</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">pert1</span><span class="p">,</span> <span class="n">pert2</span><span class="p">,</span> <span class="n">ref_point</span><span class="p">:</span> <span class="n">ref_point</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">pert1</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">pert2</span><span class="p">,</span> 
        <span class="n">pert1</span><span class="p">,</span> <span class="n">pert2</span><span class="p">,</span> <span class="n">ref_point</span><span class="p">)</span>

<span class="c1"># The optimal weights and the random sets</span>
<span class="n">reference_point</span> <span class="o">=</span> <span class="o">...</span> 
<span class="n">perturbation1</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">perturbation2</span> <span class="o">=</span> <span class="o">...</span>

<span class="c1"># Weight combinations</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">betas</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">ALPHA</span><span class="p">,</span> <span class="n">BETA</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">betas</span><span class="p">)</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="n">ALPHA</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">betas</span> <span class="o">=</span> <span class="n">BETA</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="c1"># Evaluate losses</span>
<span class="n">perturbed_params</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">create_weights</span><span class="p">,</span> 
    <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">betas</span><span class="p">,</span> 
    <span class="n">reference_point</span><span class="p">,</span> <span class="n">perturbation1</span><span class="p">,</span> <span class="n">perturbation2</span><span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">neural_network</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">))(</span><span class="n">perturbed_params</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
<span class="n">losses</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(((</span><span class="n">preds</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div>

<h3>References</h3>
<p>[1] Duchi, J., Hazan, E., Singer, Y. <a href="https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a> Journal of Machine Learning Research 12 (2011). <br>
[2] Kingma, D. P., Ba, J. <a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a> arXiv preprint arXiv:1412.6980 (2014). <br>
[3] Liu, L. et al. <a href="https://arxiv.org/abs/1908.03265v4">On the Variance of the Adaptive Learning Rate and Beyond</a> arXiv preprint arXiv:1908.03265 (2019). <br>
[4] Zhang, M. R. et al. <a href="https://arxiv.org/abs/1907.08610">Lookahead Optimizer: k steps forward, 1 step back</a> arXiv preprint arXiv:1907.08610 (2019). <br>
[5] You, Y., Gitman, I., Ginsburg, B. <a href="https://arxiv.org/abs/1708.03888">Large Batch Training of Convolutional Networks</a> arXiv preprint arXiv:1708.03888 (2017). <br>
[6] You, Y. et al. <a href="https://arxiv.org/abs/1904.00962">Large Batch Optimization for Deep Learning: Training BERT in 76 minutes</a> arXiv preprint arXiv:1904.00962 (2021). <br>
[7] Li, H., et al. <a href="https://proceedings.neurips.cc/paper_files/paper/2018/hash/a41b3bb3e6b050b6c9067c67f663b915-Abstract.html">Visualizing the loss landscape of neural nets</a> Advances in neural information processing systems 31 (2018).</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = "https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML";

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'], ['$', '$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div>
  
  <p class="post-meta">
    Tags: ai, cs
  </p>

  </article>


    </div>
  </main>


<footer class="site-footer h-card">
  <div class="wrapper">
  
  <p></p>
  

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">


    <div class="footer-col-wrapper">

      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">The Critical Section</li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          <li><a href="https://github.com/aceofgreens"><i class="fa fa-github"></i> GitHub</a></li>


        </ul>
      </div>


      <div class="footer-col footer-col-3">
        <p>A personal blog for artificial intelligence and similar topics.</p>
      </div>
    </div>

  </div>
</footer>

<script type="text/javascript">
function welcomeFunction() {
  var items = document.getElementsByTagName("code");
    for (var i = items.length; i--;) {
      items[i].setAttribute("class", "highlight");
  }
}
</script>

</body>
</html>