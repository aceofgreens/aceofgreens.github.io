---
Title: DINO, CLIP, PaliGemma
Date: 2024-12-07 07:00:00 +0200
Tags: ai
slug: vlms
---

Here we take a look at some basic use-cases with DINOv2, Clip, and Google's new visual-language model PaliGemma 2. This is not meant to be a comprehensive overview of vision-language models. We will only dip our toes into this vast topic. 

DINOv2 is a very good model. It has found extremely wide applicability due to its strong generalization. It is a vision transformer, so nothing too exotic here, trained on a large-scale diverse dataset of very high quality. Of course the quality of the learned features greatly depends on the dataset. This is not a new insight. However, I was still very impressed by how much gain we can get from a well-constructed, curated, diverse dataset.

Let's look the large variant of DINOv2. It extracts $(14, 14)$ patches and projects them linearly, similar to the ViT. In DINOv2 this happens simply using a big convolution layer whose kernel size and stride are both $(14, 14)$. This ensures that patches are non-overlapping and than no pixel remains unprocessed. Subsequently there are 24 encoder layers. In that sense the model is an encoder-only architecture. Each layer follows the transformer design - self-attention where all tokens attend to each other, layer norms for normalisation, residual connections, and a two-layer MLP with GELU activation.

The preprocessor usually needs to resize or crop the input images to be a mulitple of $14$, so that it can be patchified evenly. A common resolution when using the large model is to resize images to $(448, 448)$. This results in $448/14=32$ patches in each image axis, and a total of $32*32=1024$ image patches, each representing a visual token. Additionally, a `[CLS]` token is added to represent any global information for the image that is not spatially localized. Hidden features throughout the model are of shape $(B, N+1, D)$ where $D$ is the feature dimension and there are $N+1$ total tokens.

We won't cover how the model is trained, which was explored [previously](https://aceofgreens.github.io/ssl_algorithms_in_vision.html). Instead, let's jump into the qualitative results. To visualize the learned features, we can do the following:

1. We take the non-cls tokens of shape $(N, D)$ and do a single component PCA on them, producing $N$ scalar numbers.
2. The histogram of these numbers is usually multimodal and by rescaling and thresholding it, we can obtain those tokens which correspond to the background and the foreground of the image.
3. We sulect only the tokens corresponding to the foreground and compute their 3-component PCA projection. We plot the foreground projections, of dimension 3, as a RGB image.

Various visualizations are shown in Figure 1. The projected features from step 3. are shown in the leftmost image, while the one next to it shows these features superimposed over the original image. Note that to get the shapes to be equal one needs to upsample and interpolate the token features, which are of shape $(\sqrt{N}, \sqrt{N})$, to the original image resolution. Note how different foreground objects within the same image are assigned relatively different features. Thus, within a single image the model "mostly" focuses on distinguishing the two main objects.

This is not the case if the model processes the objects independently. The model processes the last two images as independent samples in a batch and the elephant tokens from one image do not attend to the elephant tokens from the other image. Yet, when we do PCA, we compute the projection jointly for both samples, by flattening the batch dimension. This allows us to see that the model assigns the same semantic features to the same morphological parts of the animals. In other words, the model automatically learns *part-of* relationships and can perform dense point correspondences. For sparse correspondences, we can simply compute nearest neighbors.

<figure>
    <img class='extra_big_img' src="/images/dino_predictions.PNG" alt="DINOv2 preds" width="1200">
    <figcaption> Figure 1: DINOv2 qualitative results. From left to right: 1) RGB projection of the foreground features; 2) RGB features upsampled and overlaid on the original image; 3-4) Dense feature similarity across different images.</figcaption>
</figure>

<figure>
    <img class='big_img' src="/images/dinov2_feature_norms.png" alt="DINOv2 token norms" width="1200">
    <figcaption> Figure 2: DINOv2 token norms throughout the layers.</figcaption>
</figure>


