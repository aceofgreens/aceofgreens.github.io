<!DOCTYPE html>
<html lang="en">

<head>
    <title>Thoughts After IMOL | The Critical Section</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Pelican" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <link rel="stylesheet" href="/assets/main.css" />
    <link rel="icon" href="/assets/favicon.png" />
    <link href="https://fonts.googleapis.com/css?family=Roboto|Oswald|Open+Sans" rel="stylesheet">


    <meta name="description" content="I recently attended the Fifth International Workshop on Intrinsically Motivated Open-ended Learning (IMOL 2022) in Tübingen, Germany. It was a very nice event, with interesting presentations and discussions all around. This post aims to summarize the highlights and provide some thoughts on the content discussed therein." />

    <meta name="tags" content="rl" />

</head>

<body onload="welcomeFunction()" >

  <header class="site-header" role="banner">
    <div class="wrapper">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> <!--An icon library for the button icon-->

      <a class="site-title" rel="author" href="/">The Critical Section</a>

      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
            <a class="page-link" href="/about.html">About</a>
            <a class="page-link" href="/posts.html">Posts</a>
            <a class="page-link" href="/tags.html">Tags</a>
        </div>

      </nav>

    </div>
  </header>


  <main class="page-content" aria-label="Content">
    <div class="wrapper">

  <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <header class="post-header">
      <h1 class="post-title p-name" itemprop="name headline">Thoughts After IMOL</h1>
      <p class="post-meta">
        <time class="dt-published" datetime="2022-04-08T16:00:00+02:00" itemprop="datePublished">
          8 Apr 2022
        </time>
      </p>

    </header>

    <div class="post-content">
      <p>I recently attended the <a href="https://2022.imol-conf.org/">Fifth International Workshop on Intrinsically Motivated Open-ended Learning</a> (IMOL 2022) in Tübingen, Germany. It was a very nice event, with interesting presentations and discussions all around. This post aims to summarize the highlights and provide some thoughts on the content discussed therein.</p>
<p>Reinforcement learning is a very interesting area. Unlike supervised or unsupervised learning where we have <strong>prediction</strong> tasks, here we have <strong>control</strong> tasks and control is fundamentally tied to consequences. For example, if you are forecasting sales, you cannot in any reasonable way claim that this is a control task, because your forecast will have absolutely no effect on the real sales in the future. Usually, such a forecast is useful to inform us, but it has no real-world consequences and cannot be considered an <strong>action</strong>.</p>
<p>This is not the case in RL problems, which are about acting in an optimal way. And that is what makes reinforcement learning one of the most important, and in my opinion most exciting, fields in ML right now. It is arguably the closest we have come to a study of Artificial General Intelligence (AGI).</p>
<p>While intelligence may be the ability to model complicated relationships, what ultimately matters is how you use that intelligence to act - and behaviour can only be observed in a control problem where we are endowed with the notions of time, actions, and states. What makes RL algorithms useful is this particular difference that their quality is evaluated in a behaviour space, rather than a loss space.</p>
<p>Likewise, your goals are ultimately defined in the behaviour space. Minimizing a certain loss or predicting accurately are just means to achieving these goals. Imagine if the reward function of the agent dependended not on the state, but just the action taken - this would correspond to a zombie-like agent that does not take the surroundings into consideration and therefore cannot possibly adapt to the environment.</p>
<p>Like many others, my goal is to assist in creating more advanced and general agents so that we can get insights about us ourselves. Just like a RL agent is trained to solve a task iteratively by interacting with the environment, so do we, humans, improve in understanding ourselves, by building models of ourselves (the RL agents) that we test on simpler environments. </p>
<p>However, when comparing humans and RL agents, it’s hard to abstract away the vast difference in the difficulty of the environments where the two types of agents are tested. In the real world, we judge human-ness by the cohesiveness of the responses to questions and the adequateness of verbal and motoric responses to various prompts for interaction. But in the case of an RL agent, what does human-level performance look like if the testing environment is not complex enough to support these real-world criteria? What does human-like behavior look like in an Atari or MuJoCo game, where we cannot use language as “evidence” of consciousness?</p>
<p>A first step in making RL agents more human-like is to make the rewards endogenous to the agent, as opposed to given exogenously by the environment. With humans, we intrinsically set short-term and long-term goals which guide our behavior in a principled and directed manner. Similarly, we can think of an RL agent as choosing his short-term goals by sampling a reward function. The reward function defines a preference over states and therefore an implicit goal for the agent. This opens up interesting algorithms where the agent can learn to choose his short-term goals by sampling a reward function from a Gaussian process or learning it parametrically. However, what then prevents the agent from always sampling a reward function that always returns high rewards (the equivalent of a goal of doing nothing useful)? With humans this is solved by not being able to choose our long-term preferences over the world. We cannot purposefully reach a mental state of nirvana and stay in it forever. Similarly, if the RL agent is able to choose his short-term goals, there has to be a higher more primal and fundamental meta-reward function guiding him, which the agent cannot engineer.</p>
<p>Another way to make RL agents more human-like is by improving their models of the world (their estimated models for the transition dynamics of the environment). Humans process incredibly large amounts of data each second, but consciously perceive only on a tiny part of it. Similarly, an agent should have a powerful feature extractor that processes all signals from the environment but also subsamples the output considerably (through bottlenecks, attention, pooling, etc.) so that downstream tasks are performed faster and on condensed features only. This mimics the focus in humans and the conscious perception of only a handful of features – those carrying the useful information related to the task. We can then ask what happens if we exogenously add additional features in the environment – those representing another agent, perhaps even itself, and label them “you” or even “me”. Can recognizing the features of an agent exactly like you, and labeling these features “me”, be considered developing an actual identity?</p>
<h3>IMOL Highlights</h3>
<p>RL is a big and growing research area. Within it we can identify the following possibly overlapping sub-areas:</p>
<ul>
<li><strong>Value-based methods</strong> - where the agent learns the mean value of states or state-action pairs. The policy is formed by choosing the action with the highest value;</li>
<li><strong>Policy-based methods</strong> - where the agent learns a policy for suggesting the actions directly;</li>
<li><strong>Model-based methods</strong> - where the agent has access to learned predictors for the MDP transition dynamics and can "plan" his actions;</li>
<li><strong>Offline (batch) RL</strong> - where the agent cannot interact with the environment and has to learn entirely from past trajectories;</li>
<li><strong>Imitation learning</strong> - where the agent does not have access to the reward function and has to learn an optimal policy through either mimicking the behaviour of an expert, or learning the reward function;</li>
<li><strong>Meta-RL</strong> - where the agent learns how to learn so that he's able to efficiently few-shot any unseen test tasks;</li>
<li><strong>Open-ended learning</strong> - where the agent learns to extract useful transferable knowledge in the presence of no task.</li>
</ul>
<p>While there are no hard restrictions on the content, the IMOL conference is concerned primarily with the last one - learning knowledge without having any specific tasks set. This is generally accomplished by the agent setting his own goals and <strong>intrinsic motivation</strong> - a type of self-generated reward that is independent of the presence of any extrinsic tasks - is key to that regard.</p>
<p>I've summarized the main highlights in Figure 1 below. Note that this is my personal, opinionated selection of the highlights, there were other just as good presentations and topics which I've left out.</p>
<figure>
    <img class='big_img' src="/images/imol_tree.png" alt="IMOL2022 highlights" width="1000">
    <figcaption>Figure 1: A tree representing some of the discussed topics in IMOL 2022.</figcaption>
</figure>

<p>In short, I've grouped the presentations into 5 collections.</p>
<ul>
<li>
<p><strong>Developmental robotics</strong> [1, 2, 3], which aims to study developmental behaviour in embodied intelligence. Just like a toddler explores the state space and tries out different actions, gaining precious knowledge about the world, so is the aim for a robot to explore the state space curiously in the presence of no explicit rewards. The existence of a body which you have to explore and control is a challenge which makes tasks like reaching, grasping, and moving difficult. Note that the actions in such environments are also very low-level - typically the voltage/force/torque applied to a rotor/joint - and the rewards are further down in the<br>
"action <span class="math">\(\rightarrow\)</span> effect <span class="math">\(\rightarrow\)</span> reward" chain.  </p>
</li>
<li>
<p><strong>Real-life case studies</strong> [4, 5], where deep reinforcement learning is applied to practical problems. It is fascinating to see how different academic RL is from real-life RL, and how much more work has to go into the system to make it learn in a real-world scenario. We discussed, among other things, how quantile-regression soft actor critics can be used to achieve superhuman performance in Gran Turismo and how to maximum a posteriori policy optimization (MPO) can be used to train a controller for various voltages in a magnetic tokamak reactor, achieving near-perfect sim2real transfer.  </p>
</li>
<li>
<p><strong>Novel architectures</strong> [6, 7, 8] - a collection of improvements and ideas that outperform current standards. For example, graph neural networks (GNN) have a powerful inductive bias which is surprisingly useful when it comes to recognizing the structure of the transition model. Another interesting idea was the Differential Extrinsic Plasticity approach - essentially Hebbian learning applied not to the outputs, but the output derivatives of the neurons. Lastly, the GateL0RD approach - a RNN cell with <span class="math">\(L_0\)</span>-regularization applied to the hidden state, convinced me of its utility in modelling e.g. the transition function of the MDP.  </p>
</li>
<li>
<p><strong>Infer and Collect</strong> [9-16] - DeepMind's idea of treating RL as two different optimization problems: one of optimal policy inference - estimate the target policy from a fixed set of collected data, and one of optimal data collection - estimate a behavioural policy that collects the best training data. For inference the currently popular data-efficient off-policy algorithms are MPO and DiME, which are used for multi-objective optimisation and differ in how they combine the various preferences across the tasks into a single policy. For the collection problem, successful approaches are for example <em>Simple Sensor Intentions</em>, where the agent explores by maximizing a predefined statistic/feature of the input sensors, and explore2offline, which uses Intrinsic Model Predictive Control for planning (MPC with task-agnostic intrinsic curiosity rewards).  </p>
</li>
<li>
<p><strong>Existential approaches</strong> [17-19] - these models attempt to produce fully autonomous agents with lifelong adaptive capabilities, able to succeed and generalize across different domains. In a way, with enough autonomy in the goal, this tackles existential problems (for lack of a better word) like goal generation &amp; selection, self-recognition, and multi-agent cooperation. Of course, it is still early to judge the practical merits of these algorithms. Nonetheless, they hold their ground as one of the most interesting ideas in current RL.</p>
</li>
</ul>
<p>All in all, IMOL 2022 proved that research in optimizing the exploration-exploitation trade-off is alive and kicking. Some of the hot topics include architectures for better generalization, data-efficient offline RL, and better ways to incentivize exploration in sparse reward environments. Other topics like entire cognitive architectures are slowly but steadily getting traction.</p>
<p>That being said, it seems to me that:</p>
<ul>
<li>
<p>Current research in intrinsic motivation may be <strong>too anthropocentric</strong>, specifically in areas like developmental robotics where the focus is on imitating specifically human learning. I think this focus on the humans may be hindering progress a bit because of how complex humans are. Moreover, who said that human learning is the best example of early developmental learning?  </p>
</li>
<li>
<p>I'm starting to develop a fairly strong preference toward more <strong>minimalistic</strong> models. For example, some of the researchers I spoke to held this notion of social interaction as an almost fundamental "generator" of useful signals for humans. While I do agree that social interaction is a key part of our behaviour, I believe it can easily be abstracted away into the environment. 
Agents cooperating to solve tasks together should be able to invent social norms in a way that allows them to solve tasks more efficiently. In that sense social interaction is not something fundamental, it's just a tool. And the current attitude towards these kinds of issues is not "reductionist" enough.  </p>
</li>
<li>
<p>We need better <strong>goal and action abstraction</strong> that go beyond the simple "goal - reward" duality. In economics we know that only some preference relations admit a utility function representation. What if this implies that there are goals that cannot be described with a reward function? In essence, I believe that if we have better abstractions for concepts like goal and action, then it would be easier to optimize over goals and recognize and invent new actions - a step toward more general agents.</p>
</li>
</ul>
<p>In conclusion, IMOL 2022 was a ton of fun. I met some very smart and interesting people and our discussions have given me important perspective. Looking forward to the next one.</p>
<h3>References</h3>
<p>[1] Binz, M. and Schulz, E. <a href="https://arxiv.org/abs/2201.11817">Exploration With a Finite Brain</a> arXiv preprint arXiv:2201.11817 (2022).<br>
[2] Dumini, N. et al. <a href="https://arxiv.org/abs/2102.09854">Intrinsically Motivated Open-Ended Multi-Task Learning Using Transfer Learning to Discover Task Hierarchy</a> arXiv preprint arXiv:2102.09854 (2021).<br>
[3] Gama, F. et al. <a href="https://arxiv.org/abs/2008.13483">Active exploration for body model learning through self-touch on a humanoid robot with artificial skin</a> arXic preprint arXiv:2008.13483 (2020).<br>
[4] Degrave, J. et al. <a href="https://doi.org/10.1038/s41586-021-04301-9">Magnetic control of tokamak plasmas through deep reinforcement learning</a> Nature (2022).<br>
[5] Wurman, P. et al. <a href="https://www.nature.com/articles/s41586-021-04357-7">Outracing champion Gran Turismo drivers with deep reinforcement learning</a> Nature (2022).<br>
[6] Der, R. and Martius, G. <a href="https://arxiv.org/abs/1505.00835">A novel plasticity rule can explain the development of sensorimotor intelligence</a> arXiv preprint arXiv:1505.00835 (2015).<br>
[7] Seitzer, M., Schölkopf, B. and Martius, G. <a href="https://arxiv.org/abs/2106.03443">Causal Influence Detection for Improving Efficiency in Reinforcement Learning</a> arXiv preprint arXiv:2106.03443 (2021).<br>
[8] Gumbsch, C., Butz, M. V. and Martius, G. <a href="https://arxiv.org/abs/2110.15949">Sparsely Changing Latent States for Prediction and Planning in Partially Observable Domains</a> arXiv preprint arXiv:2110.15949 (2022).<br>
[9] Riedmiller, M. et al. <a href="https://openreview.net/forum?id=qscEfLT5VJK">Collect &amp; Infer - a fresh look at data-efficient Reinforcement Learning</a> Conference on Robot Learning (2021).<br>
[10] Abdolmaleki, A. et al. <a href="https://arxiv.org/abs/2005.07513">A Distributional View on Multi-Objective Policy Optimization</a> arXiv preprint arXiv:2005.07513 (2020).<br>
[11] Abdolmaleki, A. et al. <a href="https://arxiv.org/abs/2106.08199">On Multi-objective Policy Optimization as a Tool for Reinforcement Learning</a> arXiv preprint arXiv:2106.08199 (2021).<br>
[12] Lambert, N. et al. <a href="https://arxiv.org/abs/2201.11861">The Challenges of Exploration for Offline Reinforcement Learning</a> arXiv preprint arXiv:2201.11861 (2022).<br>
[13] Whitney, W. et al. <a href="https://arxiv.org/abs/2101.09458">Decoupled Exploration and Exploitation Policies for Sample-Efficient Reinforcement Learning</a> arXiv preprint arXiv:2101.09458 (2021).<br>
[14] Riedmiller, M. et al. <a href="https://arxiv.org/abs/1802.10567">Learning by Playing - Solving Sparse Reward Tasks from Scratch</a> arXiv preprint arXiv:1802.10567 (2018).<br>
[15] Hertweck, T. et al. <a href="https://arxiv.org/abs/2005.07541">Simple Sensor Intentions for Exploration</a> arXiv preprint    arXiv:2005.07541 (2020).<br>
[16] Groth, O. et al. <a href="https://arxiv.org/abs/2109.08603">Is Curiosity All You Need? On the Utility of Emergent Behaviours from Curious Exploration</a> arXiv preprint arXiv:2109.08603 (2021).<br>
[17] Sadeghi, M. et al. <a href="https://ieeexplore.ieee.org/abstract/document/9515633">Gestalt Perception of Biological Motion: A Generative Artificial Neural Network Model</a> IEEE International Conference on Development and Learning (ICDL) (2021).<br>
[18] Colas, C. <a href="https://tel.archives-ouvertes.fr/tel-03337625">Towards Vygotskian Autotelic Agents : Learning Skills with Goals, Language and Intrinsically Motivated Deep Reinforcement Learning</a> PhD thesis, Université de Bordeaux (2021).<br>
[19] Akakzia, A. et al. <a href="https://arxiv.org/abs/2202.05129">Help Me Explore: Minimal Social Interventions for Graph-Based Autotelic Agents</a> arXiv preprint arXiv:2202.05129 (2022).</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = "https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML";

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'], ['$', '$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div>
  
  <p class="post-meta">
    Tag: rl
  </p>

  </article>


    </div>
  </main>


<footer class="site-footer h-card">
  <div class="wrapper">
  
  <p></p>
  

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">


    <div class="footer-col-wrapper">

      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">The Critical Section</li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          <li><a href="https://github.com/aceofgreens"><i class="fa fa-github"></i> GitHub</a></li>


        </ul>
      </div>


      <div class="footer-col footer-col-3">
        <p>A personal blog for artificial intelligence and similar topics.</p>
      </div>
    </div>

  </div>
</footer>

<script type="text/javascript">
function welcomeFunction() {
  var items = document.getElementsByTagName("code");
    for (var i = items.length; i--;) {
      items[i].setAttribute("class", "highlight");
  }
}
</script>

</body>
</html>