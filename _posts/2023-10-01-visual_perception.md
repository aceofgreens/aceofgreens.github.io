---
layout: post
title: Visual Perception
date: 2023-10-01 07:00:00 +0200
tags: []
# excerpt_separator: <!--more-->
---

Visual perception is the complex process of transforming the light entering the eye into meaningful mental representations of the surrounding environment. In humans the intricate mechanisms behind our visual perception are responsible for helping us recognize objects. Suppose we take the actual object recognition to be simply the assignment of a previous known semantic label to the visual pattens present in our field of view. Then it would seem that recognition is a deterministic process? Yet, clever illusion can actually show how the same shape can be recognized in different ways depending on the context. Consider the simple [Neckar cube](https://en.wikipedia.org/wiki/Necker_cube) which can snap its orientation or the complicated [pareidolia effect](https://en.wikipedia.org/wiki/Pareidolia) where people see faces in the clouds. Insofar as our perceptual awareness may be tricked into recognizing different semantics from the same input it becomes evident that perception is a deep topic tied to the nature of the human cognition.

Historically, vision theories have been gravitating around three main views:
1. **Indirect** or **constructivist theories**, championed by [Hermann von Helmholtz](https://en.wikipedia.org/wiki/Hermann_von_Helmholtz), state that the visual stimuli that reach our eyes are quite sparse and therefore our brain needs to *construct* hypotheses and make inferences about what the eyes see. To an extent, this sparsity of information should be evident. To find out the colour of an object we need both the colour of the reflected light reaching the eye, and the colour of the light shining on the object. Yet, we only have access to final reflected colour. Hence, an ill-posed problem. From here, one can make various arguments about how mechanisms promoting colour, depth, and shape constancy are utilized. Overall, in this view, meaning is *added to*, rather than contained in the stimulus.
2. **Direct** theories, advocated by [James Gibson](https://en.wikipedia.org/wiki/James_J._Gibson), argue that the visual stimulus is actually quite rich and you don't need to make any inferences or guesses. Textured surfaces create gradients of disparity in the retina. Our movement creates a constant flow of motion gradients. We perceive to act and act to perceive in a continuous loop. Moreover, there are many *invariants* from which we can pick-up information - the relative object sizes stay the same as the viewing distance changes, the relative amounts of light on objects stay the same as the overall light intensity changes and so on.
3. **Computational** theories, led by the influential [David Marr](https://en.wikipedia.org/wiki/David_Marr_(neuroscientist)), state that perception is an information processing task. To understand perception one needs to understand at a *high-level*, what are the inputs and outputs, and the goal of the system, at a *mid-level* - the algorithm that solves the problem, and at a *low-level* how is the algorithm implemented in the neuron or transistor substrate. And the ultimate goal is to recognize and localize objects and then assign meaning to them.
