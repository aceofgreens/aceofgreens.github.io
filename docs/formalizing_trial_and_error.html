<!DOCTYPE html>
<html lang="en">

<head>
    <title>Formalizing Trial And Error | The Critical Section</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Pelican" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <link rel="stylesheet" href="/assets/main.css" />
    <link rel="icon" href="/assets/favicon.png" />
    <link href="https://fonts.googleapis.com/css?family=Roboto|Oswald|Open+Sans" rel="stylesheet">


    <meta name="description" content="The performance of a predictor trained using supervised learning is bounded by the quality of the data. The performance of a policy trained using reinforcement learning (RL), on the other hand, is practically unbounded. Thus, RL has the potential of reaching superhuman performance in multiple domains. Yet, the fundamental idea of RL is quite primitive and intuitive - just reinforce those behaviours that work and penalize those that don't, this being the very very basic premise of policy gradient methods. Let's explore them a bit." />

    <meta name="tags" content="rl" />

</head>

<body onload="welcomeFunction()" >

  <header class="site-header" role="banner">
    <div class="wrapper">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> <!--An icon library for the button icon-->

      <a class="site-title" rel="author" href="/">The Critical Section</a>

      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
            <a class="page-link" href="/about.html">About</a>
            <a class="page-link" href="/posts.html">Posts</a>
            <a class="page-link" href="/tags.html">Tags</a>
        </div>

      </nav>

    </div>
  </header>


  <main class="page-content" aria-label="Content">
    <div class="wrapper">

  <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <header class="post-header">
      <h1 class="post-title p-name" itemprop="name headline">Formalizing Trial And Error</h1>
      <p class="post-meta">
        <time class="dt-published" datetime="2025-01-08T07:00:00+02:00" itemprop="datePublished">
          8 Jan 2025
        </time>
      </p>

    </header>

    <div class="post-content">
      <p>The performance of a predictor trained using supervised learning is bounded by the quality of the data. The performance of a policy trained using reinforcement learning (RL), on the other hand, is practically unbounded. Thus, RL has the potential of reaching superhuman performance in multiple domains. Yet, the fundamental idea of RL is quite primitive and intuitive - just reinforce those behaviours that work and penalize those that don't, this being the very very basic premise of policy gradient methods. Let's explore them a bit.</p>
<p>The policy is <span class="math">\(\pi_\theta\)</span> and in each state <span class="math">\(s_t\)</span> selects an action <span class="math">\(a_t\)</span> to execute. We can sample a trajectory <span class="math">\(\tau\)</span> from it by performing a roll-out according to the policy, <span class="math">\(\tau \sim \pi_\theta(\tau)\)</span>. Here the trajectory is defined as <span class="math">\(s_1, a_1, s_2, a_2, ..., s_T, a_T\)</span>. The probability of the trajectory is given by the sequential multiplicative composition of the policy <span class="math">\(\pi_\theta(a_t | s_t)\)</span> and the environment dynamics <span class="math">\(p(s_{t+1} | s_t, a_t)\)</span>, i.e. <span class="math">\(\pi_\theta(\tau) = p(s_1) \prod_{t=1} \pi_\theta(a_t | s_t) p(s_{t+1} | s_t, a_t)\)</span>. The goal of the agent is to obtain high rewards <em>on average</em>. Thus we have to maximize the sum of rewards on the average trajectory:</p>
<div class="math">$$
\max_\theta \ J = \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[\sum_{t} r_t \right].
$$</div>
<p>Here for simplicity we assume <span class="math">\(r_t\)</span> is a deterministic function returning the reward from taking action <span class="math">\(a_t\)</span> in state <span class="math">\(s_t\)</span>, i.e. <span class="math">\(r_t := r(s_t, a_t)\)</span>. If the reward is stochastic, we'd have to take the expectation over its distribution too. How does the reward depend on the policy weights <span class="math">\(\theta\)</span>? In general, if the reward function is differentiable, then the gradient <span class="math">\(\partial r_t / \partial a_t\)</span> will be available from the environment. These can then be comined with <span class="math">\(\partial a_t / \partial \theta\)</span> to optimize the policy directly. This is called <a href="https://aceofgreens.github.io/analytic_policy_gradients.html">analytic policy gradients</a> and is usually very efficient. The idea is quite straightforward: we use gradient descent to update the policy in a way that directly maximizes the rewards of the average rolled-out trajectory. Implementation is a bit tricky because it may require detaching the state in the reward calculation, i.e. <span class="math">\(r_t = r(\text{sg}[s_t], a_t)\)</span>.</p>
<p>If the environment is not differentiable it is common to treat it like a black box. The expectation over the executed trajectory can be approximated using a finite sample of <span class="math">\(N\)</span> trajectories, where we assume that the individual policy roll-outs are i.i.d:</p>
<div class="math">$$
\begin{equation}
\max_\theta \ J = \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[\sum_{t} r_t \right] = \int \pi_\theta(\tau) \Big(\sum_{t=1}r_t^\tau \Big) d\tau \approx \frac{1}{N}\sum_{i=1}^N \Big(\sum_{t=1} r_t^i\Big).
\end{equation}
$$</div>
<p>Here <span class="math">\(\tau\)</span> is a sampled trajectory, of which there are <span class="math">\(N\)</span>. Both <span class="math">\(r_t^i\)</span> and <span class="math">\(r_t^\tau\)</span> indicate the <span class="math">\(t\)</span>-th reward in a given trajectory. The gradient of the loss function, <span class="math">\(\nabla_\theta J(\theta)\)</span>, is given by</p>
<div class="math">$$
\begin{align}
\nabla_\theta J(\theta) &amp;= \int \nabla_\theta \pi_\theta(\tau) \Big(\sum_{t=1} r_t^\tau \Big) d\tau \\
&amp;= \int \pi_\theta(\tau) \nabla_\theta \log \pi_\theta(\tau) \Big(\sum_{t=1} r_t^\tau \Big) d\tau\\
&amp;= \mathbb{E}_{\tau \sim \pi_\theta(\tau)}\left[ \nabla_\theta \log \pi_\theta(\tau) \Big(\sum_{t=1} r_t \Big)\right].
\end{align}
$$</div>
<p>The last equation comes from the fact that <span class="math">\(\nabla_\theta \log \pi_\theta(\tau) = \nabla_\theta \pi_\theta(\tau) / \pi_\theta(\tau)\)</span>. Now, what is the gradient of the log probability of a given trajectory? Things simplify as follows:</p>
<div class="math">$$
\begin{align}
\nabla_\theta \log \pi_\theta(\tau) &amp;= \nabla_\theta \left[ \log \Big(p(s_1)\prod_{t=1}^T \pi_\theta(a_t | s_t) p(s_{t+1}|s_t, a_t) \Big) \right] \\
&amp;= \nabla_\theta \left[ \log p(s_1) + \sum_{t=1}^T \log \Big(\pi_\theta(a_t | s_t) p(s_{t+1}|s_t, a_t) \Big) \right] \\
&amp;= \sum_{t=1}^T \nabla_\theta \log \pi_\theta(a_t | s_t)
\end{align}
$$</div>
<p>Thus, we see that to update the policy, one only needs to compute the gradients of the log probability of the selected actions. This is natural, as the agent does not control the transition probabilities of the underlying MDP,</p>
<div class="math">$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[ \sum_{t=1} \nabla_\theta \log \pi_\theta(a_t | s_t) \Big( \sum_{t=1}r_{t} \Big) \right] \approx  \frac{1}{N} \sum_{i=1}^N \sum_{t=1} \nabla_\theta \log \pi_\theta(a_t^i | s_t^i) \Big( \sum_{t=1}r_{t}^i \Big).
$$</div>
<p>Here the sum of rewards uses the same index <span class="math">\(t\)</span> because it is factored out from the innermost sum and there is no ambiguity. The same value multiplies all the innermost terms. This setup is the classic REINFORCE algorithm [1]. To train the algorithm, we roll-out a trajectory and update the policy weights as <span class="math">\(\theta' = \theta + \eta \nabla_\theta J(\theta)\)</span>. The collected trajectory is discarded. No replay buffers are used. We can also roll-out a batch of trajectories and perform a batched update with them. But we cannot perform multiple sequential updates on the same trajectory (not yet).</p>
<p>Note that the REINFORCE algorithm simply formalizes the idea of "trial and error". The agent has to experience different rewards in order to get a sense of what works and the likelihood of which actions should be maximized. If the reward is the same everywhere, the agent doesn't learn because it does not learn to prioritize one action at the expense of another one. Same thing happens if the reward is zero - the agent does not learn. Thus, optimal policy learning is a <em>search</em> task. Unlike classification, regression, or other supervised tasks, here the agent requires variability in the reward in order to search more efficiently.</p>
<p>The problem with this basic algorithm is that its variance is too large. Since both <span class="math">\(a_t | s_t\)</span> and <span class="math">\(s_{t+1} | s_t, a_t\)</span> are random variables, the variance of <span class="math">\(\tau\)</span> can be huge and hence the updates <span class="math">\(\nabla_\theta J(\theta)\)</span> while unbiased, will be very inconsistent from iteration to iteration. To address this, we need to make a few modifications.</p>
<p>First, we don't need to have the full sum of rewards <span class="math">\(\sum_{t=1} r_t\)</span> at every single step. In reality, action <span class="math">\(a_t\)</span> only affects rewards <span class="math">\(r_t, r_{t+1}, r_{t+2}, ...,\)</span>, not <span class="math">\(r_1, ..., r_{t-1}\)</span>. Hence the log probability can be weighted by the "reward to go", <span class="math">\(G_t = \sum_{j=t} r_j\)</span>. With this change the update becomes</p>
<div class="math">$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[\sum_{t=1} \nabla_\theta \log \pi_\theta(a_t | s_t) G_t \right] = \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[\sum_{t=1} \nabla_\theta \log \pi_\theta(a_t | s_t) \sum_{j=t}^T r_j\right]
$$</div>
<p>This change improves credit assignment by weighing each log-prob with only the rewards that follow it. This reduces variance while still being unbiased. The underlying objective function optimized is the same. But the variance can be reduced even more.</p>
<p>The value of a state <span class="math">\(s_t\)</span> is defined as the total sum of discounted rewards, called <em>return</em>, collected from following the policy <span class="math">\(\pi\)</span> starting in this state. Likewise, the value of an action <span class="math">\(a_t\)</span> in a state <span class="math">\(s_t\)</span> is, similarly, the return from following <span class="math">\(\pi\)</span> after executing <span class="math">\(a_t\)</span> in <span class="math">\(s_t\)</span>:</p>
<div class="math">$$
\begin{align}
V^\pi(s_t) &amp;= \mathbb{E}_{a_t, s_{t+1}, a_{t+1}, ... \sim \pi(\tau)} \sum_{j=t} \gamma^{j - t} r_{j} \\
Q^\pi(s_t, a_t) &amp;= \mathbb{E}_{s_{t+1}, a_{t+1}, s_{t+2}, ... \sim \pi(\tau)} \sum_{j=t} \gamma^{j - t} r_{j}.
\end{align}
$$</div>
<p>The discounting factor <span class="math">\(\gamma\)</span> controls how myopic the agent is, i.e. how much it prefers rewards closer to the current timestep compared to farther into the future. A discount factor is needed to ensure finite returns in infinite-horizon settings. Naturally, we can replace the reward-to-go with <span class="math">\(Q(s_t, a_t)\)</span>. If the estimate of <span class="math">\(Q(s_t, a_t)\)</span> is unbiased, the estimate of the gradients will also be unbiased.</p>
<p>We can also use the <em>advantage</em>, <span class="math">\(A^\pi(s_t, a_t) = Q^\pi(s_t, a_t) - V^\pi(s_t)\)</span>. It quantifies how much better action <span class="math">\(a_t\)</span> is compared to the "average action" in <span class="math">\(s_t\)</span>. Substituting the advantage into the gradient estimate significantly reduces its variance:</p>
<div class="math">$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[\sum_{t=1} \nabla_\theta \log \pi_\theta(a_t | s_t) A_t \right].
$$</div>
<p>This change naturally brings us to actor-critic methods. An actor <span class="math">\(\pi_\theta\)</span> selects actions, while a critic estimates a kind of value for each action. This value could be the reward to go <span class="math">\(G_t\)</span>, or the Q-value <span class="math">\(Q(s_t, a_t)\)</span>, or the advantage <span class="math">\(A_t\)</span>. Since the critic, being a neural network, only approximates the true returns, the gradient estimates become a little biased but usually the faster learning from reduced variance is worth it.</p>
<p>There is another way to control the bias and variance. In a full Monte-Carlo setup we roll-out a trajectory, and then iterate backward to compute <span class="math">\(\sum_{j=t}^T \gamma^{j-t} r_j\)</span> for every timestep. We can also bootstrap the returns from future estimated returns. For example <span class="math">\(Q(s_t, a_t) = r_t + \gamma V(s_{t+1})\)</span> and <span class="math">\(A(s_t, a_t) = r_t + \gamma V(s_{t+1}) - V(s_t)\)</span>. By calling the learned critic at a future state, we bootstrap the value of the current state. One can choose different setups, e.g <span class="math">\(Q(s_t, a_t) = r_t + \gamma r_{t+1} + V(s_{t+2})\)</span>. This is a 2-step estimated return. The less rewards we accumulate, the more bias we introduce, but also the less variance in the gradients.</p>
<p>There is a smarter way to trade-off bias and variance when choosing the <span class="math">\(n\)</span>-step returns. We can use a parameter <span class="math">\(\lambda\)</span> which interpolates between all possible <span class="math">\(n\)</span>-step returns, of which there are infinitely many. This <span class="math">\(\lambda\)</span>-return is simply an exponentially-weighted average of <span class="math">\(n\)</span>-step returns with decay equal to <span class="math">\(\lambda\)</span>. If <span class="math">\(R_t^n\)</span> is the <span class="math">\(n\)</span>-step return, then</p>
<div class="math">$$
R_t(\lambda) = (1 - \lambda)\sum_{n=1}^\infty \lambda^{n-1} R_t^n.
$$</div>
<p>Efficient ways to calculate this are possible by starting from the end of the roll-out and going backwards. If <span class="math">\(\lambda=0\)</span>, we ge the standard single-step return. If <span class="math">\(\lambda=1\)</span>, we get the full Monte Carlo return, effectively of infinitely many steps. Computing the advantage as <span class="math">\(R_t(\lambda) - V(s_t)\)</span> is called <em>generalized advantage estimation</em>.</p>
<p>Now, note that we still cannot perform multiple updates on the same collected trajectory because we wouldn't be optimizing the specific objective above. To alleviate this, we need to use importance sampling, stating that <span class="math">\(\int p(x) f(x) dx = \int q(x) p(x)/q(x) f(x) dx\)</span>. In our case <span class="math">\(q(\cdot)\)</span> will be the old policy <span class="math">\(\pi_{\theta}\)</span> with which the trajectory was collected, and <span class="math">\(p(\cdot)\)</span> will be the current policy, <span class="math">\(\pi_{\theta'}\)</span>, after possibly many updates on the same trajectory.</p>
<div class="math">$$
\begin{align}
J(\theta')&amp; = \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[ \frac{\pi_{\theta'}(\tau)}{\pi_{\theta}(\tau)} \sum_{t=1}r_t \right] \\
\nabla_{\theta'} J(\theta') &amp; = \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[ \frac{\nabla_{\theta'} \pi_{\theta'}(\tau)}{\pi_\theta(\tau)} \sum_{t=1}r_t \right] = \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[ \frac{ \pi_{\theta'}(\tau)}{\pi_\theta(\tau)} \nabla_{\theta'} \log \pi_{\theta'}(\tau) \sum_{t=1}r_t \right] \\
&amp;= \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[ \Bigg(\frac{\prod_{t=1} \pi_{\theta'}(a_t | s_t)}{\prod_{t=1} \pi_{\theta}(a_t | s_t)} \Bigg) \left( \sum_{t=1} \nabla_{\theta'}\log \pi_{\theta'}(a_t | s_t) \right) \left( \sum_{t=1} r_t \right) \right]
\end{align}
$$</div>
<p>The more times we update on the same data, the more <span class="math">\(\theta'\)</span> becomes different from <span class="math">\(\theta\)</span>, and hence the more the first term in the last equation diverges from 1. This could very easily lead to vanishing or exploding gradients. To address this, we need to keep the policies close to each other, i.e. <span class="math">\(D_\text{KL}(\pi_{\theta'} || \pi_\theta) \le \delta\)</span>. This is the idea behind the Trust Region Policy Optimization (TRPO) algorithm [2]. There is a lot of math behind it, so we'll skip it, but the bottom line is that it improves stability by keeping the current policy sufficiently close to the old one.</p>
<p>Another method along the same idea of restricting how much the policy changes is Proximal Policy Optimization (PPO) [3]. Suppose <span class="math">\(r_t(\theta') = \pi_{\theta'}(a_t | s_t) / \pi_\theta(a_t | s_t)\)</span>. PPO simply works by maximizing the minimum between <span class="math">\(r_t(\theta)\)</span> and a clipped version of it:</p>
<div class="math">$$
L^{\text{CLIP}} = \mathbb{E}_t \left[ \min(r_t(\theta)A_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon)A_t \right].
$$</div>
<p>The loss function here is written only for a single transition. To understand this, consider a positive advantage equal to <span class="math">\(1\)</span>, so that in principle the agent should increase the probability of the selected <span class="math">\(a_t\)</span>. Suppose <span class="math">\(\epsilon = 0.2\)</span>. If <span class="math">\(r_t(\theta) = 5\)</span>, its clipped value is <span class="math">\(1.2\)</span> and the smaller between <span class="math">\(5\)</span> and <span class="math">\(1.2\)</span> is <span class="math">\(1.2\)</span>. Thus, the value to maximize is <span class="math">\(1.2\)</span> instead of <span class="math">\(5\)</span>, so this limits the magnitude of the update. If instead <span class="math">\(r_t(\theta) = 0.5\)</span>, the value to maximize is <span class="math">\(0.5\)</span>, as small updates are not penalized. If the advantage is <span class="math">\(-1\)</span>, and <span class="math">\(r_t(\theta) = 0.5\)</span>, then the objective value becomes <span class="math">\(-0.8\)</span> instead of <span class="math">\(-0.5\)</span>, so this acts as a pessimistic lower-bound on the unclipped objective.</p>
<p>PPO is the default algorithm for policy gradients. There are some variations which are in use, like DD-PPO which allows for synchronized parallel data-collection on many devices, but overall the method has held up nicely. So now that we've explored the method, let's take a look at an important application - reinforcement learning from human feedback - the main technique for aligning LLMs so that they respond in more human-likable manner.</p>
<p>Suppose we have a LLM that has been pretrained on a large corpus using next-token prediction, and then finetuned on a smaller dataset of question-answer pairs, so that it knows how to format its responses. People can still have preferences over how the LLM should respond and we'll try to align the LLM to them. Here is how RLHF works.</p>
<p>Let the initial policy be the LLM after supervised fine-tuning (SFT), <span class="math">\(\pi^{\text{SFT}}\)</span>. We select prompts <span class="math">\(x\)</span> and generate pairs of answers <span class="math">\((y_1, y_2) \sim \pi^{\text{SFT}}(y | x)\)</span>. Human reviewers then rank them by preference, denoted as <span class="math">\(y_w \succ y_l | x\)</span>, where <span class="math">\(y_w\)</span> is the preferred response and <span class="math">\(y_l\)</span> the dispreferred one. Now, in principle, we can assume that there is some underlying reward model (the equivalent of a utility function in economics) <span class="math">\(r^\ast\)</span> which generates these preferences. Our goal is then to learn an approximation <span class="math">\(r_\phi\)</span> as close as possible to <span class="math">\(r^\ast\)</span>. Note that we do not observe the true rewards <span class="math">\(r^\ast(x, y_1)\)</span> and <span class="math">\(r^\ast(x, y2)\)</span>, only their pairwise comparisons. <a href="https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model">Bradley-Terry</a> is a famous model establishing how to work with such preferences.</p>
<p>It models the probability that <span class="math">\(y_1\)</span> is preferred as given by </p>
<div class="math">$$
p(y_1 \succ y_2 |x) = \frac{\exp\big(r^\ast(x, y_1)\big)}{\exp\big(r^\ast(x, y_1)\big) + \exp\big(r^\ast(x, y_2)\big)}.
$$</div>
<p>We will learn a parameterized reward model <span class="math">\(r_\phi\)</span> to approximate <span class="math">\(r^\ast\)</span>. To do so, we view preference prediction as binary classification. The negative log-likelihood of the Bradley-Terry model directly becomes the loss function with which we can train <span class="math">\(r_\phi\)</span>:</p>
<div class="math">$$
\mathcal{L}_R(r_\phi, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \Big[ \log \sigma\big(r_\phi(x, y_w) - r_\phi(x, y_l) \big) \Big].
$$</div>
<p>If the reward for the preferred response is much larger than the reward for the dispreffered one, then their differences is a positive large number, its sigmoid is 1, and the total loss after the log becomes 0. Otherwise, the sigmoid becomes close to 0, and the final loss is very large. So this loss function makes sense. Once we have the reward model, we formulate the RL loss as:</p>
<div class="math">$$
\max_\theta \ \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(y | x)} \big[r_\phi(x, y)\big] - \beta D_\text{KL}\big(\pi_\theta(y | x)  \ \Vert \  \pi^\text{SFT}(y | x)\big).
$$</div>
<p>This encourages the model to maximize the reward while not deviating too much from the SFT policy. It also prevents mode-collapse to single high-reward answers. Of course this is very difficult to optimize, so instead we redefine the rewards as follows, and maximize using PPO: </p>
<div class="math">$$
r(x, y) = r_\phi(x, y) - \beta\big( \log \pi_\theta(y | x) - \log \pi^\text{SFT}(y|x)\big).
$$</div>
<p>This setup is too complicated. We can simplify as follows, though we'll skip the derivations. First, one can prove that the policy solving the optimization problem above with the KL has the form:</p>
<div class="math">$$
\pi_r(y | x) = \frac{1}{Z(x)}\pi^\text{SFT}(y | x) \exp\big(\frac{1}{\beta}r(x, y)\big).
$$</div>
<p>From here we can take the log and solve for <span class="math">\(r(x, y)\)</span>:</p>
<div class="math">$$ 
r(x, y) = \beta\log\frac{\pi_r(y | x)}{\pi^\text{SFT}(y|x)} + \beta\log Z(x).
$$</div>
<p>This is very convenient because in the Bradley-Terry model the probability that one sample is preferred only depends on the differences of rewards, not their actual values. We can then construct a maximum likelihood objective for a parametrized policy <span class="math">\(\pi_\theta\)</span>:</p>
<div class="math">$$
\mathcal{L}_\text{DPO}(\pi_\theta; \pi^\text{SFT}) = -\mathbb{E}_{x, y_w, y_l \sim \mathcal{D}} \left[ \log \sigma \Bigg( \beta \log \frac{\pi_\theta(y_w |x)}{\pi^\text{SFT}(y_w|x)} - \beta\log \frac{\pi_\theta(y_l | x)}{\pi^\text{SFT}(y_l | x)} \Bigg) \right]
$$</div>
<p>This method is called Direct Preference Optimization (DPO) and is very powerful. Can you find the reward model <span class="math">\(r_\phi\)</span> in it? Exactly, it's not there. In fact, the equation shows that we can finetune the LLM policy directly on human preferences, without needing to train a reward model beforehand and without needing to do RL. By finetuning with this loss the LLM learns the underlying reward function implicitly, which saves us a lot of trouble. Neat.</p>
<h3>References</h3>
<p>[1] Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018. <br>
[2] Schulman, John, et al. <a href="https://proceedings.mlr.press/v37/schulman15.html">Trust Region Policy Optimization.</a> Proceedings of the 32nd International Conference on Machine Learning, vol. 37, 2015, pp. 1889-1897. <br>
[3] Schulman, John, et al. <a href="https://arxiv.org/abs/1707.06347">Proximal policy optimization algorithms.</a> arXiv preprint 1707.06347 (2017). <br>
[4] Wijmans, Erik, et al. <a href="https://arxiv.org/abs/1911.00357">Dd-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames.</a> arXiv preprint arXiv:1911.00357 (2019). <br>
[5] Ouyang, Long, et al. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html">Training language models to follow instructions with human feedback.</a> Advances in neural information processing systems 35 (2022): 27730-27744. <br>
[6] Rafailov, Rafael, et al. <a href="https://arxiv.org/abs/2305.18290">Direct preference optimization: Your language model is secretly a reward model.</a> Advances in Neural Information Processing Systems 36 (2024).</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = "https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML";

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'], ['$', '$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div>
  
  <p class="post-meta">
    Tag: rl
  </p>

  </article>


    </div>
  </main>


<footer class="site-footer h-card">
  <div class="wrapper">
  
  <p></p>
  

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">


    <div class="footer-col-wrapper">

      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">The Critical Section</li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          <li><a href="https://github.com/aceofgreens"><i class="fa fa-github"></i> GitHub</a></li>


        </ul>
      </div>


      <div class="footer-col footer-col-3">
        <p>A personal blog for artificial intelligence and similar topics.</p>
      </div>
    </div>

  </div>
</footer>

<script type="text/javascript">
function welcomeFunction() {
  var items = document.getElementsByTagName("code");
    for (var i = items.length; i--;) {
      items[i].setAttribute("class", "highlight");
  }
}
</script>

</body>
</html>