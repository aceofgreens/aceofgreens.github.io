<!DOCTYPE html>
<html lang="en">

<head>
    <title>Value-Based Methods | The Critical Section</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Pelican" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <link rel="stylesheet" href="/assets/main.css" />
    <link rel="icon" href="/assets/favicon.png" />
    <link href="https://fonts.googleapis.com/css?family=Roboto|Oswald|Open+Sans" rel="stylesheet">


    <meta name="description" content="This post is a brief summary of the different classes of value-based RL methods. Personally, I find this topic incredibly rewarding in terms of its ability to provide precious deep intuition about how the RL algorithm landscape is spread out. Fundamental concepts like on-policy, off-policy, bootstrapping, and many others, all stem from these simple settings." />

    <meta name="tags" content="rl" />

</head>

<body onload="welcomeFunction()" >

  <header class="site-header" role="banner">
    <div class="wrapper">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> <!--An icon library for the button icon-->

      <a class="site-title" rel="author" href="/">The Critical Section</a>

      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
            <a class="page-link" href="/about.html">About</a>
            <a class="page-link" href="/posts.html">Posts</a>
            <a class="page-link" href="/tags.html">Tags</a>
        </div>

      </nav>

    </div>
  </header>


  <main class="page-content" aria-label="Content">
    <div class="wrapper">

  <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <header class="post-header">
      <h1 class="post-title p-name" itemprop="name headline">Value-Based Methods</h1>
      <p class="post-meta">
        <time class="dt-published" datetime="2025-02-11T07:00:00+02:00" itemprop="datePublished">
          11 Feb 2025
        </time>
      </p>

    </header>

    <div class="post-content">
      <p>This post is a brief summary of the different classes of value-based RL methods. Personally, I find this topic incredibly rewarding in terms of its ability to provide precious deep intuition about how the RL algorithm landscape is spread out. Fundamental concepts like on-policy, off-policy, bootstrapping, and many others, all stem from these simple settings.</p>
<p>Consider a Markov decision process (MDP) <span class="math">\((\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)\)</span> where <span class="math">\(\mathcal{S}\)</span> are the states, <span class="math">\(\mathcal{A}\)</span> the actions, <span class="math">\(\mathcal{P}\)</span> the transition probabilities, <span class="math">\(\mathcal{R}\)</span> the rewards and <span class="math">\(\gamma\)</span> is the discount rate. To most clearly understand how different algorithmic aspects are motivated by different problem settings, we'll only look at the setting where both the states <span class="math">\(\mathcal{S}\)</span> and actions <span class="math">\(\mathcal{A}\)</span> are discrete. Supposing also that there are not too many of them, it becomes possible to <em>enumerate</em> the state-action pairs. This is called the <em>tabular</em> setting.</p>
<p><strong>Discrete state-action pairs</strong>. In the tabular setting you don't need a neural network to predict the values of states and actions. Instead, you can simply instantiate a big table of Q-values and periodically update them as you are improving the policy. Thus, deep RL is only needed when either the state space is continuous or it is discrete but too large to represent the Q-values as a table. Many board games have combinatorially exploding state spaces and easily fit these settings. In those cases a neural network can effectively compress a huge number of state-action values into a fixed number of weights.</p>
<p><strong>Basic definitions</strong>. Assume experience traces come in the form of <span class="math">\((s_0, a_0, r_1, s_1, a_1, r_2, ...)\)</span>. We distinguish between the random variables <span class="math">\((S_t, A_t, R_{t+1})\)</span> and their realizations <span class="math">\((s_t, a_t, r_{t+1})\)</span>. Using standard notation, the discounted sum of rewards is <span class="math">\(G_t = \sum_{k=t+1}^T \gamma^{k-t-1}R_k\)</span>, where <span class="math">\(T\)</span> can be arbitrarily far into the future and <span class="math">\(R_k\)</span> is the random variable representing the reward at step <span class="math">\(k\)</span>. Naturally, the return is a random variable with a very complicated distribution. The value function assigns a value to a state and is defined as</p>
<div class="math">$$
v_\pi(s) = \mathbb{E}_{\pi} \left[ G_t | S_t = s \right] = \mathbb{E}_\pi \left[ \sum_{k=0}^\infty \gamma^k R_{t+k+1} | S_t = s\right], \ \forall s \in \mathcal{S}.
$$</div>
<p>Likewise, the state-action value function is similar, it assigns a value to individual actions:</p>
<div class="math">$$
q_\pi(s, a) = \mathbb{E}_{\pi} \left[ G_t | S_t = s , A_t = a\right] = \mathbb{E}_\pi \left[ \sum_{k=0}^\infty \gamma^k R_{t+k+1} | S_t = s, A_t=a \right].
$$</div>
<p>These relationships are recursive. The Bellman equations express the value of the current state <span class="math">\(s\)</span> using the value of the future state <span class="math">\(s'\)</span>. This holds for any policy <span class="math">\(\pi\)</span>:</p>
<div class="math">$$
\begin{align}
v_\pi(s) &amp;= \mathbb{E}_\pi\left[G_t | S_t = s \right] = \mathbb{E}_\pi \left[ R_{t+1} + \gamma G_{t+1} | S_t = s \right] \\ 
&amp;= \sum_{a} \pi(a|s) \sum_{s', r} p(s', r | s, a) \Big[r + \gamma v_\pi(s') \Big], \\
q_\pi(s, a) &amp;= \mathbb{E}_\pi\left[G_t | S_t = s, A_t = a \right] = \mathbb{E}_\pi \left[ R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a \right] \\ 
&amp;= \sum_{s', r} p(s', r | s, a) \Big[r + \gamma v_\pi(s') \Big] = \sum_{s', r} p(s', r | s, a) \Big[r + \gamma \sum_{a'}\pi(a'|s')q_\pi(s', a') \Big].
\end{align}
$$</div>
<p>Policies are functions that assign actions to states, <span class="math">\(\pi: \mathcal{S} \rightarrow \mathcal{A}\)</span>. Value functions define a partial ordering over policies. A policy <span class="math">\(\pi\)</span> is deemed better than policy <span class="math">\(\pi'\)</span> if and only if the value of state <span class="math">\(s\)</span> obtained from policy <span class="math">\(\pi\)</span> is greater or equal than the value of state <span class="math">\(s\)</span> from policy <span class="math">\(\pi'\)</span>, for all states:</p>
<div class="math">$$
\pi \ge \pi' \ \Longleftrightarrow \ v_\pi(s) \ge v_{\pi'}(s), \ \forall s \in \mathcal{S}.
$$</div>
<p>In that sense, there is an optimal policy <span class="math">\(\pi_\ast\)</span> whose values <span class="math">\(v_\ast\)</span> are greater than the values obtained from any other policy. There is always a unique optimal state-value function <span class="math">\(v_\ast\)</span> and action-value function <span class="math">\(q_\ast\)</span>, but there may be multiple optimal policies <span class="math">\(\pi_\ast\)</span> that attain them.</p>
<p>An optimal policy also has a recursive nature. To see this, consider that the value <span class="math">\(v_\ast(s)\)</span> must be the highest Q-value at that state, <span class="math">\(v_\ast(s) = \max_a q_\ast(s, a)\)</span>, otherwise the policy will not be optimal. Similarly, the optimal Q-value at <span class="math">\((s, a)\)</span> requires that we take the best action from the next state <span class="math">\(s'\)</span>. These are called the Bellman <em>optimality</em> equations and they play a key role in many algorithms:</p>
<div class="math">$$
\begin{align}
v_\ast(s) &amp;= \max_a q_\ast(s, a) = \max_a \sum_{s', r} p(s', r | s, a) \Big[ r + \gamma v_\ast(s') \Big] \\
q_\ast(s, a) &amp;= \sum_{s', r} p(s', r | s, a) \left[ r + \gamma \max_{a'} q_\ast(s', a') \right].
\end{align}
$$</div>
<p>A very convenient way to represent these equations is through backup diagrams, as shown in Fig. 1. States are shown as circles, while actions as squares. Arcs show max-aggregation operations.</p>
<figure>
    <img class='img' src="/images/bellman_optimality_state.svg" alt="Bellman optimality state" width="1200">
    <figcaption> Figure 1: Backup diagrams for the Bellman optimality equations.</figcaption>
</figure>

<p><strong>Dynamic programming</strong>. Consider the case when the environment dynamics <span class="math">\(p(s', r | s, a)\)</span> are known. In that case there is no need to do <em>learning</em>, we can evaluate and optimize policies directly. Specifically, we can just run multiple sweeps across all the states and use the known dynamics to estimate the value functions for the current policy <span class="math">\(\pi\)</span>. The process of obtaining <span class="math">\(V_\pi(\cdot)\)</span> from <span class="math">\(\pi\)</span> is called <em>policy evaluation</em> and looks something like the following:</p>
<ol>
<li>Instantiate <span class="math">\(V(s)\)</span> arbitrarily</li>
<li>For every state <span class="math">\(s\)</span>, set <span class="math">\(V(s) \leftarrow \sum_a \pi(a|s) \sum_{s', r}p(s', r | s, a)[r + \gamma V(s')]\)</span>.</li>
<li>Repeat step 2. until a stopping criterion is met.</li>
</ol>
<p><strong>Bootstrapping</strong>. Dynamic programming uses what's called bootstrapping - updating the value of the current state using an estimate of the value of a neighboring state. In that sense, we're pulling ourselves by our own bootstraps. It also uses full-width backups, meaning that we are using the known dynamics to aggregate information about all possible next states <span class="math">\(s'\)</span> when updating <span class="math">\(s\)</span>.</p>
<p>Policy evaluation could be implemented in a synchronous manner where we update the values of all states at the same time. This requires having two copies of the current value function. It can also be implemented in an asynchronous way, where the values of different states are updated in different order in-place. All it matters to ensure the eventual convergence is that over time all states continue to be selected for updates. </p>
<p>To improve the policy we need to use <span class="math">\(V_\pi(\cdot)\)</span> to obtain a new <span class="math">\(\pi\)</span>. A simple way to do it is to make the policy greedy with respect to the value function in every state, which is a guaranteed improvement: <span class="math">\(\pi(a | s) = \text{arg}\max_a \sum_{s', r}p(s', r | s, a)[r + \gamma V(s')]\)</span>. Once we improve the policy we can re-evaluate it and improve it again, and again, and again. This <em>policy iteration</em> approach of repeatedly evaluating the policy and improving it is common to pretty much all value-based methods. It also allows us to be flexible in terms of how many sweeps we use to evaluate the policy before we improve it. The special case of using a single sweep is called value iteration.</p>
<figure>
    <img class='img' src="/images/policy_evaluation_dyn_prog.svg" alt="Bellman optimality state" width="1200">
    <figcaption> Figure 2: Backup diagrams for the policy evaluation and Q-policy iteration. Here we simply aggregate neighboring values according to their probabilities. The left diagram finds $v_\pi(s)$, the right one finds $q_\pi(s, a)$.</figcaption>
</figure>

<p><strong>Monte Carlo</strong>. Let's now depart from the assumption of known dynamics. The class of techniques which estimate an optimal policy without knowing or learning the dynamics is called <em>model-free</em> methods. Crucially, if you don't know the probabilities <span class="math">\(p(s' | s, a)\)</span> you need to sample the state space and this is the biggest difference compared to dynamic programming - the agent will now <em>learn from experience</em>. It will play out different policies, collecting data, and then use the collected data to update its value estimates.</p>
<p>The need to collect different trajectories brings out also the need to explore. This stems from the fact that rewards provide an inherently evaluative feedback to the agent, they tell it whether some action is good or bad, without telling it which action is better/worse. Contrast this to supervised learning, where due to the presence of gradients, there is instructive feedback precisely guiding the agent in how to change its predictions.</p>
<p>So, the idea behind Monte Carlo methods is that the agent will rollout entire trajectories, from start to finish, and then will estimate values for the traversed states based on the collected rewards. Thus, instead of expected returns, it will estimate empirical returns as follows:</p>
<ol>
<li>Initialize <span class="math">\(V(s)\)</span> arbitrarily and <span class="math">\(\text{Returns}(s)\)</span> an empty list for all <span class="math">\(s\)</span>.</li>
<li>Collect a number of traces <span class="math">\(\tau\)</span>.</li>
<li>For every trace and every state <span class="math">\(s\)</span> appearing in it, append its return <span class="math">\(G_t\)</span> to <span class="math">\(\text{Returns}(s)\)</span>.</li>
<li>Set <span class="math">\(V(s) \leftarrow \text{average}(\text{Returns}(s))\)</span>, for all states <span class="math">\(s\)</span>.</li>
</ol>
<p>This simple algorithm evaluates <span class="math">\(v_\pi(s)\)</span>. We can use the same procedure to evaluate also <span class="math">\(q_\pi(s, a)\)</span>. It can be implemented in an online (update policy after every episode) or batched manner. It does not use bootstrapping, because we're updating the value function using only the true collected rewards, not estimates of them. Because of that it has high variance, but is unbiased. It also requires that episodes have finite length.</p>
<figure>
    <img class='small_img' src="/images/mc_trace.svg" alt="Bellman optimality state" width="1200">
    <figcaption> Figure 3: Backup diagrams for Monte Carlo policy iteration. The agent performs sample trace backups - rolls out a policy until a terminal state, indicated with a T, and estimates mean empirical returns. The dashed arrows show that rewards are obtained from the corresponding transitions.    </figcaption>
</figure>

<p>What about MC control? We follow the broad approach of repeatedly evaluating the policy and improving it. Policy improvement consists of making the policy greedy with respect to the Q-values, <span class="math">\(\pi(s) = \text{arg}\max_a q(s,a)\)</span>. We use the Q-values because they do not require knowing the environment dynamics. However, to guarantee convergence we need to make sure that given infinitely many episodes, all states <span class="math">\(s\)</span> are visited infinitely many times. One way to achieve this is with <em>exploring starts</em> - by starting each episode from a different state we would ensure that all possible states are eventually experienced. This is unrealistic though, and hence it's more typical for the agent to purposefully explore on its own, by using soft policies, where there is a <span class="math">\(\epsilon\)</span> chance of taking a random action. This ensures that any <span class="math">\((s, a)\)</span> pair has a nonzero probability of occurring.</p>
<p>All learning control methods face a dilemma: they seek to learn action values conditional on subsequent optimal behavior, but they need to behave non-optimally in order to explore all actions (to find the optimal actions). This leads to on-policy and off-policy learning. On-policy methods compromise in that they learn action values not for the optimal policy, but for a near-optimal policy that still explores. Off-policy methods on the other hand use two policies - a <em>behaviour policy</em> for exploring, and a <em>target policy</em> that we want to actually optimize.</p>
<p>On-policy MC control works by having an <span class="math">\(\epsilon\)</span>-greedy policy collecting trajectories. Then, as we're collecting returns and updating the Q-values, we update the policy as </p>
<div class="math">$$
\pi(a | S_t) \leftarrow \begin{cases}
1 - \epsilon + \epsilon/|\mathcal{A}(S_t) |, \ \ &amp;\text{if } a = \text{arg}\max_a Q(S_t, a) \\
\epsilon /  |\mathcal{A}(S_t) |, \ \ &amp;\text{otherwise.}\\
\end{cases}
$$</div>
<p>The off-policy MC control setup is more complicated and our discussion will be necessarily brief. It involves the target policy <span class="math">\(\pi\)</span> and the behaviour policy <span class="math">\(b\)</span> and corrects the returns <span class="math">\(G_t\)</span> according to the importance sampling ratio <span class="math">\(\prod_{k=t}^{T-1} \frac{\pi(A_k | S_k)}{b(A_k | S_k)}\)</span>. There are many ways to do it: ordinary importance sampling simply averages the weighted returns from many <span class="math">\((s, a)\)</span> occurrences. It is unbiased but can have unbounded variance. On the other hand, weighted importance sampling takes the weighted mean of the returns, weighted by their importance ratios, which has bias (converging asymptotically to zero) and drammatically lower variance.</p>
<p><strong>Temporal difference learning</strong>. The most popular type of learning setup is actually temporal difference (TD) learning. Unlike MC, which performs sample trace backups, TD performs sample transition backups. Hence we don't need to wait for the episode to end, we just bootstrap the value of the current state from the value of the next experienced state. Since initially the value function is incorrect, TD produces biased estimates, yet the variance is much smaller than that of the MC methods, which usually translates to faster learning.</p>
<p>TD algorithms work by collecting episodes and then updating the value functions using individual transitions from the episodes. <span class="math">\(V(s)\)</span> is updated by moving <span class="math">\(V(S_t)\)</span> towards <span class="math">\(R_{t+1} + \gamma V(S_{t+1})\)</span> and similarly for <span class="math">\(Q(S_t, A_t)\)</span>. If the learning rate <span class="math">\(\alpha\)</span> is decreasing properly, convergence is guaranteed: </p>
<div class="math">$$
\begin{align}
V(S_t) &amp;\leftarrow V(S_t) + \alpha\big(R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\big) \\
Q(S_t, A_t) &amp;\leftarrow V(S_t) + \alpha\big(R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\big) \\
\end{align}
$$</div>
<p>Backup diagrams for evaluating <span class="math">\(v_\pi(s)\)</span> and <span class="math">\(q_\pi(s, a)\)</span> are shown in Fig. 4. The main benefit compared to MC is that TD allows us to learn in infinite episodes, for example if the task is continuing from before. It also allows us to naturally learn in an online manner, as we don't have to wait until the episode finishes.</p>
<figure>
    <img class='img' src="/images/td_trace.svg" alt="Bellman optimality state" width="1200">
    <figcaption> Figure 4: Backup diagrams for TD algorithms. Left is policy evaluation. Middle is Q-policy iteration as used in Sarsa. Right is the TD Bellman optimality equation, as used in Q-learning. </figcaption>
</figure>

<p><strong>Sarsa</strong>. When it comes to control, the on-policy TD algorithm is called Sarsa. The name comes from the quintuple <span class="math">\((s, a, r, s', a')\)</span>. We initialize the Q-values arbitrarily and set the initial policy to be <span class="math">\(\epsilon\)</span>-greedy with respect to the Q-values. We collect transitions using that policy and update the Q-values according to the on-policy transitions. <span class="math">\(Q(s, a)\)</span> is updated toward <span class="math">\(r + \gamma Q(s', a')\)</span>, with <span class="math">\(a \sim \pi(s')\)</span>. The policy is improved by making it <span class="math">\(\epsilon\)</span>-greedy with respect to the new, updated Q-values. Since both the transitions collected and the Bellman targets are coming from the <span class="math">\(\epsilon\)</span>-greedy policy, this is an on-policy method.</p>
<p><strong>Q-learning</strong>. The off-policy TD control algorithm is the classic Q-learning, where everything is the same except that we update the Q-values as</p>
<div class="math">$$
Q(S, A) \leftarrow Q(S, A) + \alpha \left[ R + \gamma \max_a Q(S', a) - Q(S, A)\right].
$$</div>
<p>Why is this off-policy? Because the behaviour policy collecting the data is <span class="math">\(\epsilon\)</span>-greedy wrt Q, while the Bellman targets with which the policy is updated are actually greedy wrt Q. This is the mismatch. The Bellman target follows the form in the corresponding optimality equation.</p>
<p>Note that Q-learning suffers from a selection bias. Because usually the estimated Q-values are noisy, and because we always select the best one, it may happen that because of noise, a poor action is selected more often than it should. This may lead to overly optimistic Q-values and may hurt learning. The solution is to have two sets of Q-values, <span class="math">\(Q_1\)</span> and <span class="math">\(Q_2\)</span>, one for selecting the action and one for evaluating it. The value used in the Bellman target is <span class="math">\(Q_2(s, \text{arg}\max_a Q_1(s, a))\)</span>.</p>
<p>Another thing is that for all <span class="math">\(\epsilon\)</span>-greedy policies the initial starting Q-values greatly affect exploration. If the true range of Q-values in a given state is <span class="math">\([0, 1]\)</span>, but we initialize them in the range <span class="math">\([10, 11]\)</span>, then all of actions will be tried out sooner rather than later because of the <span class="math">\(\epsilon\)</span>-greedy policy. On the other hand, if one of the Q-values is initialized to <span class="math">\(-1\)</span>, then it's likely that it will rarely be picked, only ever by chance. This particular trick to encourage/discourage exploration is called optimistic/pessimistic initialization. Similarly, if we have some kind of uncertainty signal like prediction error, or information gain, and we use it to select for execution those actions with the highest such signal, this is called <em>optimism in the face of uncertainty</em>.</p>
<p><strong>Multistep bootstrapping</strong>. Finally, one should recognize that a TD target like <span class="math">\(r + \gamma V(s')\)</span> bootstraps from only one step ahead. But it may use more than that, e.g. <span class="math">\(r + \gamma r' + \gamma^2 r'' + \gamma^3 V(s''')\)</span>. All on-policy and off-policy methods could be adapted to work in this manner. This kind of multistep bootstrapping trades off increased variance with decreased bias. The more we rely on bootstrapping, the more biased is the estimate.</p>
<p>This completes our basic overview of value-based model-free tabular RL methods. The two main dimensions of interest are depth and width of the update. Depth refers to how much they bootstrap. Width refers to whether they rely on sample updates (from a trajectory) or expected updates (from a distribution of possible trajectories). TD methods have small depth (single transition) and small width (sampling). MC methods have large depth (entire traces) and small width (sampling). Dynamic programming has small depth and large width (one-step expected updates). Exhaustive search has large depth and width. </p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = "https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML";

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'], ['$', '$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div>
  
  <p class="post-meta">
    Tag: rl
  </p>

  </article>


    </div>
  </main>


<footer class="site-footer h-card">
  <div class="wrapper">
  
  <p></p>
  

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">


    <div class="footer-col-wrapper">

      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">The Critical Section</li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          <li><a href="https://github.com/aceofgreens"><i class="fa fa-github"></i> GitHub</a></li>


        </ul>
      </div>


      <div class="footer-col footer-col-3">
        <p>A personal blog for artificial intelligence and similar topics.</p>
      </div>
    </div>

  </div>
</footer>

<script type="text/javascript">
function welcomeFunction() {
  var items = document.getElementsByTagName("code");
    for (var i = items.length; i--;) {
      items[i].setAttribute("class", "highlight");
  }
}
</script>

</body>
</html>