<!DOCTYPE html>
<html lang="en">

<head>
    <title>The Ising Model | The Critical Section</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Pelican" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <link rel="stylesheet" href="/assets/main.css" />
    <link rel="icon" href="/assets/favicon.png" />
    <link href="https://fonts.googleapis.com/css?family=Roboto|Oswald|Open+Sans" rel="stylesheet">


    <meta name="description" content="The Ising model is a curious little toy model for ferromagnetism in the field of statistical mechanics. While surprisingly simple in terms of definition, it is able to produce very interesting various outcomes, whose explanations are in my opinion, quite deep. This post aims to give a quick introduction to the model, how to sample from it, and how to solve it analytically in one dimension." />

    <meta name="tags" content="cs" />

</head>

<body onload="welcomeFunction()" >

  <header class="site-header" role="banner">
    <div class="wrapper">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> <!--An icon library for the button icon-->

      <a class="site-title" rel="author" href="/">The Critical Section</a>

      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
        </label>

        <div class="trigger">
            <a class="page-link" href="/about.html">About</a>
            <a class="page-link" href="/posts.html">Posts</a>
            <a class="page-link" href="/tags.html">Tags</a>
        </div>

      </nav>

    </div>
  </header>


  <main class="page-content" aria-label="Content">
    <div class="wrapper">

  <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <header class="post-header">
      <h1 class="post-title p-name" itemprop="name headline">The Ising Model</h1>
      <p class="post-meta">
        <time class="dt-published" datetime="2022-05-11T16:00:00+02:00" itemprop="datePublished">
          11 May 2022
        </time>
      </p>

    </header>

    <div class="post-content">
      <p>The Ising model is a curious little toy model for ferromagnetism in the field of statistical mechanics. While surprisingly simple in terms of definition, it is able to produce very interesting various outcomes, whose explanations are in my opinion, quite deep. This post aims to give a quick introduction to the model, how to sample from it, and how to solve it analytically in one dimension.</p>
<h3>Introduction</h3>
<p>The Ising model consists of a grid of <span class="math">\(n\)</span> sites, where each site <span class="math">\(s_i\)</span> can have a positive spin or negative spin, i.e. <span class="math">\(s_i \in  \{+1, -1\}\)</span>. Let's call <span class="math">\(n\)</span> the size of the system. Then just based on that, we can say that the system has <span class="math">\(N = 2^n\)</span> possible states, where each state consists of the actual spins for all sites. </p>
<p>The energy of each spin depends also on the spins of the neighboring sites and in a <span class="math">\(2\)</span>-dimensional Ising model these neighbors are the sites to the north, south, east and west. The interaction between two neighboring states <span class="math">\(s_i\)</span> and <span class="math">\(s_j\)</span> is modeled with the term <span class="math">\(-s_i s_j\)</span> which suggests a ferromagnetic relationship where this quantity is <span class="math">\(1\)</span> if <span class="math">\(s_i\)</span> and <span class="math">\(s_j\)</span> have different spins and <span class="math">\(-1\)</span> otherwise. The state of the system, consisting of all spins <span class="math">\(s_i\)</span>, has overall energy <span class="math">\(E(s) = -\sum_i \sum_j s_i s_j\)</span>. Since physical systems generally try to be in a state that minimizes their energy, we have it that the system will be in a minimum-energy state if the spins at all sites are the same (which is what keeps the system magnetized).</p>
<p>Importantly, the state in which the system will eventually end up may not be the minimum energy one. We model that equilibrium state as a random variable with a Boltzmann distribution:</p>
<div class="math">$$
P_{eq}(s) = \frac{\exp(-\beta E(s))}{\sum_s \exp(-\beta E(s))}.
$$</div>
<p>Here <span class="math">\(E(s)\)</span> is the energy of the state as defined above and <span class="math">\(\beta = \frac{1}{T}\)</span> is a coefficient depending on the temperature <span class="math">\(T\)</span>. Generally, if <span class="math">\(T \rightarrow \infty\)</span>, and therefore <span class="math">\(\beta \rightarrow 0\)</span>, the system is equally likely to be in any of the states. If, on the other hand, <span class="math">\(T \rightarrow 0\)</span>, and therefore <span class="math">\(\beta \rightarrow \infty\)</span>, the system has positive probability of being only in states with minimal energy.</p>
<p>Typically, we might be interested in the magnetization, defined as the average spin <span class="math">\(m = \frac{1}{n}\sum_i s_i\)</span> over all sites and how it depends on the temperature <span class="math">\(T\)</span>.</p>
<p>Moving slightly into "physics" territory, we know that of the <span class="math">\(N=2^n\)</span> possible states, many will have the same energy. We can group these states into <em>macrostates</em>. The probability that we end up in a given macrostate is, again, given by the Boltzmann distribution and is proportional to <span class="math">\(W \mathrm{e}^{-\beta E}\)</span>, where <span class="math">\(W\)</span> is the number of states in a given macrostate with energy <span class="math">\(E\)</span>. The log of <span class="math">\(W\)</span> is called the entropy, labeled <span class="math">\(S\)</span>, and since <span class="math">\(W = \mathrm{e}^{\ln W}\)</span>, we can express <span class="math">\(W \mathrm{e}^{-\beta E}\)</span> as <span class="math">\(\mathrm{e}^{S - \beta E} = \mathrm{e}^{-\beta (E - TS)}\)</span>. The quantity <span class="math">\(E - TS\)</span> is the <em>free energy</em> and the most likely state is the one with minimal free energy.</p>
<p>The samples from the equilibrium distribution of the Ising model are very different, depending on the temperature. As it turns out, when <span class="math">\(n \rightarrow \infty\)</span>, there is a critical temperature <span class="math">\(T_c\)</span> above which the system is unmagnetized, and below which it is highly magnetized. At and very close near the critical temperature the system exhibits scale-free behaviour (see figure 1 below).</p>
<p>Above the critical temperature, states tend to become equally likely and the equilibrium distribution becomes uniform as <span class="math">\(T \rightarrow \infty\)</span>. The spin in site <span class="math">\(s_i\)</span> can be "up" or "down" with equal probability, irrespective of the neighboring spins. At finite but large temperatures, the spins are clumped together in very small clusters whose correlation to other clusters falls off rapidly with the distance.</p>
<p>Below the critical temperature all is still in the arctic wilderness. Even the tiniest movement faces an unfathomable amount of resistence. The system is magnetized and most of the spins are either "up" or "down", with only very small clusters scattered throughout. These clusters have finite size and their distribution has an exponential tail. Correlations in the state space are strong and wide.</p>
<p>At the critical temperature <span class="math">\(T_c\)</span>, the average size of the clumps of correlated spins diverges. The size of each cluster has a power-law distribution and thus much larger, and in some cases even arbitrarily large clusters can appear. They are also scale-free, meaning that if we "zoom out", the resulting distribution will be the same.</p>
<figure>
    <img class='extra_big_img' src="/images/ising_samples.png" alt="Ising samples" width="1200">
    <figcaption>Figure 1: Three samples from the Ising model. The left one is a perfect sample from the equilibrium distribution at supercritical temperature. The middle one comes from an approximation to the equlibrium distribution at the critical temperature. The right one is also a sample from an approximated equilibrium, but at subcritical temperature.</figcaption>
</figure>

<h3>Approximate sampling</h3>
<p>Sampling from the equilibrium distribution at a certain temperature can be done using a Markov chain with Metropolis dynamics. In principle a simple idea is to select a random site <span class="math">\(s_i\)</span> and compute the change in the energy <span class="math">\(\Delta E\)</span> that would result if we flip its spin. Then if the flip reduces the energy, we do it. If it increases the energy, we still do it, but with probability <span class="math">\(\exp (-\Delta E /T)\)</span>:</p>
<div class="math">$$
\begin{equation}
p(\text{flip}) = 
\begin{cases}
1 &amp; \text{ if } \Delta E &lt; 0\\
\mathrm{e}^{-\frac{1}{T} \Delta E} &amp; \text{ otherwise.}
\end{cases}
\end{equation}
$$</div>
<p>The Markov chain defined in this way satisfies the detailed balance condition and is ergodic. It thus converges to the true Boltzmann equilibrium distribution as we repeat the random spin flips ad infinitum.</p>
<p>The following Python code implements the algorithm. However, we implement a small trick that improves the running time. We note that the change in energy <span class="math">\(\Delta E\)</span> that we compute when flipping site <span class="math">\(s_i\)</span> depends only on <span class="math">\(s_i\)</span> and its neighbors. Nothing prevents us from updating also <span class="math">\(s_j\)</span> as long as <span class="math">\(s_i\)</span> and <span class="math">\(s_j\)</span> are not themselves neighbors. This means that we can update up to half of all the sites in a single step, instead of just one.</p>
<p>In practice the matrix
<span class="math">\(\small C = 
\begin{pmatrix}
0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0
\end{pmatrix}\)</span> contains indices for the four neighbors of a given site. By performing a convolution (technically a cross-correlation) on the current state with the neighbors matrix, we find the sum of the neighboring spins which is then used to compute the change in the energy. To update half of the sites, we construct a binary matrix with a checkerboard pattern that will select the sites to update. On the next iteration we take the complement of that checkerboard pattern to select the other sites. We flip the spins according to the Metropolis probability defined above, but only for those sites where the checkerboard matrix is <span class="math">\(1\)</span>. We repeat these steps for a selected number of times.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.signal</span> <span class="kn">import</span> <span class="n">correlate2d</span>

<span class="k">def</span> <span class="nf">ising_model_metropolis_sample</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">T</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Iterates the Ising model Metropolis dynamics for a fixed number</span>
<span class="sd">    of iterations. Returns a sample from the approximate equilibrium distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iters</span><span class="p">):</span>
        <span class="n">j</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">j</span>
        <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">indices</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">indices</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span>

        <span class="n">delta_E</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">s</span> <span class="o">*</span> <span class="n">correlate2d</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">boundary</span><span class="o">=</span><span class="s1">&#39;wrap&#39;</span><span class="p">)</span>
        <span class="n">cond</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">X</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">delta_E</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">s</span><span class="p">[</span><span class="n">cond</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">s</span><span class="p">[</span><span class="n">cond</span><span class="p">]</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">delta_E</span><span class="o">/</span><span class="n">T</span><span class="p">)</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
        <span class="n">cond</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">X</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">delta_E</span> <span class="o">&gt;=</span><span class="mi">0</span><span class="p">,</span> <span class="n">U</span> <span class="o">&lt;</span> <span class="n">probs</span><span class="p">)))</span>
        <span class="n">s</span><span class="p">[</span><span class="n">cond</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">s</span><span class="p">[</span><span class="n">cond</span><span class="p">]</span>       
    <span class="k">return</span> <span class="n">s</span>
</code></pre></div>

<p>Not bad. This works pretty well, as long as <span class="math">\(n\)</span>, the side of the square grid, is an even number. If it's not, the convolution boundary, which is wrapped around the edges, gets messed up, leading to some very interesting but wrong regular patterns (see figure 2).</p>
<figure>
    <img src="/images/ising_sample_wrong.png" alt="Ising sample wrong" width="750">
    <figcaption>Figure 2: The convolution edge wrapping leads to erroneous regular patterns when the square grid has a side with odd length.</figcaption>
</figure>

<p>The sampling approach based on Metropolis dynamics works well but is approximate, because while we do converge to the equilibrium, we never really sample from it - we sample from its approximation. Sampling perfectly from the <em>exact</em> equilibrium is possible with a beautiful technique called <a href="https://en.wikipedia.org/wiki/Coupling_from_the_past">Coupling from the past</a>. However, the runtime for obtaining even a single sample becomes prohibitive even for very small <span class="math">\(n\)</span> when the temperature is below its critical point. Therefore, its main strength is to sample perfectly from the unmagnetized states when the temperature is supercritical.</p>
<h3>Solving the Ising model</h3>
<p>Solving the Ising model analytically roughly means obtaining expressions for the energy and magnetization as the temperature changes. Let's recall that the normalizing constant of the Boltzmann distribution is <span class="math">\(Z = \sum_{S} \mathrm{e}^{-\beta E(S)}\)</span>. This quantity is very important in physics, where it's called <em>the partition function</em>.</p>
<p>The partition function is important because many quantities of interest can be expressed as a function of it. For example, the mean energy at equilibrium is given by <span class="math">\(\mathbb{E}[E] = -\frac{\partial}{\partial \beta}\ln Z\)</span>, the variance by
<span class="math">\(\text{Var}(E) = \frac{\partial^2}{\partial \beta^2}\ln Z\)</span>, and the heat capacity by 
<span class="math">\(\frac{\partial }{\partial T} \mathbb{E}[E]\)</span>. The quantity that appears most in these expressions is <span class="math">\(\ln Z\)</span> and typically we call <span class="math">\(\lim_{n \rightarrow \infty} \frac{\ln Z}{n}\)</span> the <em>free energy per site</em> of the system. The goal is, therefore, to calculate this quantity as all other interesting variables are just functions of it.</p>
<p>The partition function for a one-dimensional Ising chain is </p>
<div class="math">$$Z = \sum_{\{s_i\}} \mathrm{e}^{-\beta E(S)} = \sum_{\{s_i\}} \mathrm{e}^{\beta \sum_ i s_i s_{i+1}} = \sum_{\{s_i\}} \prod_i \mathrm{e}^{\beta s_i s_{i+1}}. $$</div>
<p>We simply want to calculate this quantity. This can be done in an inductive manner. Let <span class="math">\(Z_n\)</span> be the partition function for a chain of size <span class="math">\(n\)</span> vertices. Then we separate <span class="math">\(Z_n\)</span> into two parts, representing the number of chains of length <span class="math">\(n\)</span> where the last site points up or down, respectively: <span class="math">\(Z_n = Z_n^+ + Z_n^-\)</span>. Now, if we add a new site at the end, it can change the partition function in two ways. If its spin is the same as the spin of the last site, then the new edge has an energy of <span class="math">\(-1\)</span> and the contribution to <span class="math">\(Z\)</span> is a factor of <span class="math">\(\mathrm{e}^{\beta}\)</span>. If the new vertex has a different spin than the last one, then it contributes a factor of <span class="math">\(\mathrm{e}^{-\beta}\)</span>. We get the following relationships between <span class="math">\(Z_n\)</span> and <span class="math">\(Z_{n+1}\)</span>:</p>
<div class="math">$$\small
\begin{pmatrix}
Z_{n+1}^+ \\
Z_{n+1}^- \\
\end{pmatrix} =
M 
\begin{pmatrix}
Z_{n}^+ \\
Z_{n}^- \\
\end{pmatrix}
\text{, where } M = 
\begin{pmatrix}
\mathrm{e}^\beta &amp; \mathrm{e}^{-\beta} \\
\mathrm{e}^{-\beta} &amp; \mathrm{e}^\beta \\
\end{pmatrix}
$$</div>
<p>The matrix <span class="math">\(M\)</span> describes how the partition function changes as <span class="math">\(n\)</span> increases. Importantly <span class="math">\(n\)</span> here is a spacial dimension, not a temporal one. The eigenvalues of <span class="math">\(M\)</span> are <span class="math">\(\lambda_1 = \mathrm{e}^\beta + \mathrm{e}^{-\beta} = 2 \cosh \beta\)</span> and <span class="math">\(\lambda_2 = \mathrm{e}^\beta - \mathrm{e}^{-\beta} = 2 \sinh \beta\)</span>. As <span class="math">\(n \rightarrow \infty\)</span>, <span class="math">\(\lambda_1\)</span> dominates and <span class="math">\(Z\)</span> grows as</p>
<div class="math">$$
Z_n \sim \lambda_1^n = (2 \cosh \beta)^n
$$</div>
<p>Then, the asymptotic free energy per site is</p>
<div class="math">$$
f = \lim_{n \rightarrow \infty} \frac{\ln Z}{n} = \ln(2 \cosh \beta).
$$</div>
<p>The expected energy per site is then </p>
<div class="math">$$\mathbb{E}[E] = -\frac{1}{n} \frac{\partial}{\partial \beta} \ln Z = -\tanh \beta$$</div>
<p>and the variance of the energy per site is</p>
<div class="math">$$
\text{Var}(E) = \frac{1}{n} \frac{\partial^2}{\partial \beta^2} \ln Z = 1 - \tanh^2 \beta. 
$$</div>
<figure>
    <img src="/images/equilibrium_energy.png" alt="Ising equilibrium energy" width="750">
    <figcaption>Figure 3: The statistics of the equilibrium energy of the one-dimensional Ising model. As the temperature increases, the mean of the energy increases converging to 0, because the system becomes less magnetized. The variance also increases and converges to 1 as the states become equally probable.</figcaption>
</figure>

<p>Obtaining an exact solution for the two-dimensional case is possible, but complicated. Overall, I think the one-dimensional Ising model is quite useful because it yields a lot of intuition compared to the effort for solving it.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = "https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML";

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'], ['$', '$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div>
  
  <p class="post-meta">
    Tag: cs
  </p>

  </article>


    </div>
  </main>


<footer class="site-footer h-card">
  <div class="wrapper">
  
  <p></p>
  

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">


    <div class="footer-col-wrapper">

      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">The Critical Section</li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          <li><a href="https://github.com/aceofgreens"><i class="fa fa-github"></i> GitHub</a></li>


        </ul>
      </div>


      <div class="footer-col footer-col-3">
        <p>A personal blog for artificial intelligence and similar topics.</p>
      </div>
    </div>

  </div>
</footer>

<script type="text/javascript">
function welcomeFunction() {
  var items = document.getElementsByTagName("code");
    for (var i = items.length; i--;) {
      items[i].setAttribute("class", "highlight");
  }
}
</script>

</body>
</html>