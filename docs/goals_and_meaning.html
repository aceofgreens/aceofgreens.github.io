<!DOCTYPE html>
<html lang="en">

<head>
    <title>Goals and Meaning | The Critical Section</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Pelican" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <link rel="stylesheet" href="/assets/main.css" />
    <link rel="icon" href="/assets/favicon.png" />
    <link href="https://fonts.googleapis.com/css?family=Roboto|Oswald|Open+Sans" rel="stylesheet">


    <meta name="description" content="Previously, I've tried to convince you that the utter desire to live at any cost is not something fundamental or designed with any global purpose. It is a purely random quality which has serendipitously proved useful in increasing our fitness in the environment. As a result those who have the will to live survive, and the others do not. There was a time when the probability of life suceeding has been marginal, but luckily this time is long gone. The qualities that aid life have become stable and ubiquitous, and life flourishes. Given that, we still don't know how exactly it is that humans give meaning to their existence. Are there any observable patterns that can shine a light on this conundrum? Can we solve the plight of how we attribute meaning?" />

    <meta name="tags" content="ai" />
    <meta name="tags" content="ultra-rationalism" />
    <meta name="tags" content="rl" />

</head>

<body onload="welcomeFunction()" >

  <header class="site-header" role="banner">
    <div class="wrapper">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> <!--An icon library for the button icon-->

      <a class="site-title" rel="author" href="/">The Critical Section</a>

      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
        </label>

        <div class="trigger">
            <a class="page-link" href="/about.html">About</a>
            <a class="page-link" href="/posts.html">Posts</a>
            <a class="page-link" href="/tags.html">Tags</a>
        </div>

      </nav>

    </div>
  </header>


  <main class="page-content" aria-label="Content">
    <div class="wrapper">

  <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <header class="post-header">
      <h1 class="post-title p-name" itemprop="name headline">Goals and Meaning</h1>
      <p class="post-meta">
        <time class="dt-published" datetime="2022-07-26T16:00:00+02:00" itemprop="datePublished">
          26 Jul 2022
        </time>
      </p>

    </header>

    <div class="post-content">
      <p>Previously, I've <a href="https://aceofgreens.github.io/2022/05/19/why_do_beetles_love_life.html">tried to convince you</a> that the utter desire to live at any cost is not something fundamental or designed with any global purpose. It is a purely random quality which has serendipitously proved useful in increasing our fitness in the environment. As a result those who have the will to live survive, and the others do not. There was a time when the probability of life suceeding has been marginal, but luckily this time is long gone. The qualities that aid life have become stable and ubiquitous, and life flourishes. Given that, we still don't know how exactly it is that humans give meaning to their existence. Are there any observable patterns that can shine a light on this conundrum? Can we solve the plight of how we attribute meaning?</p>
<p>I will be borrowing concepts from reinforcement learning (RL) to illustrate my points. RL offers a simplified framework for agents acting purposefuly in an uncertain environment and while by simplified I mean <em>very simplified</em>, I find that such a schematic has incredible value - first because it keeps the complexity of the topic manageable, and secondly because it keeps the discussion on a sufficiently high level, above the nitty-gritty noisy details.</p>
<p>In most RL settings, the agent has global purpose. The task is phrased in the context of a <a href="https://en.wikipedia.org/wiki/Markov_decision_process">Markov Decision Process</a> (MDP). The agent observes the current state of the world <span class="math">\(s_t\)</span> and selects an action <span class="math">\(a_t\)</span>. After executing that action, the environment responds by returning a new state <span class="math">\(s_{t+1}\)</span>, i.e. the agent observes the consequences of his actions. Importantly, in this setting, the environment returns also a reward <span class="math">\(r_t\)</span> depending on the state <span class="math">\(s_t\)</span> and the action <span class="math">\(a_t\)</span>. The goal of the agent is to act in such a way that on average any trajectory (sequence of actions) yields maximum rewards.</p>
<figure>
    <img class='small_img' src="/images/mdp.svg" alt="The MDP" width="1200">
    <figcaption>Figure 1: The standard MDP setting.</figcaption>
</figure>
<p>Notice that in this setting the reward function - how the rewards depend on the agent's actions - is predefined. The reward is typically engineered to be positive in those states which are desirable and negative in those which are bad. The agent will try to go to those states yielding high rewards and as a result, we can think of reward functions as goals. Any reward function defines a goal - a preference over states of the world. The relationship between reward functions and goals is not injective however. There are many reward functions which correspond to the same goal. In fact, the <a href="https://en.wikipedia.org/wiki/Reinforcement_learning#Inverse_reinforcement_learning">inverse problem</a> of learning a reward function from optimal trajectories is ill-defined.</p>
<p>Since it may be hard to solve the MDP task using only the task-based reward <span class="math">\(r_t\)</span>, it has become common to add a second reward component which comes from the agent itself. This leads to a setting where we have both task-based rewards and intrinsic rewards <span class="math">\(\hat{r}_t\)</span>. These rewards could incentivize curiosity (by choosing actions whose outcome is more uncertain), safety (by choosing actions whose outcome is less likely to be bad), or even predetermined behaviour.</p>
<figure>
    <img class='small_img' src="/images/intrinsic_rewards.svg" alt="Intrinsic rewards" width="1200">
    <figcaption>Figure 2: An intrinsically-motivated agent in a MDP setting.</figcaption>
</figure>
<p>The most important intrinsic reward is what I call the <strong>curiosity itch</strong>. It constitues those rewards encouraging exploration, curiosity, and creativity. The curiosity itch is important because
it is only through tinkering and fiddling around, that we explore more of the state space. We live in a universe where positive rewards are sparse, and we have to explore to find the best course of action. Theory is important, sure, but coming up with a theory is not an act of interaction with the environment. It does not lead to rewards. Scientific theory is just a model of the dynamics of the MDP that we call life.</p>
<p>The RL setting most close to us is actually the autotelic one, where there are only intrinsic, endogenously-generated rewards. There are no task-based performance signals here, just raw curiosity and inner drive, depending on the agent itself. This is the setting of our world. The question "What should I aim for in the next 10 years?" is ambiguous because no one can make that decision for you. There isn't any intelligent designer (apart from evolution) who has engineered us to prefer peace over war, or tranquility over commotion.</p>
<figure>
    <img class='small_img' src="/images/autotelic.svg" alt="Autotelic setting" width="1200">
    <figcaption>Figure 3: The autotelic setting with only intrinsic rewards.</figcaption>
</figure>

<p>So what is that reward function? It surely has a curiosity component, a biochemical component to regulate our biological needs, and a psychological component whose desires aim to keep our mental models in tact.
But I bet you that none of these are consciously chosen by us. I expect that the human's reward system, our <a href="https://en.wikipedia.org/wiki/Reward_system">mesocorticolimbic circuit</a> is just another evolutionary aspect guiding our adaptation. The rewards it generates are biochemical and we cannot change them, at least not naturally. The autotelic agent is still a puppet whose preferences over world states have evolved through a computational process.</p>
<p>Before moving on, let's explore one particular concept from Douglas Hofstadter's almighty book <a href="https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach">GÃ¶del, Escher, Bach</a>. While I disagree with some of his ideas, I believe this one is spot on and fits nicely in this discussion. Let's explore the <span class="math">\(pq\)</span>-system.</p>
<p>The <span class="math">\(pq\)</span>-system consists of theorems and axioms. Both of these are made up entirely out of the symbols <span class="math">\(p\)</span>, <span class="math">\(q\)</span> and <span class="math">\(-\)</span> (a hyphen). An axiom is any combination of these symbols of the form <span class="math">\(x p - q x-\)</span>, where <span class="math">\(x\)</span> and <span class="math">\(y\)</span> consist of only hyphens. So, for example, all of the following strings belong to the <span class="math">\(pq\)</span>-system as axioms.</p>
<div class="math">$$
\begin{aligned}
&amp; - p - q - - \\
&amp; - - p - q - - - \\
&amp; - - - p - q - - - - \\
&amp; - - - - p - q - - - - - \\
\end{aligned}
$$</div>
<p>Apart from axioms, the <span class="math">\(pq\)</span>-system also has a rule for defining theorems. Theorems are strings for which we can prove they belong to the system. The only difference between axioms and theorems is that axioms are in the system by virtue of their definition, while for theorems, we can prove their inclusion using logic. That logic is called theorem-production rules. In the <span class="math">\(pq\)</span>-system the rule for defining new theorems is as follows: if <span class="math">\(x\)</span>, <span class="math">\(y\)</span>, and <span class="math">\(z\)</span> are strings containing hyphens only, and if <span class="math">\(x p y q z\)</span> is a theorem, then <span class="math">\(x p y - q z -\)</span> is also a theorem. So, for example, if</p>
<div class="math">$$
--p--q-----
$$</div>
<p>is a theorem, then by the theorem production rule, </p>
<div class="math">$$
--p---q------
$$</div>
<p>is also a theorem. The central question of the <span class="math">\(pq\)</span>-system is to determine whether any given string is in the system or not. This boils down to finding out if the given string is a theorem. For example, is <span class="math">\(- p q - q -\)</span> a theorem? No, because starting from the axiomatic strings, even if we apply the theorem production rule as many times as we want, the resulting string will never be <span class="math">\(- p q - q -\)</span>. On the other hand, is <span class="math">\( - - p - - - q - - - - -\)</span> a theorem? Yes, because starting from the axiom <span class="math">\( - - p - q - - -\)</span> and applying the production rule 2 times, we reach the required string.</p>
<p>Can you figure out a way to easily determine which strings are theorems? Have you spotted the regularity in the form of the theorems? As it turns out, a string is in the <span class="math">\(pq\)</span>-system if it represents a valid addition statement. <span class="math">\( - p - - q - - -\)</span> is in the system, because we can count the number of hyphens and reach <span class="math">\(1 + 2 = 3\)</span>. The <span class="math">\(p\)</span> stands for plus, and the <span class="math">\(q\)</span> for equals. The theorems simply resemble addition.</p>
<p>In reality, it is not that the <span class="math">\(pq\)</span>-system really performs addition, it's just that we have found a strong similarity between it and addition. We call this similarity meaning. Attaching meaning to an object means that we have found an isomorphism between the properties of the object and some of our previously known knowledge. This allows us to interpret the new object in the context of our previous knowledge.</p>
<p>With humans, I like to think that meaning and rewards are, likewise, closely related. Since rewards are sparse, it is not often that we find our actions and their results meaningful. I believe that meaning arises when we consider how our actions in the current state will increase or decrease the likelihood of obtaining rewards. Ultimately, we all have a mental model of past state-action-reward trajectories from which we learn. Our current actions become meaningful only when we find an isomorphism between the perceived effect of these actions in terms of reward collection and the past trajectories of collected state-action-reward transitions. In other words, I believe that the meaning we attribute to actions is ultimately tied to the biochemical rewards on which we depend.</p>
<p>This isomorphism idea of meaning is powerful, but it's not the only available one. We can also try an <a href="https://en.wikipedia.org/wiki/Operational_semantics">operational semantics</a> approach. This comes from computer science where the semantics (meaning) of an executable statement are defined to be the exact consequences of that statement and how it is to be executed by a, <em>broadly</em> speaking, interpreter.
For example, we can say that</p>
<div class="math">$$
\big ( \langle C_1, s \rangle \rightarrow s' \big ) \Rightarrow \big (\langle C_1; C_2, s \rangle \rightarrow \langle C_2, s' \rangle \big).
$$</div>
<p>This means that if a program <span class="math">\(C_1\)</span>, starting in state <span class="math">\(s\)</span>, changes the state to <span class="math">\(s'\)</span>, then piping two programs <span class="math">\(C_1\)</span> and <span class="math">\(C_2\)</span> sequentially and starting from <span class="math">\(s\)</span>, will change the state to whichever state <span class="math">\(C_2\)</span> changes it to, starting from <span class="math">\(s'\)</span>. This gives meaning to the sequential composition of two programs <span class="math">\(C_1;C_2\)</span> and this meaning is defined by following the operation of the program.</p>
<p>Note that this type of meaning, where the meaning of statements is based on how they are executed, is quite raw and primitive. It's very general, but it doesn't fit our setting of actions and rewards intuitively. 
We can always say that the programs <span class="math">\(C_1\)</span> and <span class="math">\(C_2\)</span> are two policies controlling our actions. In that sense, operational semantics tells us that the effect of composing two sequences of actions, given that we know how the first sequence will change the environment, is based on how the second sequence will change the environment. If we think of each state <span class="math">\(s\)</span> as having its own reward <span class="math">\(r\)</span>, then yes, actions sequences are meaningful, depending on the states <span class="math">\(s\)</span> and <span class="math">\(s'\)</span> through which you pass.</p>
<p>We've explored how rewards and meaning mix together. But what about the rewards themselves? How do they arise? Since rewards are biochemical, I think it's logical to say that the actions our reward system incentivizes are those that provoke the highest positive biochemical response. The agent seeks pleasure, delight, and satisfaction in an almost hedonistic way. Even agents that are far-sighted (with very high time preferences) still form goals based on the maximization of biochemical rewards.</p>
<p>It is not uncommon to find people whose ultimate desires are to "sit on the front porch, relax, drink whiskey, and smoke cigars". This maximizes their rewards and you can't blame them. What gives their actions meaning is how much each action contributes to achieving this blissful state. And yet, other more abstract goals like "become financially independent" are just as common. One possible explanation for their occurence is that these "extrinsic" rewards derive their motivational value as a result of learned association with real biochemical rewards. In order to enjoy the finest cigars I need to be financially independent, or so it goes. As a result, the extrinsic rewards of being financially secured become correlated with the intrinsic biochemcal ones.</p>
<p>Another thing is that our reward production is very, for lack of a better word, psychosomatic. If you're down or depressed, menial activities that usually produce immediate rewards may not produce now. Depressed people enjoy chocolate far less than non-depressed ones. In that sense, your mental health affects your physical health. Likewise, your physical state can also affect your mental state. For example, if you're desperately hungry, the reward from consumption is the only one that matters right now.</p>
<p>One small nitpick here is that the word psychosomatic implies, in some way, the existence of separate mental and physical "foundations". I think this form of <a href="https://en.wikipedia.org/wiki/Mind%E2%80%93body_dualism#Cartesian_dualism">Cartesian duality</a> is an absurd idea and is not at all what I mean. To be precise, reward production is a physical process, itself determined by many other physical processes - the heart maintains the blood flow, the brain maintains the mental simulation in which we live (sometimes eerily called the psychosphere) and so on. Reward is psychosomatic in the sense that both physical and psychological factors affect it. Nonetheless, psychological factors have physical precursors.</p>
<p>To summarize, humans are agents that learn through a sophisticated kind of reinforcement. A set of evolved intrinsic reward signals has helped us modulate our behaviours and adapt to the dynamic environment. Being autotelic agents, we attribute personal meaning to our actions, but this does not mean that we are free to choose that meaning. We maximize biochemical rewards. Good life can't be understood in any other way than through the psychosomatic dopamine secretions in your brain. This is what gives you meaning.
The observed abundance in meaning results from associating high level outcomes to low-level biochemical rewards. I'd like to think that this is all there is, at least in principle. We'll see...</p>
<!-- The process of assigning rewards to state-action pairs is called deriving a meaning. Actions which lead to rewards (positive or negative) are meaningful. What gives meaning is an isomorphism between your mental model and the observables from the environment.

What are the rewards - psychosomatic, 
association. -->

<!-- Main ideas: 
Association between higher order states of the world
psychosomatic rewards
Control through sensors
 -->

<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = "https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML";

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'], ['$', '$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div>
  
  <p class="post-meta">
    Tags: ai, rl, ultra-rationalism
  </p>

  </article>


    </div>
  </main>


<footer class="site-footer h-card">
  <div class="wrapper">
  
  <p></p>
  

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">


    <div class="footer-col-wrapper">

      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">The Critical Section</li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          <li><a href="https://github.com/aceofgreens"><i class="fa fa-github"></i> GitHub</a></li>


        </ul>
      </div>


      <div class="footer-col footer-col-3">
        <p>A personal blog for artificial intelligence and similar topics.</p>
      </div>
    </div>

  </div>
</footer>

<script type="text/javascript">
function welcomeFunction() {
  var items = document.getElementsByTagName("code");
    for (var i = items.length; i--;) {
      items[i].setAttribute("class", "highlight");
  }
}
</script>

</body>
</html>