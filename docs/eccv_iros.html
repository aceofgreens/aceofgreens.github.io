<!DOCTYPE html>
<html lang="en">

<head>
    <title>ECCV & IROS | The Critical Section</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Pelican" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <link rel="stylesheet" href="/assets/main.css" />
    <link rel="icon" href="/assets/favicon.png" />
    <link href="https://fonts.googleapis.com/css?family=Roboto|Oswald|Open+Sans" rel="stylesheet">


    <meta name="description" content="These are some brief notes from ECCV and IROS, two of the biggest AI conferences, held in early and mid October 2024, in less than 2 weeks apart. I attended both, which was very satisfying and tiring. This pose summarizes some findings and impressions." />

    <meta name="tags" content="ai" />
    <meta name="tags" content="rl" />

</head>

<body onload="welcomeFunction()" >

  <header class="site-header" role="banner">
    <div class="wrapper">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> <!--An icon library for the button icon-->

      <a class="site-title" rel="author" href="/">The Critical Section</a>

      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
            <a class="page-link" href="/about.html">About</a>
            <a class="page-link" href="/posts.html">Posts</a>
            <a class="page-link" href="/tags.html">Tags</a>
        </div>

      </nav>

    </div>
  </header>


  <main class="page-content" aria-label="Content">
    <div class="wrapper">

  <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <header class="post-header">
      <h1 class="post-title p-name" itemprop="name headline">ECCV &amp; IROS</h1>
      <p class="post-meta">
        <time class="dt-published" datetime="2024-10-15T07:00:00+02:00" itemprop="datePublished">
          15 Oct 2024
        </time>
      </p>

    </header>

    <div class="post-content">
      <p>These are some brief notes from ECCV and IROS, two of the biggest AI conferences, held in early and mid October 2024, in less than 2 weeks apart. I attended both, which was very satisfying and tiring. This pose summarizes some findings and impressions.</p>
<h3>The State of Computer Vision</h3>
<p>Milan is a nice city. I had already spent a lot of time there even before the conference, so my description here will be very brief. Last time I was there was about four years prior. In that time Milan seems to have grown well - it's becoming a modern, high-tech city. A unique feature is the all-you-can-eat sushi restaurants packed in the center. I'm only half-joking about this, they really do have great food. Anyway, it was nice to catch up on the latest political and tech issues with some fellow libertarian friends there.   </p>
<p>ECCV was awesome - very high quality content and signal-to-noise ratio, lots of workshops, posters and talks. There is <em>a lot</em> that you can learn from attending one of these conferences. Its high status and popularity are merited. Altogether, 5 days of awesome computer vision topics.</p>
<p>Let's start with self-supervised learning. DINO [1] came out in 2021 and has already proved its utility. It is widely used. However, without supervision its attention heads attend to different parts of the image and it's hard to distinguish what are the objects. There are methods [2] that can do object localization with zero annotations by looking at the correlation between different patch features. A binary similarity graph is computed by simply looking at which patches have a positive cosine similarity. Then, objects are those patches which have the smallest degree. Other methods formulate a graph-cut problem [3], use spectral clustering [4] or otherwise manipulate these kinds of graphs based on feature similarity. Other works detect the background by looking at the patches correlated to the one that receives the least attention [5].</p>
<p>Can we move from unsupervised localization to unsupervised segmentation? CLIP [33] offers the possibility for open-vocabulary segmentation but lacks spatial awareness. SSL methods usually have good spatial awareness without the need for labels. Hence, they can be combined. To extract dense semantic features from CLIP, one typically removes the last layer global attention pooling and replaces it with a <span class="math">\(1\times1\)</span> convolution, producing <span class="math">\(N\)</span> patch tokens of dimension <span class="math">\(d\)</span>. To do segmentation, you select a bunch of text classes, extract their embeddings, and compute the cosine similarity between each of them with each of the patch tokens, which is called <em>MaskCLIP</em> [6]. The resulting segmentation masks are noisy and there exist methods to improve them [7]. They work by averaging nearby CLIP features according to the similarity of the DINO patch features.</p>
<p>From the bigger models, there were EmuVideo [8], Cambrian-1 [9], VideoMamba [10], and Sapiens [11]. More and more works are exploring the problem of visual search. This is the problem of finding very small regions of interest in the image and answering questions about them. This is not an easy task. Suppose you are given a gigantic HD satellite image and I ask you about some small building. To find it, you'll likely have to iteratively zoom-in and zoom-out and otherwise make educated guesses. Naturally, this becomes a search task where you also need some form of working memory. A good paper here is <span class="math">\(V^*\)</span> [12]. Similarly, searching for a specific scene in a long video requires making smart guesses about whether it occurs before or after the current one.</p>
<p>Inspired by [13], there were also good papers that address various visual artifacts in DINO [14, 15]. Given the generality of these features, I find such works very useful. For autonomous vehicles, there were some good posters, including M<span class="math">\(^2\)</span>Depth [16], which uses neighboring temporal frames to better estimate surrounding metric depth, RealGen [17], which stores encoded trajectories in a memory bank and uses retrieval-augmented generation to combine similar trajectories into new ones, and DriveLM [18], which uses a vision-language model for driving and question answering.</p>
<figure>
    <img class='img' src="/images/dust3r.PNG" alt="DUSt3R" width="1200">
    <figcaption>Figure 1: DUSt3R. Consider that the method only receives two extremely different, almost opposite, views of the object, yet it manages to reconstruct it reasonably. The images on the left show the RGB views, obtained depth maps, predicted confidence maps, and the reconstruction. Image taken from [21].</figcaption>
</figure>

<p>There were a lot of works on controllable diffusion along the lines of [19], Gaussian splatting, 6D pose estimation, synthetic avatars, radiance fields, and many others. [20] was an interesting twist on <a href="https://en.wikipedia.org/wiki/Structure_from_motion">structure from motion</a>, where they use scene coordinate networks (networks which directly predict the 3D location of a pixel). Also, I was quite amazed by the DUSt3R line of work [21, 22, 23]. The idea I'd say is quite beautiful - have a network that simply predicts the 3D location of every pixel. The inputs are two image views. They are encoded and decoded using transformers. The self-attention only attends over tokens in the same view. The cross-attention attends also over tokens from the other view. The outputs are two pointmaps in <span class="math">\(\mathbb{R}^{H \times W \times 3}\)</span>, in the coordinate frame of the first camera, which can be easily supervised. Given these pointmaps, it is easy to find correspondences, find camera parameters, or estimate camera poses. </p>
<h3>The Desert Sessions</h3>
<p>Next up was IROS. We arrived in Doha, late in the evening, with a small delay, and barely cought our next flight to Abu Dhabz. The first moments in this new world were interesting. Locals, i.e. Arabs, are dressed in pristine white/black gowns and typically wear sandals and kanduras/hijabs. The nonlocals, usually immigrants, form the majority of the working-class and deal with menial tasks, manual labour and general work. When you go outside of the airport the hot wind hits you mercilessly in the face. Feels almost like Arizona, except that instead of cacti, there are palm trees. Unlike European cities, where you can get anywhere by walking or taking some public transportation, here the city is vast and less dense. The way to travel is by car. Luckily, Uber, an examplary product of capitalism, is available and is beyond convenient.</p>
<p>One particular nice sequence of events was after the first day of workshops and tutorials. Sunset was at 6 pm. You Uber to the observation deck at 300, the highest vantage point in the city and look at the skyscrapers which have filled out the space northeast of you. The view is unique, you don't see lines or textures when it's dark, only individual shimmering golden points, like in a point cloud. Then walk to the Marina Mall where one can observe the lavish lifestyle of the locals. Then, walk to the end of the dock for a nice view of the downtown. Finally, walk all the way back to the Corniche beach. While you can't swim at night, you can walk along the shore. I can finally say I've dipped my feet into the Persian Gulf. The Corniche Street itself is amazing - wide, exotic, with palm trees and beach boardwalks on the side. Reminds me of Orchard Rd, Singapore. One can also explore Corniche from the north, which is similarly memorable. You literally walk for quite a long time, with the waves on your right, and the palm trees and skyscrapers on your left.</p>
<p>In terms of robotics, I'm impressed by how many practical exhibitions there were. All of them were amazing, owing to the fact that I'm new to the field. Some of the random things I saw:</p>
<ul>
<li>Biomimetic robots for swimming or flying. These include fish-like softrobots and bird-like robots that actually have a set of wings that they can flap.</li>
<li>Aerial drone racing, this was quite impressive since the drones are fully-autonomous and fast. They fly in specifically designed cages (otherwise it's dangerous for the spectators) on a predefined route, marked by a number of square frames on which big QR code-like patterns are painted. The drone flyes by localizing these patterns and navigating through the frames. The goal is to go through the course as quickly as possible.</li>
<li>Lots of bipeds, quadrupeds and other kinds of robots. These were typically remote controlled. I was amazed to see some of them be able to go up and down stairs and other slanted surfaces. In fact, there was a whole course devoted to quadrupeds moving in hazardous terrains.</li>
<li>The first hanging of a robot. Just kidding, the robot was simply strapped across his neck, though it did resemble like he was hanging like a bag of sand in the air.</li>
<li>Robo soccer players moving clumsily across the field and hitting the ball only to miss the gate. They even let a bunch of humanoid robots play soccer against a bunch of quadrupeds. This was a fun sight, a bunch of robots moving around a small playground with the surrounding people cheering them loudly. What's next? Robo boxing?</li>
</ul>
<p>I find the robotics community quite down to earth, compared to the vision one. They explicitly state that their models and designs may not be always the best and usually avoid grandiose claims about solving this and this task. In general, current robotics is <em>very</em> far from being solved. People are starting to talk about robotic foundation models, although generalist models like OpenVLA [24] or Octo [25] are still rudimentary and do not work in the wild.</p>
<figure>
    <img class='img' src="/images/anymal_terrain_mapping.PNG" alt="Terrain mapping" width="1200">
    <figcaption>Figure 1: Terrain mapping. Consider all the tasks that need solving in order to navigate real-world terrain. Here, terrain is estimated as a grid-based elevation map with lower and upper confidence levels. Image taken from [26].</figcaption>
</figure>

<p>Robotics is a huge discipline and at IROS there were workshops and poster for literally any niche topic: multi-robot cooperation, path planning, mobile robots, maritime robotics, cybernetic avatars, manipulation, navigation, locomotion, all the different types of <a href="https://en.wikipedia.org/wiki/Odometry">odometry</a>, robotics for healthcare, <a href="https://en.wikipedia.org/wiki/Biomimetics">biomimetics</a>, <a href="https://en.wikipedia.org/wiki/Haptic_technology">haptics</a>, drones, <a href="https://en.wikipedia.org/wiki/Teleoperation">teleoperation</a>, embodied intelligence, neuromorphic cameras, simultaneous localization and mapping, sensor design, terrain estimation, autonomous vehicles, control, simulation, <a href="https://en.wikipedia.org/wiki/Soft_robotics">soft robotics</a>, biohybrid (cyborg) systems, and others.</p>
<p>A solid number of talks and presentations were about simultaneous localization and mapping (SLAM), the fundametal task of localizing the camera pose throughout a sequence of frames, and at the same time reconstructing the 3D scene captured by it. Starting from foundational works like iMap [27], we've come such a long way. Now there are SLAMs that use Nerfs [28] or GaussianSplats [29] for mapping, or that use RGB only instead of RGBD [30].</p>
<p>One of the most interesting ones, however, I think is Clio [31]. It tackles a fundamental problem in robotics - to create a useful map representation of the scene observed by the robot, where usefulness is measured by the ability of the robot to use the map to complete tasks of interest. This is important. Consider that with general class-agnostic segmentors like SAM [32] and open-set semantic embeddings like CLIP [33], we can now build maps with coutless semantic variations and objects. What is the right granularity for the representation? It is precisely that governed by the task. Robust perception relies on simultaneously understanding geometry, semantics, physics and relations in 3D. One approach to this is to build <em>scene graphs</em>. These are directed graphs where the nodes are spatial concepts and the edges are spatio-temporal relations. The key insight is that the scene graph needs to be hierarchical, owing to the fact that the environment can be described at different levels of abstraction. Which particular abstraction is utilized depends on the task.</p>
<figure>
    <img class='extra_big_img' src="/images/scene_graph.PNG" alt="Scene graphs" width="1200">
    <figcaption>Figure 2: Scene graphs, showing the scene representation captured by a Spot robot exploring. At the bottom we have a geometric photorealistic reconstruction. Within it, objects can be detected, shown as cubes. Objects are mapped to places, shown as spheres. Spheres are grouped into regions, shown as cubes again. Places and tasks are colored by their closest task. Image taken from [8].</figcaption>
</figure>

<p>Other notable activities during our stay there were eating camel burgers (tastes good, I recommend) and going to the Jubail mangrove park. We went there after sunset, yet managed to go through the boardwalks spanning a solid territory of mangrove forest. The ambience of this swamp is quite unique - you can hear the constant loud buzzing of crickets and cicada-like creatures, the water is relatively shallow and crystal clear, hence you can see the bottom and all the marina animals, mostly crabs and fish. Finally, lots of strange roots are sprouting up from the muddy ground. These are called <em>pneumatophores</em>, grow <a href="https://en.wikipedia.org/wiki/Aerial_root">above the ground</a>, and are common for mangrove-like plant habitats.</p>
<p>Another interesting location was the <a href="https://en.wikipedia.org/wiki/Sheikh_Zayed_Grand_Mosque">Sheikh Zayed Grand Mosque</a>, the biggest in the country. Right next to it, we had the IROS gala dinner, a nice outdoor dinner in the courtyard of the ERTH hotel. Overall, a highly enjoyable and authentic experience and a great conclusion to IROS 2024.</p>
<h3>References</h3>
<p>[1] Caron, Mathilde, et al. <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper">Emerging properties in self-supervised vision transformers.</a> Proceedings of the IEEE/CVF international conference on computer vision. 2021. <br>
[2] Siméoni, Oriane, et al. <a href="https://arxiv.org/abs/2109.14279">Localizing objects with self-supervised transformers and no labels.</a> arXiv preprint arXiv:2109.14279 (2021). <br>
[3] Wang, Yangtao, et al. <a href="https://ieeexplore.ieee.org/abstract/document/10224285">Tokencut: Segmenting objects in images and videos with self-supervised transformer and normalized cut.</a> IEEE transactions on pattern analysis and machine intelligence (2023).  <br>
[4] Melas-Kyriazi, Luke, et al. <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Melas-Kyriazi_Deep_Spectral_Methods_A_Surprisingly_Strong_Baseline_for_Unsupervised_Semantic_CVPR_2022_paper.html">Deep spectral methods: A surprisingly strong baseline for unsupervised semantic segmentation and localization.</a> Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022. <br>
[5] Siméoni, Oriane, et al. <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Simeoni_Unsupervised_Object_Localization_Observing_the_Background_To_Discover_Objects_CVPR_2023_paper.html">Unsupervised object localization: Observing the background to discover objects.</a> Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023. <br>
[6] Zhou, Chong, Chen Change Loy, and Bo Dai. <a href="https://link.springer.com/chapter/10.1007/978-3-031-19815-1_40">Extract free dense labels from clip.</a> European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022. <br>
[7] Wysoczańska, Monika, et al. <a href="https://arxiv.org/abs/2312.12359">Clip-dinoiser: Teaching clip a few dino tricks.</a> arXiv preprint arXiv:2312.12359 (2023). <br>
[8] Girdhar, Rohit, et al. <a href="https://arxiv.org/abs/2311.10709">Emu video: Factorizing text-to-video generation by explicit image conditioning.</a> arXiv preprint arXiv:2311.10709 (2023). <br>
[9] Tong, Shengbang, et al. <a href="https://arxiv.org/abs/2406.16860">Cambrian-1: A fully open, vision-centric exploration of multimodal llms.</a> arXiv preprint arXiv:2406.16860 (2024).<br>
[10] Li, Kunchang, et al. <a href="https://arxiv.org/abs/2403.06977">Videomamba: State space model for efficient video understanding.</a> arXiv preprint arXiv:2403.06977 (2024). <br>
[11] Khirodkar, Rawal, et al. <a href="https://link.springer.com/chapter/10.1007/978-3-031-73235-5_12">Sapiens: Foundation for Human Vision Models.</a> European Conference on Computer Vision. Springer, Cham, 2025. <br>
[12] Wu, Penghao, and Saining Xie. <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wu_V_Guided_Visual_Search_as_a_Core_Mechanism_in_Multimodal_CVPR_2024_paper.html">V<span class="math">\(^*\)</span>: Guided Visual Search as a Core Mechanism in Multimodal LLMs.</a> Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024. <br>
[13] Darcet, Timothée, et al. <a href="https://arxiv.org/abs/2309.16588">Vision transformers need registers.</a> arXiv preprint arXiv:2309.16588 (2023). <br>
[14] Yang, Jiawei, et al. <a href="https://arxiv.org/abs/2401.02957">Denoising vision transformers.</a> arXiv preprint arXiv:2401.02957 (2024). <br>
[15] Wang, Haoqi, Tong Zhang, and Mathieu Salzmann. <a href="https://link.springer.com/chapter/10.1007/978-3-031-72667-5_2">SINDER: Repairing the Singular Defects of DINOv2.</a> European Conference on Computer Vision. Springer, Cham, 2025. <br>
[16] Zou, Yingshuang, et al. <a href="https://link.springer.com/chapter/10.1007/978-3-031-72952-2_16">M<span class="math">\(^2\)</span> Depth: Self-supervised Two-Frame Multi-camera Metric Depth Estimation.</a> European Conference on Computer Vision. Springer, Cham, 2025. <br>
[17] Ding, Wenhao, et al. <a href="https://arxiv.org/abs/2312.13303">Realgen: Retrieval augmented generation for controllable traffic scenarios.</a> arXiv preprint arXiv:2312.13303 (2023). <br>
[18] Sima, Chonghao, et al. <a href="https://arxiv.org/abs/2312.14150">Drivelm: Driving with graph visual question answering.</a> arXiv preprint arXiv:2312.14150 (2023). <br>
[19] Wang, Xudong, et al. <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_InstanceDiffusion_Instance-level_Control_for_Image_Generation_CVPR_2024_paper.html">Instancediffusion: Instance-level control for image generation.</a> Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024. <br>
[20] Brachmann, Eric, et al. <a href="https://arxiv.org/abs/2404.14351">Scene Coordinate Reconstruction: Posing of Image Collections via Incremental Learning of a Relocalizer.</a> arXiv preprint arXiv:2404.14351 (2024).  <br>
[21] Wang, Shuzhe, et al. <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_DUSt3R_Geometric_3D_Vision_Made_Easy_CVPR_2024_paper.html">Dust3r: Geometric 3d vision made easy.</a> Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024. <br>
[22] Leroy, Vincent, Yohann Cabon, and Jérôme Revaud. <a href="https://arxiv.org/abs/2406.09756">Grounding Image Matching in 3D with MASt3R.</a> arXiv preprint arXiv:2406.09756 (2024). <br>
[23] Zhang, Junyi, et al. <a href="https://arxiv.org/abs/2410.03825">MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion.</a> arXiv preprint arXiv:2410.03825 (2024). <br>
[24] Kim, Moo Jin, et al. <a href="https://arxiv.org/abs/2406.09246">OpenVLA: An Open-Source Vision-Language-Action Model.</a> arXiv preprint arXiv:2406.09246 (2024). <br>
[25] Team, Octo Model, et al. <a href="https://octo-models.github.io/">Octo: An open-source generalist robot policy.</a> arXiv preprint arXiv:2405.12213 (2024). <br>
[26] Fankhauser, Péter, Michael Bloesch, and Marco Hutter. <a href="https://ieeexplore.ieee.org/abstract/document/8392399">Probabilistic terrain mapping for mobile robots with uncertain localization.</a> IEEE Robotics and Automation Letters 3.4 (2018): 3019-3026. <br>
[27] Sucar, Edgar, et al. <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Sucar_iMAP_Implicit_Mapping_and_Positioning_in_Real-Time_ICCV_2021_paper.html">imap: Implicit mapping and positioning in real-time.</a> Proceedings of the IEEE/CVF international conference on computer vision. 2021. <br>
[28] Zhu, Zihan, et al. <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_NICE-SLAM_Neural_Implicit_Scalable_Encoding_for_SLAM_CVPR_2022_paper.html">Nice-slam: Neural implicit scalable encoding for slam.</a> Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022. <br>
[29] Matsuki, Hidenobu, et al. <a href="https://rmurai.co.uk/projects/GaussianSplattingSLAM/">Gaussian splatting slam.</a> Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024. <br>
[30] Zhu, Zihan, et al. <a href="https://ieeexplore.ieee.org/abstract/document/10550721">Nicer-slam: Neural implicit scene encoding for rgb slam.</a> 2024 International Conference on 3D Vision (3DV). IEEE, 2024. <br>
[31] Maggio, Dominic, et al. <a href="https://arxiv.org/abs/2404.13696">Clio: Real-time Task-Driven Open-Set 3D Scene Graphs.</a> arXiv preprint arXiv:2404.13696 (2024). <br>
[32] Kirillov, Alexander, et al. <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html">Segment anything.</a> Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023. <br>
[33] Radford, Alec, et al. <a href="https://proceedings.mlr.press/v139/radford21a">Learning transferable visual models from natural language supervision.</a> International conference on machine learning. PMLR, 2021.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = "https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML";

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'], ['$', '$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div>
  
  <p class="post-meta">
    Tags: ai, rl
  </p>

  </article>


    </div>
  </main>


<footer class="site-footer h-card">
  <div class="wrapper">
  
  <p></p>
  

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">


    <div class="footer-col-wrapper">

      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">The Critical Section</li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          <li><a href="https://github.com/aceofgreens"><i class="fa fa-github"></i> GitHub</a></li>


        </ul>
      </div>


      <div class="footer-col footer-col-3">
        <p>A personal blog for artificial intelligence and similar topics.</p>
      </div>
    </div>

  </div>
</footer>

<script type="text/javascript">
function welcomeFunction() {
  var items = document.getElementsByTagName("code");
    for (var i = items.length; i--;) {
      items[i].setAttribute("class", "highlight");
  }
}
</script>

</body>
</html>