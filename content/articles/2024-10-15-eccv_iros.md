---
Title: ECCV & IROS
Date: 2024-10-15 07:00:00 +0200
Tags: ai, rl
slug: eccv_iros
---

These are some brief notes from ECCV and IROS, two of the biggest AI conferences, held in early and mid October 2024, in less than 2 weeks apart. I attended both, which was very satisfying and tiring.



### The Desert Sessions

Next up was IROS. We arrived in Doha, late in the evening, with a small delay, and barely cought our next flight to Abu Dhabz. The first moments in this new world were interesting. Locals, i.e. Arabs, are dressed in pristine white/black gowns and typically wear sandals and kanduras/hijabs. The nonlocals, usually immigrants, form the majority of the working-class and deal with menial tasks, manual labour and general work. When you go outside of the airport the hot wind hits you mercilessly in the face. Feels almost like Arizona, except that instead of cacti, there are palm trees. Unlike European cities, where you can get anywhere by walking or taking some public transportation, here the city is vast and less dense. The way to travel is by car. Luckily, Uber, an examplary product of capitalism, is available and is beyond convenient.

One particular nice sequence of events was after the first day of workshops and tutorials. Sunset was at 6 pm. You Uber to the observation deck at 300, the highest vantage point in the city and look at the skyscrapers which have filled out the space northeast of you. The view is unique, you don't see lines or textures when it's dark, only individual shimmering golden points, like in a point cloud. Then walk to the Marina Mall where one can observe the lavish lifestyle of the locals. Then, walk to the end of the dock for a nice view of the downtown. Finally, walk all the way back to the Corniche beach. While you can't swim at night, you can walk along the shore. I can finally say I've dipped my feet into the Persian Gulf. The Corniche Street itself is amazing - wide, exotic, with palm trees and beach boardwalks on the side. Reminds me of Orchard Rd, Singapore. One can also explore Corniche from the north, which is similarly memorable. You literally walk for quite a long time, with the waves on your right, and the palm trees and skyscrapers on your left.

In terms of robotics, I'm impressed by how many practical exhibitions there were. All of them were amazing, owing to the fact that I'm new to the field. Some of the random things I saw:

- Biomimetic robots for swimming or flying. These include fish-like softrobots and bird-like robots that actually have a set of wings that they can flap.
- Aerial drone racing, this was quite impressive since the drones are fully-autonomous and fast. They fly in specifically designed cages (otherwise it's dangerous for the spectators) on a predefined route, marked by a number of square frames on which big QR code-like patterns are painted. The drone flyes by localizing these patterns and navigating through the frames. The goal is to go through the course as quickly as possible.
- Lots of bipeds, quadrupeds and other kinds of robots. These were typically remote controlled. I was amazed to see some of them be able to go up and down stairs and other slanted surfaces. In fact, there was a whole course devoted to quadrupeds moving in hazardous terrains.
- The first hanging of a robot. Just kidding, the robot was simply strapped across his neck, though it did resemble like he was hanging like a bag of sand in the air.
- Robo soccer players moving clumsily across the field and hitting the ball only to miss the gate. They even let a bunch of humanoid robots play soccer against a bunch of quadrupeds. This was a fun sight, a bunch of robots moving around a small playground with the surrounding people cheering them loudly. What's next? Robo boxing?

I find the robotics community quite down to earth, compared to the vision one. They explicitly state that their models and designs may not be always the best and usually avoid grandiose claims about solving this and this task. In general, current robotics is *very* far from being solved. People are starting to talk about robotic foundation models, although generalist models like OpenVLA [1] or Octo [2] are still rudimentary and do not work in the wild.

<figure>
    <img class='img' src="/images/anymal_terrain_mapping.PNG" alt="Terrain mapping" width="1200">
    <figcaption>Figure 1: Terrain mapping. Consider all the tasks that need solving in order to navigate real-world terrain. Here, terrain is estimated as a grid-based elevation map with lower and upper confidence levels. Image taken from [3].</figcaption>
</figure>

Robotics is a huge discipline and at IROS there were workshops and poster for literally any niche topic: multi-robot cooperation, path planning, mobile robots, maritime robotics, cybernetic avatars, manipulation, navigation, locomotion, all the different types of [odometry](https://en.wikipedia.org/wiki/Odometry), robotics for healthcare, [biomimetics](https://en.wikipedia.org/wiki/Biomimetics), [haptics](https://en.wikipedia.org/wiki/Haptic_technology), drones, [teleoperation](https://en.wikipedia.org/wiki/Teleoperation), embodied intelligence, neuromorphic cameras, simultaneous localization and mapping, sensor design, terrain estimation, autonomous vehicles, control, simulation, [soft robotics](https://en.wikipedia.org/wiki/Soft_robotics), biohybrid (cyborg) systems, and others.

A solid number of talks and presentations were about simultaneous localization and mapping (SLAM), the fundametal task of localizing the camera pose throughout a sequence of frames, and at the same time reconstructing the 3D scene captured by it. Starting from foundational works like iMap [4], we've come such a long way. Now there are SLAMs that use Nerfs [5] or GaussianSplats [6] for mapping, or that use RGB only instead of RGBD [7].

One of the most interesting ones, however, I think is Clio [8]. It tackles a fundamental problem in robotics - to create a useful map representation of the scene observed by the robot, where usefulness is measured by the ability of the robot to use the map to complete tasks of interest. This is important. Consider that with general class-agnostic segmentors like SAM [9] and open-set semantic embeddings like Clip [10], we can now build maps with coutless semantic variations and objects. What is the right granularity for the representation? It is precisely that governed by the task. Robust perception relies on simultaneously understanding geometry, semantics, physics and relations in 3D. One approach to this is to build *scene graphs*. These are directed graphs where the nodes are spatial concepts and the edges are spatio-temporal relations. The key insight is that the scene graph needs to be hierarchical, owing to the fact that the environment can be described at different levels of abstraction. Which particular abstraction is utilized depends on the task.

<figure>
    <img class='extra_big_img' src="/images/scene_graph.PNG" alt="Scene graphs" width="1200">
    <figcaption>Figure 2: Scene graphs, showing the scene representation captured by a Spot robot exploring. At the bottom we have a geometric photorealistic reconstruction. Within it, objects can be detected, shown as cubes. Objects are mapped to places, shown as spheres. Spheres are grouped into regions, shown as cubes again. Places and tasks are colored by their closest task. Image taken from [8].</figcaption>
</figure>

Other notable activities during our stay there were eating camel burgers (tastes good, I recommend) and going to the Jubail mangrove park. We went there after sunset, yet managed to go through the boardwalks spanning a solid territory of mangrove forest. The ambience of this swamp is quite unique - you can hear the constant loud buzzing of crickets and cicada-like creatures, the water is relatively shallow and crystal clear, hence you can see the bottom and all the marina animals, mostly crabs and fish. Finally, lots of strange roots are sprouting up from the muddy ground. These are called *pneumatophores*, grow [above the ground](https://en.wikipedia.org/wiki/Aerial_root), and are common for mangrove-like plant habitats.

Another interesting location was the [Sheikh Zayed Grand Mosque](https://en.wikipedia.org/wiki/Sheikh_Zayed_Grand_Mosque), the biggest in the country. Right next to it, we had the IROS gala dinner, a nice outdoor dinner in the courtyard of the ERTH hotel. Overall, a highly enjoyable and authentic experience and a great conclusion to IROS 2024.

### References
[1] Kim, Moo Jin, et al. [OpenVLA: An Open-Source Vision-Language-Action Model.](https://arxiv.org/abs/2406.09246) arXiv preprint arXiv:2406.09246 (2024).   
[2] Team, Octo Model, et al. [Octo: An open-source generalist robot policy.](https://octo-models.github.io/) arXiv preprint arXiv:2405.12213 (2024).   
[3] Fankhauser, PÃ©ter, Michael Bloesch, and Marco Hutter. [Probabilistic terrain mapping for mobile robots with uncertain localization.](https://ieeexplore.ieee.org/abstract/document/8392399) IEEE Robotics and Automation Letters 3.4 (2018): 3019-3026.   
[4] Sucar, Edgar, et al. [imap: Implicit mapping and positioning in real-time.](https://openaccess.thecvf.com/content/ICCV2021/html/Sucar_iMAP_Implicit_Mapping_and_Positioning_in_Real-Time_ICCV_2021_paper.html) Proceedings of the IEEE/CVF international conference on computer vision. 2021.   
[5] Zhu, Zihan, et al. [Nice-slam: Neural implicit scalable encoding for slam.](https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_NICE-SLAM_Neural_Implicit_Scalable_Encoding_for_SLAM_CVPR_2022_paper.html) Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.   
[6] Matsuki, Hidenobu, et al. [Gaussian splatting slam.](https://rmurai.co.uk/projects/GaussianSplattingSLAM/) Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.   
[7] Zhu, Zihan, et al. [Nicer-slam: Neural implicit scene encoding for rgb slam.](https://ieeexplore.ieee.org/abstract/document/10550721) 2024 International Conference on 3D Vision (3DV). IEEE, 2024.   
[8] Maggio, Dominic, et al. [Clio: Real-time Task-Driven Open-Set 3D Scene Graphs.](https://arxiv.org/abs/2404.13696) arXiv preprint arXiv:2404.13696 (2024).   
[9] Kirillov, Alexander, et al. [Segment anything.](https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html) Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.   
[10] Radford, Alec, et al. [Learning transferable visual models from natural language supervision.](https://proceedings.mlr.press/v139/radford21a) International conference on machine learning. PMLR, 2021.

