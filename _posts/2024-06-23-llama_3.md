---
layout: post
title: Llama-3, A Deep Dive
date: 2024-06-23 07:00:00 +0200
tags: [ai]
# excerpt_separator: <!--more-->
---

In this post we explore Llama-3, a popular LLM. Because of its open-source nature and its compact architecture, it serves as a great example for studying how LLMs work and what are the main research directions around optimizing their use-cases. We focus mainly on the architecture as it literally affects all other aspects of the model - how it is used, how it is optimized, and how it can be modified.

To feed text inputs to the LLM we first need to tokenize them. Tokenization refers to breaking the input into a sequence of discrete elements which are then processed by the model. Text tokenization happens by breaking sentences into words or sub-words and associating dense embeddings to each of the resulting text chunks. Image tokenization instead breaks up the image into non-overlapping square patches, for example of size $(14, 14)$, flattens them, and then passes them to the model. Tokenization is a very powerful approach because it allows us to convert any data modality into discrete chunks which can be processed by the same model. This is one approach to building multimodal architectures.

For text tokenization, most LLMs use [byte-pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding). This algorithm iteratively builds up a vocabulary of recognized tokens out of a large body of text as follows. First   

