---
layout: post
title: Video Coding and MPEG
date: 2023-09-13 07:00:00 +0200
tags: [cs]
# excerpt_separator: <!--more-->
---

I find the topics of image analysis and video compression very enjoyable. A previous [post](https://aceofgreens.github.io/steganography_and_jpeg.html) has covered JPEG, the most widely used image compression standard. Now it's time to explore the [MPEG standards](https://en.wikipedia.org/wiki/Moving_Picture_Experts_Group#Standards), at least some of them. This is a super important topic, as videos are too large to store or stream across the internet in their raw format. So we necessarily need efficient compression algorithms. And there is some solid math, like the DCT, as well as various engineering tricks, like quantization and predictive coding, involved in them...

Media files often contain multiple streams - video, audio, subtitles, etc. These need to be synchronized correctly such that the right frames and sounds appear at the right time. Media containers, also known as wrapper formats, are file formats that can contain various types of data, such as video, audio, metadata, subtitles, and more. They package these streams together, allowing them to be played, streamed, or saved as a single file. Popular containers are `mp4` ([MPEG-4](https://en.wikipedia.org/wiki/MP4_file_format)), `mkv` ([Matroska](https://en.wikipedia.org/wiki/Matroska)), `avi` ([Audio Video Interleave](https://en.wikipedia.org/wiki/Audio_Video_Interleave)), `mov` ([QuickTime File Format](https://en.wikipedia.org/wiki/QuickTime_File_Format)), `webm` ([WebM](https://en.wikipedia.org/wiki/WebM) by Google). There is considerable [difference](https://en.wikipedia.org/wiki/Comparison_of_video_container_formats) between containers - they have different supported codecs, owners, licenses, some support metadata, video chapters, and interactive menus, others do not.

The process of bundling invidividual streams together is called **multiplexing** (or muxing). Usually a muxer takes the raw data streams and breaks them into packets suitable for the chosen container format. Then, metadata about the codec, bitrate, and resolution is added. For example, the [Matroska container](https://www.matroska.org/technical/diagram.html) allows a simple file to have just two elements: an `EBML Header` and a `Segment`. The `Segment` itself can contain all the so-called top-level elements: 1) `SeekHead`, containing an index of the locations of other top-level elements in the segment, 2) `Info` for metadata used to identify the segment, 3) `Tracks` storing metadata for each video & audio track, 4) `Chapters`, 5) `Cluster` contains the actual content, e.g. video-frames, of each track, 6) `Cues` for seeking when playing back the file, 7) `Attachments` for things like pictures & fonts, 8) `Tags` containing metadata about the segment.

The reverse process of obtaining individual streams from a single container file is called **demultiplexing** (demuxing). Here the demuxer reads the container and based on its structure extracts data into separate streams - one for video, possibly many for audio (e.g. different languages) while keeping the streams time-synchronized.

The extracted data streams can be either compressed or not. Without video compression even the shortest videos become unnecessarily large. For example a HD video with resolution $(1280, 720)$, $24$ frames per second (fps), $3$ bytes per pixel, $1$ minute long, takes about $3.7$ gigabytes for storage, and requires $\approx 0.5$ Gbps when streaming. This is too much. Using a proper video **codec**, the combination of an encoder and a decoder, one can enjoy sometimes up to $200:1$ compression ratios without a drastic reduction in the quality of the reconstruction.

Most codecs rely on the discrete cosine transform (DCT) and motion compensation. Natural videos contain a very high amount of temporal redundancy between nearby frames, and spatial redundancy between individual pixels within a single frame. Inter-frame compression utilizes nearby frames and codes one frame based on a previous one, which requires storing just the differences for those pixels which have changed. Intra-frame compression instead utilizes information from within a single frame and very much resembles the ideas in the [JPEG standard](https://en.wikipedia.org/wiki/JPEG).

The inter frame coding attemps to express the current frame in terms of previous ones. More specifically, the current frame is divided into [macroblocks](https://en.wikipedia.org/wiki/Macroblock). These consist of $16 \times 16$ elements. There are two types of macroblocks - *transform* blocks, which will be fed to a linear block transform, and *prediction* blocks, which are used in the motion compensation.

It is common to work with macroblocks not in RGB but in the [YCbCr](https://en.wikipedia.org/wiki/YCbCr) colour space. It is known that human vision is more sensitive to changes in the luminance Y than to changes in the chrominances (Cb is the difference between blue and a reference value and Cr is similar but for red). This allows us to [subsample](https://en.wikipedia.org/wiki/Chroma_subsampling#) the chroma channels without much perceptual degradation in the image quality. Usually a chroma subsampling ratio of 4:2:0 is used. To understand what this means, one can imagine a region of pixels of width $4$ and height $2$. In the first row of $4$ pixels the chroma channels will be sampled in regular intervals exactly $2$ times. Then, the $0$ indicates that there are a total of $0$ changes in the $4$ chroma values across the first row and the second row. Overall, this yields a 50% data reduction compared to no subsampling.

The temporal prediction happens at the level of a macroblock using a [block matching algorithm](https://en.wikipedia.org/wiki/Block-matching_algorithm). Suppose we have a macroblock from frame $t$ and we want to find the most similar macroblock from frame $t-1$. Once we do that, we can encode the current macroblock with a motion vector pointing to the found past block, along with any residual information that has changed. Exhaustive search for the most-similar previous macroblock is prohibitive. It is common to only search the macroblocks which are up to $p=7$ pixels on either side of the corresponding macroblock in the previous frame.

For evaluating the similarity between two blocks $C$ and $R$, both of shape $(N, N)$, it's common to use the mean absolute difference (MAD), mean squared difference (MSE), or the peak signal-to-noise ration (PSNR):

$$
\begin{align}
\text{MAD} &= \frac{1}{N^2} \sum_{i = 0}^{n-1} \sum_{j = 0}^{N-1} | C_{i, j} - R_{i, j}| \\
\text{MSE} &= \frac{1}{N^2} \sum_{i = 0}^{n-1} \sum_{j = 0}^{N-1} (C_{i, j} - R_{i, j})^2 \\ 
\text{PSNR} &= 10 \log_{10} \frac{\text{Maximum pixel value}^2}{\text{MSE}}.
\end{align}
$$

To find the best matching macroblock, one can always resort to brute force search. This involves computing the cost functions above for every possible macroblock within the $p$ pixels difference on each side. For $p=7$ this involves 225 cost function calculations. A much faster, but less accurate approach is the three step search algorithm which makes evaluates only 25 macroblocks. Suppose the block in the past frame corresponding to the current one has centered coordinates $(0, 0)$. Then in the first step we set $S=4$ and we evaluate the blocks at $\pm S$ pixels from $(0, 0)$, along with $(0, 0)$ itself. This is 9 evaluations. Then we center the current coordinate around the most similar block. In the second step we repeat with $S=2$ and in the third with $S=1$. When $S=1$ the macroblock with the smallest block is selected. There are many other algorithms, each offering various speedups to the basic block matching idea.

 <!-- The encoder can decide to encode this block without relying on previous information - as an intra-block - or using the previous blocks - an inter-block.  -->

Even the best-matching found block may not match exactly.For that reason the encoder takes the difference, called the *residual*, and encodes it. Note that nothing prevents the encoding of the past macroblock to itself depend on yet another previous macroblock. Thus, motion prediction can be recursive, but obviously not infinitely... This motivates a distinction between how blocks can be encoded:
1. **I-frames** (Intra-frames) are self-contained frames in a video sequence that are encoded without referencing any other frames. They serve as reference points for subsequent predictive frames (P-frames and B-frames) and act as "reset" points in a video stream, allowing for random access and error recovery.
2. **P-frames** (Predictive frames) are encoded using data from previous I-frames or P-frames as a reference to reduce redundancy. They are encoded as a motion vector and the remaining residual data.
3. **B-frames** (Bidirectional frames) are encoded using data from both previous and subsequent I-frames or P-frames, exploiting temporal redundancy from two directions. These can yield the highest compression amount compared to size of I-frames.

Based on the temporal and spatial relationships one can define the **Group of Pictures** (GOP) structure. Typically it looks like IBBPBBIBBPBBI... The first frame is an intra-frame which is self-sufficient to decode. The next two B-frames are decoded from the first, I-frame and the fourth, P-frame, which itself depends on the I-frame. The periodic I-frames serve to reset the errors which may accumulate. The ample use of B-frames increases the overall compression ratio but forces us to transmit the frames *out of order*, as we need to submit the subsequent P-frame before the B-frames. This also introduces a bit of decoding latency.






<!-- Demuxing separates these streams so they can be processed individually. -->
