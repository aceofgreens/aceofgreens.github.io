<!DOCTYPE html>
<html lang="en">

<head>
    <title>Markov Chain Mixing Times | The Critical Section</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Pelican" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <link rel="stylesheet" href="/assets/main.css" />
    <link rel="icon" href="/assets/favicon.png" />
    <link href="https://fonts.googleapis.com/css?family=Roboto|Oswald|Open+Sans" rel="stylesheet">


    <meta name="description" content="Markov chain techniques have proven themselves incredibly valuable in fields like statistics and machine learning where the goal is to sample from a distribution whose density is too complicated to calculate analytically. Algorithms like Metropolis-Hastings produce samples from the required distribution by iterating a discrete stochastic process where the next state depends only on the current state (this being the Markov property). The typical statistical treatment of such an algorithm ends somewhere around here - you implement the algorithm, you let it run for a predefined number of steps, and you assess the quality of the results by looking at outputs like trace plots, autocorrelation plots, and burn-in periods. However, like any other algorithm, Markov chain sampling algorithms have a time complexity which can be analyzed... and computer science has a lot to say about that." />

    <meta name="tags" content="cs" />

</head>

<body onload="welcomeFunction()" >

  <header class="site-header" role="banner">
    <div class="wrapper">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> <!--An icon library for the button icon-->

      <a class="site-title" rel="author" href="/">The Critical Section</a>

      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
        </label>

        <div class="trigger">
            <a class="page-link" href="/about.html">About</a>
            <a class="page-link" href="/posts.html">Posts</a>
            <a class="page-link" href="/tags.html">Tags</a>
        </div>

      </nav>

    </div>
  </header>


  <main class="page-content" aria-label="Content">
    <div class="wrapper">

  <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <header class="post-header">
      <h1 class="post-title p-name" itemprop="name headline">Markov Chain Mixing Times</h1>
      <p class="post-meta">
        <time class="dt-published" datetime="2022-04-27T16:00:00+02:00" itemprop="datePublished">
          27 Apr 2022
        </time>
      </p>

    </header>

    <div class="post-content">
      <p>Markov chain techniques have proven themselves incredibly valuable in fields like statistics and machine learning where the goal is to sample from a distribution whose density is too complicated to calculate analytically. Algorithms like <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolis-Hastings</a> produce samples from the required distribution by iterating a discrete stochastic process where the next state depends only on the current state (this being the Markov property). The typical statistical treatment of such an algorithm ends somewhere around here - you implement the algorithm, you let it run for a predefined number of steps, and you assess the quality of the results by looking at outputs like trace plots, autocorrelation plots, and burn-in periods. However, like any other algorithm, Markov chain sampling algorithms have a time complexity which can be analyzed... and computer science has a lot to say about that.</p>
<p>The runtime of Markov chains is a beautiful topic which I find very interesting. Most of the content and examples for this post are taken from C. Moore's and S. Mertens' <a href="https://www.amazon.com/Nature-Computation-Cristopher-Moore/dp/0199233217/ref=sr_1_1?crid=2CWBHWPZCPT26&amp;keywords=nature+of+computation&amp;qid=1648843521&amp;sprefix=nature+of+computation%2Caps%2C169&amp;sr=8-1">The Nature of Computation</a> which contains an awesome overview of the ideas in this area.</p>
<h3>Basics</h3>
<p>We begin by recalling some basic facts about Markov chains.</p>
<p>Markov chains are stochastic processes where the probability of the next state in time <span class="math">\(t+1\)</span> depends only on the current state at time <span class="math">\(t\)</span>. We'll be mostly interested in discrete Markov chains which can be represented as (possibly infinite) directed graphs where the nodes are the states and the edges are the transitions. The chain is <em>irreducible</em> if any state <span class="math">\(y\)</span> is eventually reachable with non-negative probability from any chain <span class="math">\(x\)</span>. If also, for every state <span class="math">\(x\)</span> in which we start, there is a number of steps <span class="math">\(t\)</span>, such that for all <span class="math">\(t' \ge t\)</span>, the probability of returning to <span class="math">\(x\)</span> is positive, then this chain is <em>aperiodic</em>. If a Markov chain with finitely many states is irreducible and aperiodic, then it is ergodic - it will always converge to a unique equilibrium distribution as <span class="math">\(t \rightarrow \infty\)</span>.</p>
<p>The Markov chain can be represented as a stochastic matrix where the value in row <span class="math">\(i\)</span> and column <span class="math">\(j\)</span> shows the probability of moving to state <span class="math">\(i\)</span> if we are currently in state <span class="math">\(j\)</span>. If the chain is ergodic, this matrix has an eigenvalue of 1, whose eigenvector is the equilibrium distribution. All other eigenvalues have an absolute norm less than 1.</p>
<p>Typically, we're interested in quantifying how close is the current distribution over the states at time <span class="math">\(t\)</span> compared to the equilibrium distribution. This can be done with the total variation distance:</p>
<div class="math">$$
0 \le {\lVert P - Q \rVert}_{\text{tv}} = \frac{1}{2}\sum_{x}|P(x) - Q(x)| \le 1
$$</div>
<p>Consider this example, we have a Markov chain with two states. From state 1 we always transition to state 2, while in state 2 we move to state 1 with 50% probability and remain in state 2 with 50% probability. Then the stochastic matrix corresponding to this Markov chain is <span class="math">\(\small M = \begin{pmatrix}
0 &amp; 1/2 \\
1 &amp; 1/2
\end{pmatrix}\)</span>. Its eigenvalues are <span class="math">\(1\)</span> and <span class="math">\(\lambda = -\frac{1}{2}\)</span>, and the corresponding eigenvectors are <span class="math">\( \small P_{eq} = 
\begin{pmatrix}
1 \\
2
\end{pmatrix}\)</span> and <span class="math">\(\small v = 
\begin{pmatrix}
1 \\
-1
\end{pmatrix}\)</span>. The eigenvectors form a basis and therefore any distribution over the states at <span class="math">\(t=0\)</span>, for example <span class="math">\(
\small P_0 = 
\begin{pmatrix}
1 \\
0
\end{pmatrix}\)</span>, can be represented as a combination of the equilibrium distribution <span class="math">\(P_{eq}\)</span> and the other eigenvector <span class="math">\(v\)</span>: <span class="math">\(
P_0 = P_{eq} + \frac{2}{3}v\)</span>. The distribution after <span class="math">\(t\)</span> steps is then</p>
<div class="math">$$
P_t = M^t (P_{eq} + \frac{2}{3}v) =  P_{eq} + \frac{2}{3} \lambda^t v = P_{eq} + (-1)^t 2^{-t} \small\begin{pmatrix}
2 \\ -3 \end{pmatrix}.
$$</div>
<p>Importantly, as <span class="math">\(t \rightarrow \infty\)</span>, the memory of the initial state decays exponentially fast as <span class="math">\(\|\lambda \|^t = 2^{-t}\)</span> and as a result the total variation decays similarly </p>
<div class="math">$${\lVert P_t - P_{eq} \rVert}_{\text{tv}} = \frac{2}{3} \left| \lambda \right|^t = \frac{2}{3}2^{-t}.$$</div>
<p>The number of steps needed to reach a small distance <span class="math">\(\epsilon\)</span> from <span class="math">\(P_{eq}\)</span> is given by <span class="math">\(t = \log_2 \frac{2}{3\epsilon} = O(\log \epsilon^{-1})\)</span>, which motivates the definition of <em>mixing time</em> - a "loose" measure of the runtime, or computational complexity of that Markov chain. The <span class="math">\(\epsilon\)</span>-mixing time is the smallest <span class="math">\(t\)</span> such that irrespective of the starting distribution, at time <span class="math">\(t\)</span> the total variation between <span class="math">\(P_t\)</span> and <span class="math">\(P_{eq}\)</span> is at most <span class="math">\(\epsilon\)</span>:</p>
<div class="math">$$
\tau_\epsilon = \min \big\{ t \ | \max_{P_0} {\lVert P_t - P_{eq} \rVert}_{\text{tv}} \le \epsilon \big\}.
$$</div>
<p>In reality, what we are interested in is how the mixing time depends on <span class="math">\(n\)</span> - the system size, not <span class="math">\(\epsilon\)</span> per se, because this allows us to understand how the mixing time scales with larger and larger problem instances. The total number of possible states <span class="math">\(N\)</span> could be exponentially large in <span class="math">\(n\)</span> (as in Ising models or graph coloring problems) and to be able to sample from such large spaces we'll need polynomial mixing times in <span class="math">\(n\)</span>, i.e. <span class="math">\(\tau = O(\text{poly}(n))\)</span>. An optimal mixing time which we call <em>rapid mixing</em> is when <span class="math">\(\tau\)</span> scales as <span class="math">\(O(n \log n)\)</span>.</p>
<h3>Spectral Analysis</h3>
<p>From the toy example above we saw that the mixing time is related to how fast the memory from the initial state decays, which is in turn related to the eigenvalues of the stochastic matrix. We can use spectral theory - a mathematical formalization that allows the idea of eigenvalues &amp; eigenvectors to be applied to other more general structures and phenomena - to derive general bounds on the mixing time.</p>
<p>Similar to above, the system size will be <span class="math">\(n\)</span> and the number of possible states will be <span class="math">\(N\)</span>. Then, the Markov chain will be defined through a stochastic <span class="math">\(N \times N\)</span> matrix <span class="math">\(M\)</span> expressing the probabilistic dynamics. The equilibrium distribution <span class="math">\(P_{eq}\)</span> is <span class="math">\(v_0 = P_{eq}\)</span> and its corresponding eigenvalue is <span class="math">\(\lambda_0 = 1\)</span>. Then, the important quantity is the <strong>spectral gap</strong></p>
<div class="math">$$
\delta = 1 - \max_{k \ge 1} |\lambda_k|.
$$</div>
<p>The spectral gap is just the difference between the two largest eigenvalues by absolute size (the first of which is always <span class="math">\(1\)</span> for any stochastic matrix). In the toy example above we saw that the non-equilibrium part of the probability decayed according to <span class="math">\(\lambda^t\)</span> so intuitively, if the second largest eigenvalue is small, this decays very fast and the chain takes little time to mix. This suggests that a large spectral gap means faster mixing and a small spectral gap means slow mixing.</p>
<p>Why look only at the second largest eigenvalues by absolute size? In reality, it is true that a Markov chain on 50 states will have its state probability determined by 49 non-equilibrium eigenvectors. If we calculate them, we can calculate exactly how much influence each one of them has and compute the mixing time <span class="math">\(\tau_\epsilon\)</span> <em>exactly</em>. However, for practical problems this is unfeasible, given that <span class="math">\(N\)</span> is typically exponential in <span class="math">\(n\)</span>. In such cases, what suffices is to use only the second largest eigenvalue to obtain an upper bound on the total-variation distance, and therefore the mixing time. This is still enough to provide at least an estimate for the order of magnitude of the mixing time.</p>
<p>Speaking of bounds on the mixing time, thinking in terms of spectral gap provides the following result.</p>
<p><strong>Theorem</strong>: <em>Let <span class="math">\(M\)</span> be an ergodic, symmetric Markov chain, obeying the detail balance condition. Also let <span class="math">\(M\)</span> have <span class="math">\(N\)</span> states and a spectral gap <span class="math">\(\delta\)</span>. Then, for sufficiently small <span class="math">\(\epsilon\)</span> the mixing time is bounded by</em></p>
<div class="math">$$
\frac{\ln (2\epsilon)^{-1}}{2\delta} \le \tau_\epsilon \le \frac{\ln (N \epsilon^{-1})}{\delta}.
$$</div>
<p>If we set <span class="math">\(\epsilon\)</span> to some small value, we get <span class="math">\(\frac{1}{\delta} \lesssim \tau \lesssim \frac{\ln N}{\delta}\)</span> or in other words <span class="math">\(\tau = \Omega(\frac{1}{\delta})\)</span> and <span class="math">\(\tau = O(\frac{\ln N}{\delta})\)</span>. Note that if the spectral gap is polynomial in <span class="math">\(n\)</span>, then the mixing time is polynomial. If the spectral gap is exponential in <span class="math">\(n\)</span>, then the mixing time is also exponential, although the above relationship does not show the exact exponent.</p>
<p>The following plot shows how the mixing time bounds depend on the spectral gap when fixing <span class="math">\(\epsilon=10^{-5}\)</span> and <span class="math">\(N=4\)</span>. Since the spectral gap appears in the denominator of the expressions, a chain with a very small spectral gap may require a very large number of iterations to mix well.</p>
<figure>
    <img src="/images/spectral_gap.png" alt="Spectral gap and mixing times" width="1000">
    <figcaption>Figure 1: Relationship between the mixing time bounds and the spectral gap.</figcaption>
</figure>

<p>We can build some more intuition by running two simple numerical examples. In the first one, we'll measure the tightness of the bounds and the mixing time for a chain with a relatively large spectral gap. In the second example, we'll do the same but for a graph with a very small spectral gap.</p>
<p>For the first example, we have the following well-connected Markov chain.</p>
<figure>
    <img src="/images/graph_a.png" alt="Graph A" width="1000">
    <figcaption>Figure 2: A graph with a relatively high spectral gap.</figcaption>
</figure>

<div class="math">$$\small M = 
\begin{pmatrix}
0 &amp; 0.25 &amp; 0.0 &amp; 0.2 &amp; 0.0 &amp; 0.5 \\
0.4 &amp; 0.0 &amp; 0.0 &amp; 0.2 &amp; 0.0 &amp; 0.0 \\
0.0 &amp; 0.3 &amp; 0.0 &amp; 0.2 &amp; 0.33 &amp; 0.1 \\
0.2 &amp; 0.0 &amp; 0.5 &amp; 0.0 &amp; 0.33 &amp; 0.0 \\
0.0 &amp; 0.45 &amp; 0.5 &amp; 0.2 &amp; 0.0 &amp; 0.4 \\
0.4 &amp; 0.0 &amp; 0.0 &amp; 0.2 &amp; 0.33 &amp; 0.0
\end{pmatrix}.
$$</div>
<p>Its spectral gap is approximately <span class="math">\(0.33\)</span>, which implies that convergence is relatively fast, compared to the second example we'll see. The following plot shows that if we let the chain run, the state distribution converges exponentially fast toward the equilibrium (note that the <span class="math">\(y\)</span>-axis is logarithmic). It takes only around 23 steps for the total variation to drop below <span class="math">\(10^{-4}\)</span>.</p>
<figure>
    <img src="/images/graph_a_convergence.png" alt="Convergence on graph A" width="1000">
    <figcaption>Figure 3: Relationship between epsilon and the number of iterations.</figcaption>
</figure>

<p>We can also see how tight the bounds are when we vary <span class="math">\(\epsilon\)</span>. In the next graph, the <span class="math">\(x\)</span>-axis shows <span class="math">\(\epsilon\)</span> - our total variation tolerance - and the <span class="math">\(y\)</span>-axis shows the mixing time, in iterations, it takes for the total variation to become less than that amount. The results suggest that once the total variation drops below a certain amount, we can make it arbitrarily small by waiting a few more iterations (logarithmic <span class="math">\(x\)</span>-axis). That is one reason why it's more useful to focus on how the mixing time depends on <span class="math">\(n\)</span>, not <span class="math">\(\epsilon\)</span>. This is not shown on this plot because we keep the graph fixed.</p>
<figure>
    <img src="/images/graph_a_bounds.png" alt="Bounds on graph A" width="1000">
    <figcaption>Figure 4: Mixing time and predicted bounds, for varying levels of the total variation.</figcaption>
</figure>

<p>Now, let's take a look at the second example which has, again, few nodes, but with a very small spectral gap. We can think of this graph as having two well-connected components where the transition probabilities inside each component are almost uniform, but the transition probabilities between the two components are almost zero. This creates a natural difficulty in how the random walk can "flow" through the two components.</p>
<figure>
    <img src="/images/graph_b.png" alt="Graph B" width="1000">
    <figcaption>Figure 5: An example of a graph with a bottleneck on the probability flow.</figcaption>
</figure>

<p>The spectral gap is approximately <span class="math">\(0.001\)</span>. The bounds and the actual mixing time are much higher, as the following plot shows. To drop the total variation under <span class="math">\(10^{-10}\)</span> requires about <span class="math">\(20000\)</span> iterations, or about <span class="math">\(285\)</span> times the number needed when the spectral gap is <span class="math">\(0.33\)</span>.</p>
<figure>
    <img src="/images/graph_b_bounds.png" alt="Bounds on graph B" width="1000">
    <figcaption>Figure 6: Mixing time and predicted bounds, for varying levels of the total variation.</figcaption>
</figure>

<h3>Conductance of Probabilities</h3>
<p>The last numerical example showed that the mixing time is related to whether the chain has any "bottlenecks" - subsets of nodes for which the probability of the walk moving out of them (or escaping) is low. This flow of probability is very similar to conductance in an electric circuit.</p>
<p>If <span class="math">\(M\)</span> is the Markov chain and <span class="math">\(x\)</span> and <span class="math">\(y\)</span> are two adjacent states, the flow of probability from <span class="math">\(x\)</span> to <span class="math">\(y\)</span> at equilibrium is <span class="math">\(Q(x \rightarrow y) = P_{eq}(x)M(x \rightarrow y)\)</span>. The total flow outside of a region of nodes <span class="math">\(S\)</span> is similarly <span class="math">\(Q(S, \bar{S}) = \sum_{x \in S, y \not \in S} Q(x \rightarrow y)\)</span>. Therefore, then the probability of escaping <span class="math">\(S\)</span>, given that we are in it, is</p>
<div class="math">$$
\Phi(S) = \frac{\sum_{x \in S, y \not \in S} Q(x \rightarrow y)}{\sum_{x \in S}P_{eq}(x)}.
$$</div>
<p>Finally, if we shorten <span class="math">\(\sum_{x \in S} P_{eq}(x)\)</span> to <span class="math">\(P_{eq}(S)\)</span>, the conductance of the Markov chain is the probability of escaping from the most inescapable set <span class="math">\(S\)</span>.</p>
<div class="math">$$ \Phi = \min_{S: P_{eq}(S) \le 1/2} \Phi(S).$$</div>
<p>The requirement that <span class="math">\(S\)</span> has at most <span class="math">\(1/2\)</span> equilibrium probability is needed because by allowing <span class="math">\(S\)</span> to be very large, the conductance outside of <span class="math">\(S\)</span> naturally goes down, as <span class="math">\(\bar{S}\)</span> has less and less nodes in it.</p>
<p>An important result linking the spectral gap and the conductance is the following.<br>
<strong>Theorem</strong>: <em>If <span class="math">\(M\)</span> is a ergodic, symmetric Markov chain with nonnegative eigenvalues, then its spectral gap <span class="math">\(\delta\)</span> and conductance <span class="math">\(\Phi\)</span> are related by</em></p>
<div class="math">$$
\frac{\Phi^2}{2} \le \delta \le 2\Phi.
$$</div>
<p>Combining this result with the previous one, the mixing time, for fixed <span class="math">\(\epsilon\)</span>, is bounded by</p>
<div class="math">$$
\frac{1}{\Phi} \lesssim \tau \lesssim \frac{\log N}{\Phi^2}.
$$</div>
<p>With the concept of conductance defined, one practical benefit is that it can be proved that <strong>the mixing time of a random walk on any undirected graph is polynomial in the number of nodes</strong>. Since the random walk chooses the next node to move to uniformly from the current neighbors, this prevents the existence of any states to which the flow is disproportionately small. While in reality, our purposefully-constructed chains are not random walks, this result is still useful, as it is one of the few places where we can get bounds on the mixing time depending on the problem instance size.</p>
<p>That being said, it is now time to discuss one particularly beautiful object - an <em>expander</em> graph.</p>
<p>An expander is a graph that has no bottlenecks. We say that a <span class="math">\(d\)</span>-regular graph <span class="math">\(G=(V, E)\)</span> with <span class="math">\(N\)</span> vertices is an expander if there is a constant <span class="math">\(\mu\)</span> such that for all subsets <span class="math">\(S \subseteq V\)</span> with less than or equal to half the total number of vertices, <span class="math">\(\frac{\|E(S, \bar{S})\|}{d\|S\|} \ge \mu\)</span>. Intuitively, <span class="math">\(G\)</span> is an expander if for every cut <span class="math">\(S\)</span> and <span class="math">\(V \setminus S\)</span>, there are a lot of edges going out of <span class="math">\(S\)</span> and into <span class="math">\(V \setminus S\)</span>. Importantly, if we make <span class="math">\(S\)</span> larger, then the number of edges is also increased proportionally.</p>
<p>Note that the <em>expansion</em> <span class="math">\(\mu\)</span> is also equal to the conductance of a random walk on that graph. This ensures that the chain is well-connected and as a result Markov chains on expanders mix very fast - something very useful for applications.</p>
<p>In reality, any graph is an expander for some <span class="math">\(\mu\)</span>. To be useful to solve problems of varying sizes however, we would like to have a family of graphs <span class="math">\(\{G_N\}\)</span> - one or more graphs for every possible number of nodes <span class="math">\(N\)</span> - such that the expansion <span class="math">\(\mu\)</span> is constant as <span class="math">\(N\)</span> increases. If this were not the case, we could imagine a scenario where for some problem of size <span class="math">\(25\)</span>, we use an expander with expansion <span class="math">\(3\)</span>, but for a problem instance of size <span class="math">\(26\)</span>, we have access to a graph with expansion <span class="math">\(1\)</span>. This just overcomplicates things, which is one reason we want constant expansion as <span class="math">\(N\)</span> increases. If such an infinite family <span class="math">\(\{G_N\}\)</span> of <span class="math">\(d\)</span>-regular graphs exist, then the following are equivalent:</p>
<ul>
<li>For some <span class="math">\(\mu &gt; 0\)</span>, <span class="math">\(G_N\)</span> is an expander with expansion at least <span class="math">\(\mu\)</span>;</li>
<li>For some <span class="math">\(\Phi &gt; 0\)</span>, the lazy walk on <span class="math">\(G_N\)</span> with a modified transition probability <span class="math">\(M' = (M + I)/2\)</span>, has conductance at least <span class="math">\(\Phi\)</span>;</li>
<li>For some <span class="math">\(\delta &gt; 0\)</span>, the lazy walk on <span class="math">\(G_N\)</span> has a spectral gap at least <span class="math">\(\delta\)</span>.</li>
</ul>
<p>It turns out, such families of graphs are very common and can be generated using random algorithms. Most random <span class="math">\(d\)</span>-regular graphs are expanders. Few deterministic algorithms for generating expanders exist as of today (see <a href="https://en.wikipedia.org/wiki/Cayley_graph">Cayley graphs</a> and the <a href="https://en.wikipedia.org/wiki/Zig-zag_product">Zig Zag product</a>).</p>
<p>Ultimately, expanders are very useful objects. They can be used to reduce the number of random bits needed to run certain algorithms and are useful constructs in some important proofs. There are many interesting aspects worthy of discussion, which unfortunately become too technical for a quick overview. Therefore, I'll leave them for a future post, for when the muse calls.</p>
<figure>
    <img src="/images/paley_17.png" alt="Paley 17 nodes" width="1000">
    <figcaption>Figure 7: The Paley expander with 17 nodes. The two graphs are the same, but with different node layouts for better visualization of the connectivity.</figcaption>
</figure>

<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = "https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML";

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'], ['$', '$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div>
  
  <p class="post-meta">
    Tag: cs
  </p>

  </article>


    </div>
  </main>


<footer class="site-footer h-card">
  <div class="wrapper">
  
  <p></p>
  

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">


    <div class="footer-col-wrapper">

      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">The Critical Section</li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          <li><a href="https://github.com/aceofgreens"><i class="fa fa-github"></i> GitHub</a></li>


        </ul>
      </div>


      <div class="footer-col footer-col-3">
        <p>A personal blog for artificial intelligence and similar topics.</p>
      </div>
    </div>

  </div>
</footer>

<script type="text/javascript">
function welcomeFunction() {
  var items = document.getElementsByTagName("code");
    for (var i = items.length; i--;) {
      items[i].setAttribute("class", "highlight");
  }
}
</script>

</body>
</html>