<!DOCTYPE html>
<html lang="en">

<head>
    <title>Information, Codes, and Content | The Critical Section</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Pelican" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <link rel="stylesheet" href="/assets/main.css" />
    <link rel="icon" href="/assets/favicon.png" />
    <link href="https://fonts.googleapis.com/css?family=Roboto|Oswald|Open+Sans" rel="stylesheet">


    <meta name="description" content="Information theory is the subfield of applied mathematics concerned with topics like information content, data compression, information storage and transmission. The key terms and concepts appear often in machine learning and I have found myself sometimes cross-referencing them from both disciplines. In this post, we cover some of the absolute basics - entropy, some simple codes, and limiting results. I think these ideas integrate nicely with other fields and are useful as a general background context for machine learning. Even more so, they are fundamental, due to how learning is inherently related to data compression." />

    <meta name="tags" content="cs" />

</head>

<body onload="welcomeFunction()" >

  <header class="site-header" role="banner">
    <div class="wrapper">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> <!--An icon library for the button icon-->

      <a class="site-title" rel="author" href="/">The Critical Section</a>

      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
        </label>

        <div class="trigger">
            <a class="page-link" href="/about.html">About</a>
            <a class="page-link" href="/posts.html">Posts</a>
            <a class="page-link" href="/tags.html">Tags</a>
        </div>

      </nav>

    </div>
  </header>


  <main class="page-content" aria-label="Content">
    <div class="wrapper">

  <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <header class="post-header">
      <h1 class="post-title p-name" itemprop="name headline">Information, Codes, and Content</h1>
      <p class="post-meta">
        <time class="dt-published" datetime="2024-02-10T07:00:00+02:00" itemprop="datePublished">
          10 Feb 2024
        </time>
      </p>

    </header>

    <div class="post-content">
      <p>Information theory is the subfield of applied mathematics concerned with topics like information content, data compression, information storage and transmission. The key terms and concepts appear often in machine learning and I have found myself sometimes cross-referencing them from both disciplines. In this post, we cover some of the absolute basics - entropy, some simple codes, and limiting results. I think these ideas integrate nicely with other fields and are useful as a general background context for machine learning. Even more so, they are fundamental, due to how learning is inherently related to data compression.</p>
<p>What is a bit? What is information? These questions are surprisingly abstract and difficult to formalize. Some philosophers are dedicating their entire careers on these concepts. For us, a good definition I've heard is: a bit is a <em>single discernible difference</em>. This difference is always between two comparable <em>entities</em>. It doesn't matter what they are - could be "yes" vs "no", "red" vs "blue", "likely" vs "definitely", "zero" vs "one" - these are all discernible differences. The elegance of defining the bit in such a way lies in its formal implication - it is defined purely by its function to distinguish, capturing any variance between two items, akin to how the numeral 5 can symbolize any collection comprising five elements. With two bits we can represent 4 different entities - for example "no", "somewhat", "almost", and "yes". With <span class="math">\(n\)</span> bits we can represent <span class="math">\(2^n\)</span> different states.</p>
<p>More generally, discernible differences exist also when it comes to unknown or random events. Even if we live in a deterministic universe, our <em>beliefs</em> about the current world state are limited and can be represented as probabilities - this being called the subjective interpretation of probability, as opposed to the frequentist one. In that case the objects of interest become random variables. And by virtue of the fact that random variables can have multiple <em>discernably different</em> outcomes, we conclude that the information of an outcome depends on its probability.</p>
<p>Suppose we have a random variable <span class="math">\(X\)</span> with outcomes <span class="math">\(\mathcal{A}_X\)</span> and a probability distribution <span class="math">\(p(x)\)</span>. The information content, or uncertainty, of a random outcome <span class="math">\(x\)</span> is given by <span class="math">\(H(X = x) = -\log_2 p(x)\)</span>. The entropy of the whole random variable <span class="math">\(X\)</span> is the average uncertainty of any one outcome, </p>
<div class="math">$$
H(X) = -\sum_{x \in \mathcal{A}_X} p(x) \log_2 p(x) = \mathbb{E}_x \big[- \log_2 p(x) \big].
$$</div>
<p>It is non-negative and maximal when all outcomes are equiprobable. The information entropy and by extension the entropy is zero if the probability of the (only) outcome is <span class="math">\(1\)</span>. If the entropy is <span class="math">\(n\)</span> bits, then observing any outcome gives us, on average, <span class="math">\(n\)</span> bits of information. Some particular outcomes may give us more or less information.</p>
<p>In general an <em>encoding</em> is a function which maps outcomes to strings of symbols. For simplicity, we can think of it as taking a particular <span class="math">\(x\)</span> and mapping it to a binary string called a <em>code</em>, e.g. <span class="math">\(011011\)</span>. If the mapping is isomorphic we can uniquely identify and distinguish each outcome based on its code. The length of the code is given by the number of bits it has. Typically we are interested in finding an encoding which minimizes the length of the average code, according to the probabilities of the random variable <span class="math">\(X\)</span>, while still being unambiguous and easy to decode. This will allow us to reduce data size of any data stored or transmitted. As we'll see soon, there is a fundamental limit on how short the average code can be without losing information.</p>
<p>Compressing data refers to encoding it in a way such that the total code length is less than the length of the unencoded data. One important criterium is average information content per source symbol. Suppose we have a source file and we encode it into a file with <span class="math">\(L\)</span> bits per source symbol. If we can recover the data reliably, then necessarily the average information content of that source is at most <span class="math">\(L\)</span> bits per symbol. Not all encodings compress the data. If there are <span class="math">\(M\)</span> outcomes we can always encode them with <span class="math">\(M\)</span> codes, all of the same length, each with <span class="math">\(\log_2 M\)</span> bits. This encoding does not consider the probabilities and hence does not compress anything. </p>
<p>There are two types of compressors. А lossy algorithm maps <em>all</em> outcomes to shorter codewords, but some outcomes are mapped to the same codeword. As a result, the decoder cannot unambiguously reconstruct the outcome, which sometimes leads to decoding failures. Instead, a lossless algorithm maps <em>most</em> outcomes to shorter keywords. It preserves all information and shortens the codes of the most probable <span class="math">\(x\)</span>-s but the rare ones have longer codewords, beyond the <span class="math">\(\log_2 M\)</span> bits corresponding to the codelength when not compressing.</p>
<p>Let's consider lossy compression. For any <span class="math">\(0 \le \delta \le 1\)</span>, we can compute the smallest <span class="math">\(\delta\)</span>-sufficient subset <span class="math">\(S_\delta\)</span>, satisfying <span class="math">\(P( x \in S_\delta) \ge 1 - \delta\)</span>. Intuitively, this set contains the most probable outcomes - those until their sum becomes greater than <span class="math">\(1 - \delta\)</span>. If one wants to be able to decode as many codes as possible, then the most common ones should be preferred and the subset <span class="math">\(S_\delta\)</span> should be used. The quantity <span class="math">\(\delta\)</span> is then the probability, set by us, that an outcome will not be encoded at all. If <span class="math">\(\delta\)</span> is very high, most outcomes will not be encoded. Those that will be, will be very few, and hence the codes can be short overall. The quantity <span class="math">\(\log_2 | S_\delta |\)</span> is called the essential bit content of <span class="math">\(X\)</span>, <span class="math">\(H_\delta(X)\)</span>. It gives the number of symbols needed to distinguish the most common outcomes of <span class="math">\(X\)</span>, with an accepted error of <span class="math">\(\delta\)</span>.</p>
<p>Next, consider <span class="math">\(N\)</span> i.i.d. random variables like <span class="math">\(X\)</span>, <span class="math">\(X^N = (X_1, X_2, ..., X_N)\)</span>. We'll denote the joint sample as <span class="math">\(\textbf{x}\)</span>. The joint entropy is additive for independent outcomes and is <span class="math">\(H(X^N) = NH(X)\)</span>, for any one <span class="math">\(X\)</span>. But can we say anything about <span class="math">\(H_\delta(X^N)\)</span>? For concreteness assume the outcomes <span class="math">\(\mathcal{A}_X\)</span> are different symbols from an alphabet. Consider the "most typical" outcome of <span class="math">\(X^N\)</span>. <span class="math">\(X^N\)</span> will likely contain <span class="math">\(p(x_1)N\)</span> instances of the first symbol, <span class="math">\(p(x_2)N\)</span> instances of the second symbol and so on. The information content of this typical string is precisely <span class="math">\(NH\)</span>. Slightly less typical strings will have information content somewhere around <span class="math">\(NH\)</span>. Based on this, we define the set of "typical" string samples - those strings whose information content per symbol is within a threshold <span class="math">\(\beta\)</span> of the expected information content per symbol:</p>
<div class="math">$$
T_{N\beta} = \Big\{ \textbf{x} \in \mathcal{A}_X^N : \lvert  -\frac{1}{N} \log_2 p(\textbf{x})  - H \rvert &lt; \beta \Big\}.
$$</div>
<p>Here <span class="math">\(\mathcal{A}_X^N\)</span> is the set of all joint outcomes. Naturally, the probability of any typical <span class="math">\(\textbf{x}\)</span> satisfies 
<span class="math">\(2^{-N(H + \beta)} &lt; p(x) &lt; 2^{-N(H - \beta)}\)</span> and hence, by the law of large numbers, along with Chebyshev's inequality, the probability that <span class="math">\(\textbf{x}\)</span> is typical will become close to unity as <span class="math">\(N\)</span> increases:</p>
<div class="math">$$
\begin{align}
P(\textbf{x} \in T_{N\beta}) &amp;\ge 1 - \frac{\sigma^2}{\beta^2 N}, \\
\sigma^2 &amp;= \text{Var}(- \log_2 p(x_n)).
\end{align}
$$</div>
<p>Therefore, as <span class="math">\(N\)</span> increases, most strings <span class="math">\(\textbf{x}\)</span> will be typical. This is a first observation. A second useful one is that we can approximately count the number of typical elements. The smallest probability that a typical <span class="math">\(\textbf{x}\)</span> can have is <span class="math">\(2^{-N(H + \beta)}\)</span>. We need the total probability of the typical elements to be less than unity, <span class="math">\(| T_{N\beta}| 2^{-N(H + \beta)} &lt; 1\)</span> and hence <span class="math">\(| T_{N\beta} | &lt; 2^{N(H + \beta)}\)</span>.</p>
<p>The typical set <span class="math">\(T_{N\beta}\)</span> is not the best set for compression, <span class="math">\(S_\delta(X^N)\)</span> is. For a fixed <span class="math">\(\beta\)</span>, <span class="math">\(S_\delta(X^N)\)</span> necessarily has at most elements as <span class="math">\(T_{N\beta}\)</span>, so in fact the size of the typical set provides an upper bound on <span class="math">\(H_\delta(X^N)\)</span>. Hence <span class="math">\(H_\delta(X^N)\)</span> is bounded from above by <span class="math">\(\log_2 | T_{N\beta}| = N(H + \beta)\)</span>. We now set <span class="math">\(\beta = \epsilon\)</span> and choose some possibly large <span class="math">\(N_0\)</span> such that <span class="math">\(\frac{\sigma^2}{\epsilon^2 N_0} \le \delta\)</span>. The result is that <span class="math">\(1/N \cdot H_\delta(X^N) &lt; H + \epsilon\)</span>, which is independent of <span class="math">\(\delta\)</span>.</p>
<p>A proof by condradiction based on similar logic shows that <span class="math">\(1/N \cdot H_\delta(X^N) &gt; H - \epsilon\)</span>, which is again, indepedent of <span class="math">\(\delta\)</span>. This result is quite deep and is called <a href="https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem">Shannon's source coding theorem</a>:</p>
<div class="math">$$
 H - \epsilon &lt; \frac{1}{N} \cdot H_\delta(X^N) &lt;  H + \epsilon.
$$</div>
<p>Here's how to interpret it. We have a source of information - a random variable. We first fix the probability of error <span class="math">\(\delta\)</span> that we are willing to accept. We might even fix it something arbitrarily close to <span class="math">\(0\)</span>. Now, the average minimal bits per symbol consistent with <span class="math">\(\delta\)</span> are given by <span class="math">\(H_\delta(X^N)/N\)</span>. For any finite sequence of <span class="math">\(N\)</span> samples, the required bits per symbol do not exceed <span class="math">\(H + \epsilon\)</span>. <span class="math">\(N\)</span> and <span class="math">\(\epsilon\)</span> depend on each other, so for a fixed length <span class="math">\(N\)</span>, we can calculate an <span class="math">\(\epsilon\)</span> or vice versa, for a desired <span class="math">\(\epsilon\)</span> we can calculate a required <span class="math">\(N\)</span>. As <span class="math">\(N\)</span> tends to infinity the bits per symbol of the encoding cannot increase beyond the entropy. Likewise, one needs at least <span class="math">\(H-\epsilon\)</span> bits per symbol to specify <span class="math">\(\textbf{x}\)</span>, even if we allow errors most of the time. Therefore, this result establishes that for lossless compression the average optimal code length cannot be shorter than the entropy.</p>
<p>Therefore, the entropy <span class="math">\(H(X)\)</span> obtains а new meaning. It gives a lower bound on the average bitlength of a lossless encoding of <span class="math">\(X\)</span>. The cross-entropy, <span class="math">\(H(p, q)\)</span>, used commonly in machine learning, also has an interesting meaning. Through the cross-entropy decomposition</p>
<div class="math">$$
\begin{align}
H(p, q) &amp;= \mathbb{E}_{x \sim p(x)}[-\log_2 q(x)] = - \sum_{x \in \mathcal{X}} p(x) \log_2 q(x) \\
&amp;= H(p) + D_{KL}(p \Vert q) = -\sum_{x \in \mathcal{X}} p(x)\log_2 p(x) + \sum_{x \in \mathcal{X}}p(x) \log_2 \left(\frac{p(x)}{q(x)} \right)
\end{align}
$$</div>
<p>one can see that it gives a lower bound on the average bitlength when one is coding the distribution <span class="math">\(q(x)\)</span> rather than the true one <span class="math">\(p(x)\)</span>. Similarly, the KL divergence gives precisely the difference between the codelength when coding for <span class="math">\(q(x)\)</span> and when coding for <span class="math">\(p(x)\)</span>.</p>
<p>The source coding theorem tells us that near-optimal codes exist, but it does not tell us how to find them. It also encodes entire blocks of symbols. <em>Symbol codes</em> are variable-length codes which are lossless and encode one symbol at a time. To encode a whole message <span class="math">\(x_1 x_2 ... x_N\)</span> we simply concatenate the codes <span class="math">\(c(x_1)c(x_2)...c(x_N)\)</span>. Ideally these codes should be uniquely decodable, which requires that no two symbols have the same encoding. Moreover, it should be easily decodable. This is achieved when no codeword is a prefix of any other codeword. This implies that one can read the codemessage left-to-right and decode as they go, without looking back or ahead of the current codeword. Such codes are called <em>prefix</em> or <em>self-punctuating</em> codes. Lastly, to achieve maximum compression, the average codelength should be minimal.</p>
<p>There are some useful results for symbol codes. First, the Kraft inequality states that if a code is uniquely-decodable, then its lengths satisfy <span class="math">\(\sum_{x \in \mathcal{X}}2^{-l(c(x))} \le 1\)</span>, where <span class="math">\(l(\cdot)\)</span> is the length operator. If the codes satisfy this inequality, then there exists a prefix code with those lengths. Moreover, the optimal source codelength satisfy <span class="math">\(l_i = -\log_2 p_i\)</span> and similarly to the source coding theorem, there exists a prefix code for <span class="math">\(X\)</span> satisfying <span class="math">\(H(X) \le L(C, X) \le H(X) + 1\)</span>. The <a href="https://en.wikipedia.org/wiki/Huffman_coding">Huffman coding</a> is an optimal variable-length prefix symbol code.</p>
<figure>
    <img class='img' src="/images/huffman.png" alt="Huffman coding" width="1200">
    <figcaption>Figure 1: A Huffman code. We iteratively merge the two current least-likely outcomes, forming a binary tree. The codes are obtained by traversing the branches of the tree. The mean codelength is bounded by the entropy.</figcaption>
</figure>

<p>Compared to symbol codes, <em>stream codes</em> represent the data as a stream of possibly non i.i.d tokens and adapt the encoding accordingly. <em>Arithmetic</em> codes are efficient stream codes and are currently one of the go-to solutions for lossless compression. For notation, we have a sequence <span class="math">\(x_1, x_2, ..., x_N\)</span> where <span class="math">\(x_i\)</span> is the <span class="math">\(i\)</span>-th random variable and <span class="math">\(a_i\)</span> is the particular observed symbol from it. The symbol alphabet has <span class="math">\(I\)</span> symbols and <span class="math">\(a_I\)</span> is an end-of-sequence (EOS) token. Suppose we have access to a probabilistic model for <span class="math">\(p(x_i = a_i | x_1, x_2, ..., x_{i-1})\)</span>. Arithemtic codes work by modeling intervals within <span class="math">\([0, 1)\)</span>. Consider that we can always divide <span class="math">\([0, 1)\)</span> into <span class="math">\(I\)</span> intervals according to <span class="math">\(p(x_1)\)</span>. Then, depending on the exact value of <span class="math">\(x_1\)</span>, suppose <span class="math">\(a_i\)</span>, we can take its interval and divide it again in <span class="math">\(I\)</span> intervals. Let's denote the <span class="math">\(j\)</span>-th new interval as <span class="math">\(a_{i,j}\)</span>. Its length will be precisely </p>
<div class="math">$$
p(x_1=a_i, x_2=a_j) = p(x_1 = a_i) p(x_2 = a_j | x_1 = a_i).
$$</div>
<p>Binary codes can be associated with intervals. For example, the code message <span class="math">\(01101\)</span> is interpreted as the binary interval <span class="math">\([0.01101, 0.01110)_2\)</span> which corresponds to the decimal interval <span class="math">\([0.40625, 0.4375)\)</span>. To encode the full string <span class="math">\(x_1x_2...x_N\)</span> we compute the interval corresponding to it and send any binary string whose interval belongs fully within this interval. The actual encoding happens symbol by symbol and whenever the current interval falls within the previous estimated ones, the encoder can produce the next bit. The decoder works similarly, it uses the model to estimate the bounds of the various next intervals and decodes the next symbol as soon as the observed interval falls within one of them. It can also generate samples by starting from random bits, similar to various deep learning models.</p>
<p>Lempel-Ziv codes are another useful type of stream codes which do not need a probability model for the underlying data. They works by memorizing sequences of substrings and replacing subsequent encounters of these substrings with pointers to them. They are universal, in the sense that given an ergodic source these codes achieve asymptotic compression down to the entropy per symbol. In practice, they have proved useful and have found wide applicability in tools like <code>compress</code> and <code>gzip</code>, although various modifications of the algorithms are typically used.</p>
<p>None of these codes will be decoded properly if any of the message bits are changed. Thus, in a noisy channel, where the environment can mutate the codes before they have been passed to the decoder, one needs to be careful - error detection and correction algorithms are needed.</p>
<p>A discrete noisy memoryless channel <span class="math">\(Q\)</span> adds noise to the codewords and is characterized by an input and output alphabets <span class="math">\(\mathcal{A}_X\)</span> and <span class="math">\(\mathcal{A}_Y\)</span>, along with the probabilities <span class="math">\(p(y | x)\)</span>. We can measure how much information the output <span class="math">\(Y\)</span> conveys about the input <span class="math">\(X\)</span> using the mutual information <span class="math">\(I(X; Y)\)</span> defined as:</p>
<div class="math">$$
\begin{align}
I(X; Y) &amp;= H(X) - H(X | Y) = H(Y) - H( Y | X) \\
H(A|B) &amp;= -\sum_{a \in \mathcal{A}_A} \sum_{ b \in \mathcal{A}_B} p(a, b) \log_2 p(a | b).
\end{align}
$$</div>
<p>The mutual information <span class="math">\(I(X; Y)\)</span> depends on the input <span class="math">\(X\)</span> which is under our control. Hence one can find that particular distribution <span class="math">\(\mathcal{P}_X\)</span> that maximizes the mutual information. The resulting maximal mutual information is called the <em>channel capacity</em> <span class="math">\(C(Q)\)</span> and plays a crucial role in limiting the amount of information that is transmitted through the channel in an error-free manner.</p>
<p>As an example of some noisy channels consider the following:</p>
<ul>
<li>A binary symmetric channel randomly flips a given bit irrespective of its current value</li>
<li>A binary erasure channel randomly deletes some bits, producing a new token like "?"</li>
<li>A noisy typewriter channel randomly maps one outcome to its nearest neighbors. E.g. it may map the symbol B to either A, B, or C.</li>
</ul>
<p>The channel coding theorem, again due to Claude Shannon, states that, intuitively, given any noisy channel, with any degree of data contamination, it is possible to communicate information nearly error-free up to some computable limit rate through the channel. This computable limit is given by the channel capacity. It is very easy to confirm the theorem for the noisy typewriter channel. Suppose the underlying alphabet <span class="math">\(\mathcal{A}_X\)</span> is the English alhabet, with symbols <span class="math">\(A, B, C, ..., Z, \square\)</span>, where <span class="math">\(\square\)</span> is the EOS symbol. Now, if you only use 9 symbols - <span class="math">\(B, E, H, ..., Z\)</span>, any output can be uniquely decoded because these input symbols constitute a non-confusable subset. The error-free information rate of this channel is <span class="math">\(\log_2 9\)</span> bits, which is exactly the capacity.</p>
<p>Something interesting happens if we are allowed to use the noisy channel <span class="math">\(N\)</span> times, instead of just once - the multi-use channel starts looking more and more like a noisy typewriter one (stated without proof) and we can find a non-confusable input subset with which to transmit data error-free. Indeed, the number of typical <span class="math">\(\textbf{y}\)</span> is <span class="math">\(2^{NH(Y)}\)</span>. The number of typical <span class="math">\(\textbf{y}\)</span> given a typical <span class="math">\(\textbf{x}\)</span> is <span class="math">\(2^{NH(Y | X)}\)</span>. Hence, the non-confusable subsets are at most <span class="math">\(2^{NH(Y) - NH(Y | X)} = 2^{NI(X;Y)}\)</span>. This shows that for sufficiently large <span class="math">\(N\)</span> it is possible to encode and transmit a message with essentially null error up to some rate given by the channel capacity.</p>
<p>These results are strong but they are only scraping the surface when it comes to codes. Some of the most powerful codes like low-density parity-check codes, convolution codes, and turbo codes build on top of more theory and hence we have to leave them for the future.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = "https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML";

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'], ['$', '$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div>
  
  <p class="post-meta">
    Tag: cs
  </p>

  </article>


    </div>
  </main>


<footer class="site-footer h-card">
  <div class="wrapper">
  
  <p></p>
  

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">


    <div class="footer-col-wrapper">

      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">The Critical Section</li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          <li><a href="https://github.com/aceofgreens"><i class="fa fa-github"></i> GitHub</a></li>


        </ul>
      </div>


      <div class="footer-col footer-col-3">
        <p>A personal blog for artificial intelligence and similar topics.</p>
      </div>
    </div>

  </div>
</footer>

<script type="text/javascript">
function welcomeFunction() {
  var items = document.getElementsByTagName("code");
    for (var i = items.length; i--;) {
      items[i].setAttribute("class", "highlight");
  }
}
</script>

</body>
</html>