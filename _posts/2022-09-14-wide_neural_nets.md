---
layout: post
title: Infinitely Wide Neural Networks
date: 2022-09-14 16:00:00 +0200
tags: [ai]
---

Deep neural networks *seem* to break the common U-shaped bias-variance risk curve. Current state-of-the-art networks have significantly more parameters than dataset samples, and yet still perform surprisingly well on the test set. We can't really say they're overfitting. So what's going on? This post explores recent results and insights around over-parameterized networks and their supposedly different training regimes. In particular, a lot of intuition can be gained from analyzing what happens in networks with infinitely many units in the hidden layers.

### Bayesian Inference

The first major limiting result is that infinitely wide neural networks are Gaussian processes. A Gaussian process is a stochastic process such that any finite collection of random variables from it, have a joint Gaussian distribution. Intuitively, we can think of it like a probability distribution on functions $f(x)$, such that if one selects a number of points $\\{x_1, ..., x_N\\}$, then the corresponding values $\\{f(x_1), ..., f(x_N)\\}$ come from a multidimensional normal distribution with mean vector $\\{m(x_1), ..., m(x_N) \\}$ and a $N \times N$ covariance matrix $\Sigma$ containing the covariances between $f(x_i)$ and $f(x_j)$.

In practice, instead of specifying the covariance matrix for any finite set of variables, we use a kernel function $k(\cdot, \cdot)$ which takes in a pair of elements $x$ and $x'$ and computes their covariance $k(x, x')$:

$$
\begin{align}
f(x) \sim GP(m(x), k(\cdot, \cdot)) \Leftrightarrow & \\ 

{\small \begin{bmatrix}
f(x_1) \\
\vdots \\
f(x_N) \\
\end{bmatrix}
} \sim & \ \mathcal{N} \Big(
    
    \mu = {\small \begin{bmatrix}
    m(x_1) \\
    \vdots \\
    m(x_2) \\
    \end{bmatrix}},
    
    \Sigma = {\small \begin{bmatrix}
    k(x_1, x_1) & \cdots & k(x_1, x_N) \\
    \vdots & \ddots & \vdots \\
    k(x_N, x_1) & \cdots & k(x_N, x_N) \\
    \end{bmatrix}}

    \Big )
\end{align} 
$$

A Gaussian process is completely specified by its mean function $m(x)$ and its kernel function $k(\cdot, \cdot)$. Hence, to describe the GP generated by a infinitely wide neural network we'll have to specify its mean and kernel.

Suppose we have a network with $L$ layers and the number of units in layer $l$ is $n^l$. We'll annotate the pre-activations as $z^l$ and the post-activations (after applying a nonlinearity) with $y^l$. Assume the input is $y^0$, and the first hidden layer is $z^0$. After we apply the element-wise activation $\phi(\cdot)$ on $z^l$, we get $y^{l + 1}$. The weights for layer $l$ are initialized randomly from $\mathcal{N}(0, \frac{\sigma^2_w}{n^l})$ and those for the biases from $\mathcal{N}(0, \sigma^2_b)$.

The random initialization of the parameters of the network induces a probability distribution on its outputs. Sampling different parameters will produce different outputs for the same inputs. Likewise, we can talk about the function being modelled as also coming from a probability distribution over functions. This distribution is hard to characterize. However, the main point is that in a network with infinitely many hidden units, this distribution over functions becomes a Gaussian process. Let's see how this happens.

Considering layer $l$ we note that it's weights $W^l$ and biases $b^l$ are random Gaussian variables. If we condition on the previous activations $y^l$, then the pre-activations $z^l$ are just a linear map of the independent Gaussian random variables from the weights and the biases. As a result, the pre-activations are also independent and Gaussian. This is true even with finitely wide layers.

$$
z^l|y^l \sim GP(0, \sigma^2_2 K^l + \sigma^2_b)
$$

$K^l$ is the second moment matrix of previous activations and $K(x, x') = \frac{1}{n^l}\sum_{i = 1}^{n^l} y_i(x) y_i(x')$.  
What does it mean that $z^l | y^l$ is a Gaussian process? It means that if we pick two inputs $x$ and $x'$, and we propagate them up to layer $l$, then we'll get $y^l(x)$ and $y^l(x')$. These two values will have a covariance given by $K^l(x, x')$. Conditioning on $y^l(x)$ and $y^l(x')$, we can sample various weights for $W^l$ and $b^l$, and the distribution on the resulting function $z^l$ will be a Gaussian process.

We can also say that $z^l$ depends on $K^l$, so technically $z^l \| K^l$ is a Gaussian process with zero mean and the same kernel. Now, we condition $K^l$ on the previous pre-activations $z^{l-1}$:

$$
K^l(x, x') = \frac{1}{n^l} \sum_{i = 1}^{n^l} y^l_i(x) y^l_i(x') = \frac{1}{n^l} \sum_{i = 1}^{n^l} \phi(z_i^{l-1}(x)) \phi(z_i^{l-1}(x'))
$$

Since $z_i^{l - 1} \| K^{l - 1}$ is a GP, then $K^l(x, x')$ is calculated by sampling $n^l$ pairs of points $z_i^{l-1}(x)$ and $z_i^{l-1}(x')$, multiplying them, and averaging. And here's the crucial part. If $n^l \rightarrow \infty$, then the result of this averaging becomes deterministic (as we are averaging over infinitely many samples), and $K^l(x, x') = \mathbb{E}[\phi(z^{l-1}(x)) \phi(z^{l-1}(x'))]$. Even better, this can be calculated analytically for some specific activations like $\text{ReLU}$, $\text{GeLU}$, and $\text{erf}$. Thus $K^l \| K^{l-1}$ becomes deterministic once $n^l \rightarrow \infty$.

At this point, if all layer widths go to infinity, we can recursively define $K^l$ as an exact deterministic calculation of $K^0$ assuming we can solve the expectation integrals above. $K^0(x, x')$ is simply $\frac{1}{n^0} \sum_{i = 1}^{n^0} x_i x'_i$ and based on it, we can compute the kernel function of the network output. The Gaussian process resulting from the last layer of an infinite width neural network is called the *Neural Network Gaussian Process$ (NNGP). It's zero centered, and it's kernel function is defined recursively, as described above.







### Gradient Descent

Let's set some notation. Let $f(\cdot, \theta)$ represent a neural network with parameters $\theta$, flattened across all layers. We have a standard supervised seeting where the output on training sample $x$ is $f(x, \theta)$, the corresponding label is $y$, and the loss on that sample is $\ell(f(x, \theta), y)$, which could be the mean squared loss, binary cross entropy loss, or any other loss function. The training set contains $N$ samples, $\mathcal{X} = \\{x_1, x_2, ..., x_N \\}$, along with their corresponding labels $\mathcal{Y} = \\{y_1, y_2, ..., y_N \\}$.

The loss that we are minimizing is averaged across all samples in the training set:

$$
J = \frac{1}{N} \sum_{i = 1}^N \ell(f(x_i, \theta), y_i).
$$

With gradient descent we minimize $J$ by repeatedly computing the gradient $\nabla_\theta J$ and taking a small step $\eta$ in the opposite direction from it. The gradient of the loss function with respect to the parameters is given by

$$
\nabla_\theta J = \frac{1}{N} \sum_{i = 1}^N \nabla_\theta f(x_i, \theta) \nabla_f \ell(f(x_i, \theta), y_i).
$$

If the step size $\eta$ is infinitesimally small, we can actually model the change in the weights $\theta$ through time as a derivative, instead of a sequence of discrete updates:

$$
\frac{d\theta}{dt} = - \frac{1}{N} \sum_{i = 1}^N \nabla_\theta f(x_i, \theta) \nabla_f \ell(f(x_i, \theta), y_i).
$$

This is called continuous gradient descent, or even *gradient flow*. It simply shows that if the learning rate is *very* small and we let the neural network train for a very long time, the instantaneous change in the parameters depends on the gradient of the loss function at the current parameters, averaged across the training samples. This is how the network evolves in parameter space. But we can also look at how it evolves in function space.

Let's fix a single sample $x'$ and look at how the network output for $x'$ changes with time. That is, we want to calculate $\frac{df(x', \theta)}{dt}$. This derivative is given by the chain rule.

$$
\frac{df(x', \theta)}{dt} = \frac{df(x', \theta)}{d\theta} \frac{d\theta}{dt} = - \frac{1}{N} \sum_{i = 1}^N  \nabla_\theta f(x', \theta)^T \nabla_\theta f(x_i, \theta) \nabla_f \ell(f(x_i, \theta), y_i).
$$

The main quantity of interest here is $\nabla_\theta f(x', \theta)^T \nabla_\theta f(x_i, \theta)$, which is called the *neural tangent kernel* (NTK). This is a very important quantity because it directly shows how the network evolves in function space.

Let's write the NTK between two data points $x$ and $x'$ as $K_\theta(x, x') = \nabla_\theta f(x, \theta)^T \nabla_\theta f(x', \theta)$. It is the dot product between the gradients of the network output w.r.t. the parameters evaluated at $x$ and $x'$, and is therefore symmetric, as all kernels should be. It depends on $\theta$ and hence, as $\theta$ changes, the NTK changes. Since all the weights and biases $\theta$ are initialized randomly, the NTK is also random.